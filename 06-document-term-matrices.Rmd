# Tidying and casting document-term matrices {#dtm}

```{r echo = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 150)
library(ggplot2)
theme_set(theme_light())
```

So far, we've been analyzing data in a tidy text structure: a data frame with one-token-per-document-per-row. This lets us use the popular suite of tidy tools such as dplyr, tidyr, and ggplot2. We've demonstrated that many text analyses can be performed using these principles.

But many of the existing tools for natural language processing don't work with this kind of structure. The [CRAN Task View for Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) lists a large selection of packages that take other inputs. One of the most common is the [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix), a sparse matrix with one row for each document in a collection and one column for each term or word. The value that goes into the matrix is usually a word count or sometimes tf-idf. These matrices are sparse (they consist mostly of zeroes), so special algorithms and data structures can be used to deal with them that are efficient and fast.

The tidytext package can integrate these packages into an analysis while still relying on our tidy tools. The two key verbs are:

* `tidy()`: Constructs a data frame that summarizes a model's statistical findings.
* `cast_`: Turns a tidy one-term-per-row data frame into a document-term matrix. This includes `cast_sparse()` (sparse Matrix), `cast_dtm()` (`DocumentTermMatrix` objects from tm), and `cast_dfm()` (`dfm` objects from quanteda).

## Tidying a document-term matrix

As we have discussed, many existing text mining datasets expect and provide a **document-term matrix**, or DTM. A DTM is a matrix where

* each row represents one document,
* each column represents one term, and
* each value typically contains the number of appearances of that term in that document.

DTMs are usually implemented as sparse matrices, meaning the vast majority of values are 0. These objects can be interacted with as though they were matrices, but are stored in a more efficient format.

One commonly used implementation of DTMs in R is the `DocumentTermMatrix` class in the tm package. For example, consider the corpus of 2246 Associated Press articles from the topicmodels package.

```{r AssociatedPress}
library(tm)

data("AssociatedPress", package = "topicmodels")
class(AssociatedPress)
AssociatedPress
```

We see that this dataset contains `r nrow(AssociatedPress)` documents (each of them an AP article) and `r ncol(AssociatedPress)` terms (words). Notice that this example DTM is 99% sparse.

If we want to analyze this with tidy tools, we need to turn it into a one-token-per-document-per-row data frame first. The broom package [@R-broom] introduced the `tidy` verb, which takes a non-tidy object and turns it into a data frame. The tidytext package implements that method for `DocumentTermClass` objects:

```{r ap_td, dependson = "AssociatedPress"}
library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress)
ap_td
```

Notice that we now have a tidy three-column `tbl_df`, with variables `document`, `term`, and `count`. This tidying operation is similar to the `melt` function from the reshape2 package [@R-reshape2] for non-sparse matrices.

As we've seen in chapters 2-5, this form is convenient for analysis with the dplyr and tidytext packages. For example, you can perform sentiment analysis on these newspaper articles.

```{r ap_sentiments, dependson = "ap_td"}
bing <- sentiments %>%
  filter(lexicon == "bing") %>%
  select(word, sentiment)

ap_sentiments <- ap_td %>%
  inner_join(bing, by = c(term = "word"))

ap_sentiments
```

This could, for example, let us visualize which words from these AP articles most often contributed to positive or negative sentiment:

```{r dependson = "ap_sentiments", fig.height = 6, fig.width = 7}
library(ggplot2)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 150) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(alpha = 0.8, stat = "identity") +
  ylab("Contribution to sentiment") +
  coord_flip()
```

A tidier is also available for the `dfm` (document-feature matrix) class from the quanteda package [@R-quanteda]. Consider the corpus of presidential inauguration speeches that comes with the quanteda package:

```{r quanteda_corpus}
library(methods)

data("inaugCorpus", package = "quanteda")
d <- quanteda::dfm(inaugCorpus)

d

tidy(d)
```

We could find the words most specific to several inaugural speeches using `bind_tf_idf` from chapter 4:

```{r presidents, fig.width=8, fig.height=8}
speeches <- c("1861-Lincoln", "1945-Roosevelt",
              "1961-Kennedy", "2009-Obama")

inaug_tf_idf <- tidy(d) %>%
  bind_tf_idf(term, document, count) %>%
  arrange(desc(tf_idf)) %>%
  filter(document %in% speeches)

inaug_tf_idf

inaug_tf_idf %>%
  group_by(document) %>%
  top_n(8, tf_idf) %>%
  ungroup() %>%
  mutate(term = reorder(term, tf_idf)) %>%
  ggplot(aes(term, tf_idf, fill = document)) +
  geom_bar(stat = "identity", alpha = 0.8,show.legend = FALSE) +
  facet_wrap(~ document, scales = "free") +
  coord_flip()
```

## Casting tidy text data into a DocumentTermMatrix

Some existing text mining tools or algorithms work only on sparse document-term matrices. Therefore, tidytext provides `cast_` verbs for converting from a tidy form to these matrices.

For example, we could take the tidied AP dataset and cast it back into a document-term matrix:

```{r}
ap_td

ap_td %>%
  cast_dtm(document, term, count)
```

Similarly, we could cast it into a Term-Document Matrix with `cast_tdm`, or quanteda's dfm with `cast_dfm`:

```{r }
# cast into a Term-Document Matrix
ap_td %>%
  cast_tdm(term, document, count)

# cast into quanteda's dfm
ap_td %>%
  cast_dfm(term, document, count)
```

Some tools simply require a sparse matrix:

```{r}
library(Matrix)

# cast into a Matrix object
m <- ap_td %>%
  cast_sparse(document, term, count)

class(m)
dim(m)
```

This casting process allows for easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications.

## Tidying corpus objects with metadata

You can also tidy Corpus objects from the tm package. For example, consider a Corpus containing 20 documents:

```{r reuters}
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578),
                   readerControl = list(reader = readReut21578XMLasPlain))

reuters
```

The `tidy` verb creates a table with one row per document:

```{r reuters_td, dependson = "reuters"}
reuters_td <- tidy(reuters)
reuters_td
```

Another variation of a corpus object is `corpus` from the quanteda package:

```{r inaug_td}
library(quanteda)

data("inaugCorpus")

inaugCorpus

inaug_td <- tidy(inaugCorpus)
inaug_td
```

This lets us work with tidy tools like `unnest_tokens` to analyze the text alongside the metadata.

```{r inaug_words, dependson = "inaug_td"}
inaug_words <- inaug_td %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

inaug_words
```

We could then, for example, see how the appearance of a word changes over time:

```{r inaug_freq, dependson = "inaug_words"}
library(tidyr)

inaug_freq <- inaug_words %>%
  count(Year, word) %>%
  ungroup() %>%
  complete(Year, word, fill = list(n = 0)) %>%
  group_by(Year) %>%
  mutate(year_total = sum(n),
         percent = n / year_total) %>%
  ungroup()

inaug_freq %>%
  filter(word == "america")
```

For instance, we could display the top 6 terms that have changed in frequency over time.

```{r dependson = "models", fig.width=8, fig.height=6}
library(scales)

inaug_freq %>%
  filter(word %in% c("americans", "century", "foreign", "god",
                     "union", "constitution")) %>%
  ggplot(aes(Year, percent)) +
  geom_point(alpha = 0.8) +
  geom_smooth() +
  facet_wrap(~ word, scales = "free_y") +
  scale_y_continuous(labels = percent_format()) +
  ylab("Frequency of word in speech")
```
