[
["index.html", "Tidy Text Mining with R Welcome to Tidy Text Mining with R", " Tidy Text Mining with R Julia Silge and David Robinson 2016-10-24 Welcome to Tidy Text Mining with R This is the website for Tidy Text Mining with R! Visit the GitHub repository for this site. This work by Julia Silge and David Robinson is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License. "],
["intro.html", "1 Introduction 1.1 What is tidy text? 1.2 About this book 1.3 Outline 1.4 Topics this book does not cover", " 1 Introduction If you work in analytics or data science, like we do, you are familiar with the fact that data is being generated all the time at ever faster rates. (You may even be a little weary of people pontificating about this fact.) Analysts are often trained to handle tabular or rectangular data that is mostly numeric, but much of the data proliferating today is unstructured and typically text-heavy. Many of us who work in analytic fields are not trained in even simple interpretation of natural language. We developed a new R package, tidytext (Silge and Robinson 2016), because we were familiar with many methods for data wrangling and visualiation, but couldn’t easily apply these same methods to text. We found that using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. By treating text as data frames of words, we can manipulate, summarize, and visualize the characteristics of text easily and integrate natural language processing into effective workflows we were already using. The tools provided by the tidytext package are relatively simple; what is important is the possible applications. Thus, this book provides compelling examples of real text mining problems. 1.1 What is tidy text? As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: each variable is a column each observation is a row each type of observational unit is a table We thus define the tidy text format as being a table with one-term-per-row. This is in contrast to the ways text is often stored in current analyses, as raw strings or perhaps a specialized format like a document-term matrix. For tidy text mining, the term that is stored in each row can be, mostly commonly, a single word, or perhaps an n-gram, a sentence, or another unit of text of interest for a certain analysis. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson et al. 2015). By keeping the input and output in tidy tables, users can transition fluidly between these tools. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy objects (see the broom package (Robinson et al. 2015)) from popular text mining R packages such as tm (Ingo Feinerer and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow with easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. 1.2 About this book This book is focused on practical software examples and data explorations. There are few equations, but a great deal of code. We especially focus on generating real insights from the literature, news, and social media that we analyze. We don’t assume any previous knowledge of text mining; professional linguists and text analysts will likely find our examples elementary, though we are confident they can build on the framework for their own analyses. We do assume that the reader is at least slightly familiar with dplyr, ggplot2, and the %&gt;% “pipe” operator in R, and is interested in applying these tools to text data. For users who don’t have this background, we recommend books such as R for Data Science. We believe that with a basic background and interest in tidy data, even a user early in their R career can understand and apply our examples. 1.3 Outline We start by introducing the tidy text format, and some of the ways dplyr, tidyr, and tidytext allow informative analyses of this structure. Chapter 2 outlines the tidy text format and the unnest_tokens function. It also introduces the gutenbergr and janeaustenr packages, which provide useful literary text datasets that we’ll use throughout this book. Chapter 3 shows how to perform sentiment analysis on a tidy text dataset, using the sentiments dataset from tidytext and inner_join from dplyr. Chapter 4 describes the tf-idf statistic (term frequency times inverse document frequency), a quantity used for identifying terms that are especially important to a particular document. Chapter 5 introduces n-grams and how to analyze word networks in text using the widyr package. Text won’t be tidy at all stages of an analysis, and it is important to be able to convert back and forth from a tidy format. Chapter 6 introduces methods for tidying document-term matrices and corpus objects from the tm and quanteda packages, as well as for casting tidy text datasets into those formats. Chapter 7 explores the concept of topic modeling, and uses the tidy method for interpreting and visualizing the output of the topicmodels package. We conclude with several tidy text analyses that bring together multiple text mining approaches we’ve learned. Chapter 8 demonstrates an application of a tidy text analysis by analyzing the authors’ own Twitter archives. How do Dave’s and Julia’s tweeting habits compare? Chapter 9 explores metadata from over 32,000 NASA datasets by looking at how keywords from the datasets are connected to title and description fields. Chapter 10 analyzes a dataset of Usenet messages from a diverse set of newsgroups (focused on topics like politics, hockey, technology, atheism, and more) to understand patterns across the groups. 1.4 Topics this book does not cover This book serves as an introduction to a framework along with a collection of examples, but it is far from a complete exploration of natural language processing. The CRAN Task View on Natural Language Processing provides details on more ways to use R for computational linguistics. There are several areas that you may want to explore in more detail according to your needs. Supervised classification and prediction: Machine learning on text is a vast topic that could easily fill its own volume. We introduce one method of unsupervised clustering (topic modeling through latent Dirichlet allocation) in Chapter 7 but many more machine learning algorithms can be used in dealing with text. More complex tokenization: We hand tokenization off to the tokenizers package (Mullen 2016), which itself wraps a variety of tokenizers with a consistent interface, but many others exist for specific applications. Languages other than English: Some of our users have had success applying tidytext to their text mining needs for languages other than English but we are not covering those issues in this book. We feel that the tidy data philosphy is a powerful tool to make handling data easier and more effective, and this is no less true when it comes to handling text. We also believe that the tools provided by tidy data principles and specifically tidy text mining are well suited to extensions beyond the examples we provide here. References "],
["tidytext.html", "2 The tidy text format 2.1 The unnest_tokens function 2.2 Tidying the works of Jane Austen 2.3 The gutenbergr package 2.4 Word frequencies", " 2 The tidy text format We define the tidy text format as being a table with one-term-per-row. Structuring text data in this way means that it conforms to tidy data principles and can be manipulated with a set of consistent tools. This is worth contrasting with the ways text is often stored in text mining approaches. Raw string: Text can, of course, be stored as raw strings within R, and often text data is first read into memory in this form. Corpus: These types of objects typically annotate the raw string content with additional metadata and details. Document-term matrix: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf (see Chapter 4). Let’s hold off on exploring structures like a document-term matrix until Chapter 6, and get down to the basics of converting text to a tidy format. 2.1 The unnest_tokens function Emily Dickinson wrote some lovely text in her time. text &lt;- c(&quot;Because I could not stop for Death -&quot;, &quot;He kindly stopped for me -&quot;, &quot;The Carriage held but just Ourselves -&quot;, &quot;and Immortality&quot;) text ## [1] &quot;Because I could not stop for Death -&quot; &quot;He kindly stopped for me -&quot; ## [3] &quot;The Carriage held but just Ourselves -&quot; &quot;and Immortality&quot; This is a typical character vector that we might want to analyze. In order to turn it into a tidy text dataset, we first need to put it into a data frame. library(dplyr) text_df &lt;- data_frame(line = 1:4, text = text) text_df ## # A tibble: 4 × 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 Because I could not stop for Death - ## 2 2 He kindly stopped for me - ## 3 3 The Carriage held but just Ourselves - ## 4 4 and Immortality Notice that this data frame isn’t yet compatible with tidy tools. We can’t filter out words or count which occur most frequently, since each row is made up of multiple combined tokens. We need to turn this into one-token-per-document-per-row. To do this, we use tidytext’s unnest_tokens function. library(tidytext) text_df %&gt;% unnest_tokens(word, text) ## # A tibble: 20 × 2 ## line word ## &lt;int&gt; &lt;chr&gt; ## 1 1 because ## 2 1 i ## 3 1 could ## 4 1 not ## 5 1 stop ## 6 1 for ## 7 1 death ## 8 2 he ## 9 2 kindly ## 10 2 stopped ## # ... with 10 more rows We’ve now split each row so that there’s one token (word) in each row of the new data frame. Also notice: Other columns, such as the line number each word came from, are retained. Punctuation has been stripped. By default, unnest_tokens converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior). Having the text data in this format lets us manipulate, process, and visualize the text using the standard set of tidy tools, namely dplyr, tidyr, ggplot2, and broom. 2.2 Tidying the works of Jane Austen Let’s use the text of Jane Austen’s 6 completed, published novels from the janeaustenr package (Silge 2016), and transform them into a tidy format. The janeaustenr package provides these texts in a one-row-per-line format. Let’s start with that, annotate a linenumber quantity to keep track of lines in the original format, and use a regex to find where all the chapters are. library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 × 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 SENSE AND SENSIBILITY Sense &amp; Sensibility 1 0 ## 2 Sense &amp; Sensibility 2 0 ## 3 by Jane Austen Sense &amp; Sensibility 3 0 ## 4 Sense &amp; Sensibility 4 0 ## 5 (1811) Sense &amp; Sensibility 5 0 ## 6 Sense &amp; Sensibility 6 0 ## 7 Sense &amp; Sensibility 7 0 ## 8 Sense &amp; Sensibility 8 0 ## 9 Sense &amp; Sensibility 9 0 ## 10 CHAPTER 1 Sense &amp; Sensibility 10 1 ## # ... with 73,412 more rows To work with this as a tidy dataset, we need to restructure it as one-token-per-row format. The unnest_tokens function is a way to convert a dataframe with a text column to be one-token-per-row. library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,054 × 4 ## book linenumber chapter word ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # ... with 725,044 more rows This function uses the tokenizers package (Mullen 2016) to separate each line into words. The default tokenizing is for words, but other options include characters, n-grams, sentences, lines, paragraphs, or separation around a regex pattern. Now that the data is in one-word-per-row format, we can manipulate it with tidy tools like dplyr. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join. data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) We can also use count to find the most common words in all the books as a whole. tidy_books %&gt;% count(word, sort = TRUE) ## # A tibble: 13,914 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 1855 ## 2 time 1337 ## 3 fanny 862 ## 4 dear 822 ## 5 lady 817 ## 6 sir 806 ## 7 day 797 ## 8 emma 787 ## 9 sister 727 ## 10 house 699 ## # ... with 13,904 more rows For example, this allows us to visualize the commonly used words using ggplot2. library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 600) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_bar(stat = &quot;identity&quot;) + xlab(NULL) + coord_flip() We could pipe this straight into ggplot2 because of our consistent use of tidy tools. 2.3 The gutenbergr package Now that we’ve used the janeaustenr package, let’s introduce the gutenbergr package (Robinson 2016). The gutenbergr package provides access to the public domain works from the Project Gutenberg collection. The package includes tools both for downloading books (stripping out the unhelpful header/footer information), and a complete dataset of Project Gutenberg metadata that can be used to find works of interest. In this book, we will mostly use the function gutenberg_download() that downloads one or more works from Project Gutenberg by ID, but you can also use other functions to explore metadata, pair Gutenberg ID with title, author, language, etc., or gather information about authors. To learn more about gutenbergr, check out the package’s tutorial at rOpenSci, where it is one of rOpenSci’s packages for data access. 2.4 Word frequencies A common task in text mining is to look at word frequencies, just like we have done above for Jane Austen’s novels, and to compare frequencies across different texts. We can do this intuitively and smoothly using tidy data principles. We already have Jane Austen’s works; let’s get two more sets of texts to compare to. First, let’s look at some science fiction and fantasy novels by H.G. Wells, who lived in the late 19th and early 20th centuries. Let’s get The Time Machine, The War of the Worlds, The Invisible Man, and The Island of Doctor Moreau. library(gutenbergr) hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159)) tidy_hgwells &lt;- hgwells %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) Just for kicks, what are the most common words in these novels of H.G. Wells? tidy_hgwells %&gt;% count(word, sort = TRUE) ## # A tibble: 11,769 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 454 ## 2 people 302 ## 3 door 260 ## 4 heard 249 ## 5 black 232 ## 6 stood 229 ## 7 white 222 ## 8 hand 218 ## 9 kemp 213 ## 10 eyes 210 ## # ... with 11,759 more rows Now let’s get some well-known works of the Brontë sisters, whose lives overlapped with Jane Austen’s somewhat but who wrote in a rather different style. Let’s get Jane Eyre, Wuthering Heights, The Tenant of Wildfell Hall, Villette, and Agnes Grey. bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 766)) tidy_bronte &lt;- bronte %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) What are the most common words in these novels of the Brontë sisters? tidy_bronte %&gt;% count(word, sort = TRUE) ## # A tibble: 25,714 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 1586 ## 2 miss 1388 ## 3 hand 1239 ## 4 day 1136 ## 5 eyes 1023 ## 6 night 1011 ## 7 house 960 ## 8 head 957 ## 9 looked 949 ## 10 aunt 896 ## # ... with 25,704 more rows Interesting that “time”, “eyes”, and “hand” are in the top 10 for both H.G. Wells and the Brontë sisters. Now, let’s calculate the frequency for each word for the works of Jane Austen, the Brontë sisters, and H.G. Wells. tidy_both &lt;- bind_rows( mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;), mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;)) frequency &lt;- tidy_both %&gt;% mutate(word = str_extract(word, &quot;[a-z]+&quot;)) %&gt;% count(author, word) %&gt;% rename(other = n) %&gt;% inner_join(count(tidy_books, word)) %&gt;% rename(Austen = n) %&gt;% mutate(other = other / sum(other), Austen = Austen / sum(Austen)) %&gt;% ungroup() We use str_extract here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (like italics). The tokenizer treated these as words but we don’t want to count “_any_” separately from “any”. Now let’s plot. library(scales) ggplot(frequency, aes(x = other, y = Austen, color = abs(Austen - other))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.4, height = 0.4) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~author, ncol = 2) + theme(legend.position=&quot;none&quot;) + labs(y = &quot;Jane Austen&quot;, x = NULL) Words that are close to the line in these plots have similar frequencies in both sets of texts, for example, in both Austen and Brontë texts (“miss”, “time”, “lady”, “day” at the upper frequency end) or in both Austen and Wells texts (“time”, “day”, “mind”, “brother” at the high frequency end). Words that are far from the line are words that are found more in one set of texts than another. For example, in the Austen-Brontë plot, words like “elizabeth”, “emma”, “captain”, and “bath” (all proper nouns) are found in Austen’s texts but not much in the Brontë texts, while words like “arthur”, “dark”, “dog”, and “doctor” are found in the Brontë texts but not the Austen texts. In comparing H.G. Wells with Jane Austen, Wells uses words like “beast”, “guns”, “brute”, and “animal” that Austen does not, while Austen uses words like “family”, “friend”, “letter”, and “agreeable” that Wells does not. Overall, notice that the words in the Austen-Brontë plot are closer to the zero-slope line than in the Austen-Wells plot and also extend to lower frequencies; Austen and the Brontë sisters use more similar words than Austen and H.G. Wells. Also, we might notice the percent frequencies for individual words are different in one plot when compared to another because of the inner join; not all the words are found in all three sets of texts so the percent frequency is a different quantity. Let’s quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Brontë sisters, and between Austen and Wells? cor.test(data = frequency[frequency$author == &quot;Brontë Sisters&quot;,], ~ other + Austen) ## ## Pearson&#39;s product-moment correlation ## ## data: other and Austen ## t = 122.51, df = 10620, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7572588 0.7730232 ## sample estimates: ## cor ## 0.7652557 cor.test(data = frequency[frequency$author == &quot;H.G. Wells&quot;,], ~ other + Austen) ## ## Pearson&#39;s product-moment correlation ## ## data: other and Austen ## t = 36.058, df = 5962, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4020611 0.4437385 ## sample estimates: ## cor ## 0.4231236 The relationship between the word frequencies is different between these sets of texts, as it appears in the plots. References "],
["sentiment.html", "3 Sentiment analysis with tidy data 3.1 The sentiments dataset 3.2 Sentiment analysis with inner join 3.3 Most common positive and negative words 3.4 Wordclouds 3.5 Looking at units beyond just words", " 3 Sentiment analysis with tidy data 3.1 The sentiments dataset There are a variety of methods and dictionaries that exist for evaluating the opinion or emotion in text. The tidytext package contains three sentiment lexicons in the sentiments dataset. library(tidytext) sentiments ## # A tibble: 23,165 × 4 ## word sentiment lexicon score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 abacus trust nrc NA ## 2 abandon fear nrc NA ## 3 abandon negative nrc NA ## 4 abandon sadness nrc NA ## 5 abandoned anger nrc NA ## 6 abandoned fear nrc NA ## 7 abandoned negative nrc NA ## 8 abandoned sadness nrc NA ## 9 abandonment anger nrc NA ## 10 abandonment fear nrc NA ## # ... with 23,155 more rows The three lexicons are AFINN from Finn Årup Nielsen, bing from Bing Liu and collaborators, and nrc from Saif Mohammad and Peter Turney. All three of these lexicons are based on unigrams (or single words). These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset. These dictionary-based methods find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text. Not every English word is in the lexicons because many English words are pretty neutral. It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only. For many kinds of text (like the narrative examples below), there are not sustained sections of sarcasm or negated text, so this is not an important effect. One last caveat is that the size of the chunk of text that we add up unigram sentiment scores for can have an important effect for an analysis. A paragraph-sized text can often have positive and negative sentiment averaged out to about zero, while sentence-sized text often works better. 3.2 Sentiment analysis with inner join With data in a tidy format, sentiment analysis can be done as an inner join. Let’s look at the words with a joy score from the NRC lexicon. What are the most common joy words in Emma? library(janeaustenr) library(dplyr) library(stringr) tidy_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(word, text) nrcjoy &lt;- sentiments %&gt;% filter(lexicon == &quot;nrc&quot;, sentiment == &quot;joy&quot;) tidy_books %&gt;% filter(book == &quot;Emma&quot;) %&gt;% semi_join(nrcjoy) %&gt;% count(word, sort = TRUE) ## # A tibble: 303 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 good 359 ## 2 young 192 ## 3 friend 166 ## 4 hope 143 ## 5 happy 125 ## 6 love 117 ## 7 deal 92 ## 8 found 92 ## 9 present 89 ## 10 kind 82 ## # ... with 293 more rows Or instead we could examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel. library(tidyr) bing &lt;- sentiments %&gt;% filter(lexicon == &quot;bing&quot;) %&gt;% select(-score) janeaustensentiment &lt;- tidy_books %&gt;% inner_join(bing) %&gt;% count(book, index = linenumber %/% 80, sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative) Now we can plot these sentiment scores across the plot trajectory of each novel. library(ggplot2) ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) We can see here how the plot of each novel changes toward more positive or negative sentiment over the trajectory of the story. TODO: The three different methods of calculating sentiment give results that are different in an absolute sense but have similar relative trajectories through the novel. 3.3 Most common positive and negative words One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment. bing_word_counts &lt;- tidy_books %&gt;% inner_join(bing) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts ## # A tibble: 2,585 × 3 ## word sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 miss negative 1855 ## 2 well positive 1523 ## 3 good positive 1380 ## 4 great positive 981 ## 5 like positive 725 ## 6 better positive 639 ## 7 enough positive 613 ## 8 happy positive 534 ## 9 love positive 495 ## 10 pleasure positive 462 ## # ... with 2,575 more rows This can be shown visually, and we can pipe straight into ggplot2, if we like, because of the way we are consistently using tools built for handling tidy data frames. bing_word_counts %&gt;% filter(n &gt; 150) %&gt;% mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() This lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using bind_rows. 3.4 Wordclouds We’ve seen that this tidy text mining approach works well with ggplot2, but having our data in a tidy format is useful for other plots as well. For example, consider the wordcloud package. Let’s look at the most common words in Jane Austen’s works as a whole again. library(wordcloud) tidy_books %&gt;% anti_join(stop_words) %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100)) In other functions, such as comparison.cloud, you may need to turn the data frame into a matrix with reshape2’s acast. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud, this can all be done with joins, piping, and dplyr because our data is in tidy format. library(reshape2) tidy_books %&gt;% inner_join(bing) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;#F8766D&quot;, &quot;#00BFC4&quot;), max.words = 100) 3.5 Looking at units beyond just words Lots of useful work can be done by tokenizing at the word level, but sometimes it is useful or necessary to look at different units of text. For example, some sentiment analysis algorithms look beyond only unigrams (i.e. single words) to try to understand the sentiment of a sentence as a whole. These algorithms try to understand that I am not having a good day. is a sad sentence, not a happy one, because of negation. The Stanford CoreNLP tools and the sentimentr R package (currently available on Github but not CRAN) are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences. PandP_sentences &lt;- data_frame(text = prideprejudice) %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) Let’s look at just one. PandP_sentences$sentence[2] ## [1] &quot;however little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.&quot; The sentence tokenizing does seem to have a bit of trouble with UTF-8 encoded text, especially with sections of dialogue; it does much better with punctuation in ASCII. One possibility, if this is important, is to try using iconv(), with something like iconv(text, to = 'latin1') in a mutate statement before unnesting. Another option in unnest_tokens is to split into tokens using a regex pattern. We could use this, for example, to split the text of Jane Austen’s novels into a data frame by chapter. austen_chapters &lt;- austen_books() %&gt;% mutate(book = factor(book, levels = unique(book))) %&gt;% group_by(book) %&gt;% unnest_tokens(chapter, text, token = &quot;regex&quot;, pattern = &quot;Chapter|CHAPTER [\\\\dIVXLC]&quot;) %&gt;% ungroup() austen_chapters %&gt;% group_by(book) %&gt;% summarise(chapters = n()) ## # A tibble: 6 × 2 ## book chapters ## &lt;fctr&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility 51 ## 2 Pride &amp; Prejudice 62 ## 3 Mansfield Park 49 ## 4 Emma 56 ## 5 Northanger Abbey 32 ## 6 Persuasion 25 We have recovered the correct number of chapters in each novel (plus an “extra” row for each novel title). In this data frame, each row corresponds to one chapter. Near the beginning of this vignette, we used a similar regex to find where all the chapters were in Austen’s novels for a tidy data frame organized by one-word-per-row. We can use tidy text analysis to ask questions such as what are the most negative chapters in each of Jane Austen’s novels? First, let’s get the list of negative words from the Bing lexicon. Second, let’s make a dataframe of how many words are in each chapter so we can normalize for the length of chapters. Then, let’s find the number of negative words in each chapter and divide by the total words in each chapter. Which chapter has the highest proportion of negative words? bingnegative &lt;- sentiments %&gt;% filter(lexicon == &quot;bing&quot;, sentiment == &quot;negative&quot;) wordcounts &lt;- tidy_books %&gt;% group_by(book, chapter) %&gt;% summarize(words = n()) tidy_books %&gt;% mutate(book = factor(book, levels = unique(book))) %&gt;% semi_join(bingnegative) %&gt;% group_by(book, chapter) %&gt;% summarize(negativewords = n()) %&gt;% left_join(wordcounts, by = c(&quot;book&quot;, &quot;chapter&quot;)) %&gt;% mutate(ratio = negativewords/words) %&gt;% filter(chapter != 0) %&gt;% top_n(1) %&gt;% ungroup ## # A tibble: 6 × 5 ## book chapter negativewords words ratio ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Sense &amp; Sensibility 43 161 3405 0.04728341 ## 2 Pride &amp; Prejudice 34 111 2104 0.05275665 ## 3 Mansfield Park 46 173 3685 0.04694708 ## 4 Emma 15 151 3340 0.04520958 ## 5 Northanger Abbey 21 149 2982 0.04996647 ## 6 Persuasion 4 62 1807 0.03431101 These are the chapters with the most sad words in each book, normalized for number of words in the chapter. What is happening in these chapters? In Chapter 43 of Sense and Sensibility Marianne is seriously ill, near death, and in Chapter 34 of Pride and Prejudice Mr. Darcy proposes for the first time (so badly!). Chapter 46 of Mansfield Park is almost the end, when everyone learns of Henry’s scandalous adultery, Chapter 15 of Emma is when horrifying Mr. Elton proposes, and in Chapter 21 of Northanger Abbey Catherine is deep in her Gothic faux fantasy of murder, etc. Chapter 4 of Persuasion is when the reader gets the full flashback of Anne refusing Captain Wentworth and how sad she was and what a terrible mistake she realized it to be. "],
["tfidf.html", "4 Analyzing word and document frequency: tf-idf 4.1 Term frequency and inverse document frequency 4.2 Term frequency in Jane Austen’s novels 4.3 The bind_tf_idf function 4.4 A corpus of physics texts", " 4 Analyzing word and document frequency: tf-idf A central question in text mining and natural language processing is how to quantify what a document is about. Can we do this by looking at the words that make up the document? One measure of how important a word may be is its term frequency (tf), how frequently a word occurs in a document; we have examined how to measure word frequency in Chapter 2. There are words in a document, however, that occur many times but may not be important; in English, these are probably words like “the”, “is”, “of”, and so forth. We might take the approach of adding words like these to a list of stop words and removing them before analysis, but it is possible that some of these words might be more important in some documents than others. A list of stop words is not a very sophisticated approach to adjusting term frequency for commonly used words. 4.1 Term frequency and inverse document frequency Another approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf, the frequency of a term adjusted for how rarely it is used. It is intended to measure how important a word is to a document in a collection (or corpus) of documents. It is a rule-of-thumb or heuristic quantity; while it has proved useful in text mining, search engines, etc., its theoretical foundations are considered less than firm by information theory experts. The inverse document frequency for any given term is defined as \\[idf(\\text{term}) = \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)}\\] We can use tidy data principles, as described in Chapter 2, to approach tf-idf analysis and use consistent, effective tools to quantify how important various terms are in a document that is part of a collection. 4.2 Term frequency in Jane Austen’s novels Let’s start by looking at the published novels of Jane Austen and examine first term frequency, then tf-idf. We can start just by using dplyr verbs such as group_by and join. What are the most commonly used words in Jane Austen’s novels? (Let’s also calculate the total words in each novel here, for later use.) library(dplyr) library(janeaustenr) library(tidytext) book_words &lt;- austen_books() %&gt;% unnest_tokens(word, text) %&gt;% count(book, word, sort = TRUE) %&gt;% ungroup() total_words &lt;- book_words %&gt;% group_by(book) %&gt;% summarize(total = sum(n)) book_words &lt;- left_join(book_words, total_words) book_words ## # A tibble: 40,379 × 4 ## book word n total ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Mansfield Park the 6206 160460 ## 2 Mansfield Park to 5475 160460 ## 3 Mansfield Park and 5438 160460 ## 4 Emma to 5239 160996 ## 5 Emma the 5201 160996 ## 6 Emma and 4896 160996 ## 7 Mansfield Park of 4778 160460 ## 8 Pride &amp; Prejudice the 4331 122204 ## 9 Emma of 4291 160996 ## 10 Pride &amp; Prejudice to 4162 122204 ## # ... with 40,369 more rows The usual suspects are here, “the”, “and”, “to”, and so forth. Let’s look at the distribution of n/total for each novel, the number of times a word appears in a novel divided by the total number of terms (words) in that novel. This is exactly what term frequency is. library(ggplot2) ggplot(book_words, aes(n/total, fill = book)) + geom_histogram(alpha = 0.8, show.legend = FALSE) + xlim(NA, 0.0009) + labs(title = &quot;Term Frequency Distribution in Jane Austen&#39;s Novels&quot;) + facet_wrap(~book, ncol = 2, scales = &quot;free_y&quot;) There are very long tails to the right for these novels (those extremely common words!) that we have not shown in these plots. These plots exhibit similar distributions for all the novels, with many words that occur rarely and fewer words that occur frequently. 4.3 The bind_tf_idf function The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the group of Jane Austen’s novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. Let’s do that now. book_words &lt;- book_words %&gt;% bind_tf_idf(word, book, n) book_words ## # A tibble: 40,379 × 7 ## book word n total tf idf tf_idf ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mansfield Park the 6206 160460 0.03867631 0 0 ## 2 Mansfield Park to 5475 160460 0.03412065 0 0 ## 3 Mansfield Park and 5438 160460 0.03389007 0 0 ## 4 Emma to 5239 160996 0.03254118 0 0 ## 5 Emma the 5201 160996 0.03230515 0 0 ## 6 Emma and 4896 160996 0.03041069 0 0 ## 7 Mansfield Park of 4778 160460 0.02977689 0 0 ## 8 Pride &amp; Prejudice the 4331 122204 0.03544074 0 0 ## 9 Emma of 4291 160996 0.02665284 0 0 ## 10 Pride &amp; Prejudice to 4162 122204 0.03405780 0 0 ## # ... with 40,369 more rows Notice that idf and thus tf-idf are zero for these extremely common words. These are all words that appear in all six of Jane Austen’s novels, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection. Let’s look at terms with high tf-idf in Jane Austen’s works. book_words %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 40,379 × 6 ## book word n tf idf tf_idf ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sense &amp; Sensibility elinor 623 0.005193528 1.791759 0.009305552 ## 2 Sense &amp; Sensibility marianne 492 0.004101470 1.791759 0.007348847 ## 3 Mansfield Park crawford 493 0.003072417 1.791759 0.005505032 ## 4 Pride &amp; Prejudice darcy 373 0.003052273 1.791759 0.005468939 ## 5 Persuasion elliot 254 0.003036207 1.791759 0.005440153 ## 6 Emma emma 786 0.004882109 1.098612 0.005363545 ## 7 Northanger Abbey tilney 196 0.002519928 1.791759 0.004515105 ## 8 Emma weston 389 0.002416209 1.791759 0.004329266 ## 9 Pride &amp; Prejudice bennet 294 0.002405813 1.791759 0.004310639 ## 10 Persuasion wentworth 191 0.002283132 1.791759 0.004090824 ## # ... with 40,369 more rows Here we see all proper nouns, names that are in fact important in these novels. None of them occur in all of novels, and they are important, characteristic words for each text. Some of the values for idf are the same for different terms because there are 6 documents in this corpus and we are seeing the numerical value for \\(\\ln(6/1)\\), \\(\\ln(6/2)\\), etc. Let’s look at a visualization for these high tf-idf words. plot_austen &lt;- book_words %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) ggplot(plot_austen[1:20,], aes(word, tf_idf, fill = book)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;) + labs(title = &quot;Highest tf-idf words in Jane Austen&#39;s Novels&quot;, x = NULL, y = &quot;tf-idf&quot;) + coord_flip() Let’s look at the novels individually. plot_austen &lt;- plot_austen %&gt;% group_by(book) %&gt;% top_n(15) %&gt;% ungroup ggplot(plot_austen, aes(word, tf_idf, fill = book)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + labs(title = &quot;Highest tf-idf words in Jane Austen&#39;s Novels&quot;, x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~book, ncol = 2, scales = &quot;free&quot;) + coord_flip() Still all proper nouns! These words are, as measured by tf-idf, the most important to each novel and most readers would likely agree. 4.4 A corpus of physics texts Let’s work with another corpus of documents, to see what terms are important in a different set of works. In fact, let’s leave the world of fiction and narrative entirely. Let’s download some classic physics texts from Project Gutenberg and see what terms are important in these works, as measured by tf-idf. Let’s download Discourse on Floating Bodies by Galileo Galilei, Treatise on Light by Christiaan Huygens, Experiments with Alternate Currents of High Potential and High Frequency by Nikola Tesla, and Relativity: The Special and General Theory by Albert Einstein. This is a pretty diverse bunch. They may all be physics classics, but they were written across a 300-year timespan, and some of them were first written in other languages and then translated to English. Perfectly homogeneous these are not, but that doesn’t stop this from being an interesting exercise! library(gutenbergr) physics &lt;- gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields = &quot;author&quot;) physics_words &lt;- physics %&gt;% unnest_tokens(word, text) %&gt;% count(author, word, sort = TRUE) %&gt;% ungroup() physics_words ## # A tibble: 12,592 × 3 ## author word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Galilei, Galileo the 3760 ## 2 Tesla, Nikola the 3604 ## 3 Huygens, Christiaan the 3553 ## 4 Einstein, Albert the 2994 ## 5 Galilei, Galileo of 2049 ## 6 Einstein, Albert of 2030 ## 7 Tesla, Nikola of 1737 ## 8 Huygens, Christiaan of 1708 ## 9 Huygens, Christiaan to 1207 ## 10 Tesla, Nikola a 1176 ## # ... with 12,582 more rows Here we see just the raw counts, and of course these documents are all different lengths. Let’s go ahead and calculate tf-idf. physics_words &lt;- physics_words %&gt;% bind_tf_idf(word, author, n) plot_physics &lt;- physics_words %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% mutate(author = factor(author, levels = c(&quot;Galilei, Galileo&quot;, &quot;Huygens, Christiaan&quot;, &quot;Tesla, Nikola&quot;, &quot;Einstein, Albert&quot;))) ggplot(plot_physics[1:20,], aes(word, tf_idf, fill = author)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;) + labs(title = &quot;Highest tf-idf words in Classic Physics Texts&quot;, x = NULL, y = &quot;tf-idf&quot;) + coord_flip() Nice! Let’s look at each text individually. plot_physics &lt;- plot_physics %&gt;% group_by(author) %&gt;% top_n(15, tf_idf) %&gt;% mutate(word = reorder(word, tf_idf)) ggplot(plot_physics, aes(word, tf_idf, fill = author)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + labs(title = &quot;Highest tf-idf words in Classic Physics Texts&quot;, x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~author, ncol = 2, scales = &quot;free&quot;) + coord_flip() Very interesting indeed. One thing we see here is “gif” in the Einstein text?! grep(&quot;gif&quot;, physics$text, value = TRUE)[1:10] ## [1] &quot; Fig. 01: file fig01.gif&quot; ## [2] &quot; eq. 1: file eq01.gif&quot; ## [3] &quot; eq. 2: file eq02.gif&quot; ## [4] &quot; eq. 3: file eq03.gif&quot; ## [5] &quot; eq. 4: file eq04.gif&quot; ## [6] &quot; eq. 05a: file eq05a.gif&quot; ## [7] &quot; eq. 05b: file eq05b.gif&quot; ## [8] &quot; eq. 07: file eq07.gif&quot; ## [9] &quot; eq. 08: file eq08.gif&quot; ## [10] &quot; eq. 09: file eq09.gif&quot; Some cleaning up of the text might be in order. The same thing is true for “eq”, obviously here. “K1” is the name of a coordinate system for Einstein: grep(&quot;K1&quot;, physics$text, value = TRUE)[1] ## [1] &quot;to a second co-ordinate system K1 provided that the latter is&quot; Also notice that in this line we have “co-ordinate”, which explains why there are separate “co” and “ordinate” items in the high tf-idf words for the Einstein text. “AB”, “RC”, and so forth are names of rays, circles, angles, and so forth for Huygens. grep(&quot;AK&quot;, physics$text, value = TRUE)[1] ## [1] &quot;Now let us assume that the ray has come from A to C along AK, KC; the&quot; Let’s remove some of these less meaningful words to make a better, more meaningful plot. Notice that we make a custom list of stop words and use anti_join to remove them. mystopwords &lt;- data_frame(word = c(&quot;gif&quot;, &quot;eq&quot;, &quot;co&quot;, &quot;rc&quot;, &quot;ac&quot;, &quot;ak&quot;, &quot;bn&quot;, &quot;fig&quot;, &quot;file&quot;, &quot;cg&quot;, &quot;cb&quot;)) physics_words &lt;- anti_join(physics_words, mystopwords, by = &quot;word&quot;) plot_physics &lt;- physics_words %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(author) %&gt;% top_n(15, tf_idf) %&gt;% ungroup %&gt;% mutate(author = factor(author, levels = c(&quot;Galilei, Galileo&quot;, &quot;Huygens, Christiaan&quot;, &quot;Tesla, Nikola&quot;, &quot;Einstein, Albert&quot;))) ggplot(plot_physics, aes(word, tf_idf, fill = author)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + labs(title = &quot;Highest tf-idf words in Classic Physics Texts&quot;, x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~author, ncol = 2, scales = &quot;free&quot;) + coord_flip() We don’t hear enough about ramparts or things being ethereal in physics today. "],
["ngrams.html", "5 Working with combinations of words using n-grams and widyr 5.1 Tokenizing by n-gram 5.2 Counting and correlating pairs of words with the widyr package", " 5 Working with combinations of words using n-grams and widyr So far we’ve considered words as individual units, and connected them to documents or sentiments. However, many interesting text analyses are based on the relationships between words, whether examining words commonly used in proximity to each other or within the same documents. Here, we’ll explore some of the tools tidytext offers for determining relationships between words in your text corpus. We’ll also introduce the widyr package, which is useful for calculating pairwise correlations and distances within a tidy format. 5.1 Tokenizing by n-gram We’ve been using the unnest_tokens function to tokenize by word, or sometimes by sentence or paragraph. But we can also tokenize into consecutive sequences of words, called n-grams. We do this by adding the token = &quot;ngrams&quot; option to unnest_tokens(), as well as the n argument. When we set n to 2, we are examining pairs of two words, often called bigrams: library(dplyr) library(tidytext) library(janeaustenr) austen_bigrams &lt;- austen_books() %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) austen_bigrams ## # A tibble: 725,048 × 2 ## book bigram ## &lt;fctr&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility sense and ## 2 Sense &amp; Sensibility and sensibility ## 3 Sense &amp; Sensibility sensibility by ## 4 Sense &amp; Sensibility by jane ## 5 Sense &amp; Sensibility jane austen ## 6 Sense &amp; Sensibility austen 1811 ## 7 Sense &amp; Sensibility 1811 chapter ## 8 Sense &amp; Sensibility chapter 1 ## 9 Sense &amp; Sensibility 1 the ## 10 Sense &amp; Sensibility the family ## # ... with 725,038 more rows This data structure is still a variation of the tidy text format. It is structured as one-row-per-token, but now each token represents a bigram. Notice that these bigrams are overlapping: “sense and” is one token, while “and sensibility” is another. 5.1.1 Counting and filtering n-grams Our usual tidy tools apply equally well to n-gram analysis. We can examine the most common bigrams using count: austen_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 211,237 × 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 of the 3017 ## 2 to be 2787 ## 3 in the 2368 ## 4 it was 1781 ## 5 i am 1545 ## 6 she had 1472 ## 7 of her 1445 ## 8 to the 1387 ## 9 she was 1377 ## 10 had been 1299 ## # ... with 211,227 more rows As expected, a lot of them are pairs of common (relatively uninteresting) words. This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, which we can remove stop-words from individually: library(tidyr) bigrams_separated &lt;- austen_bigrams %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) bigrams_filtered &lt;- bigrams_separated %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) bigrams_filtered ## # A tibble: 44,784 × 3 ## book word1 word2 ## &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility jane austen ## 2 Sense &amp; Sensibility austen 1811 ## 3 Sense &amp; Sensibility 1811 chapter ## 4 Sense &amp; Sensibility chapter 1 ## 5 Sense &amp; Sensibility norland park ## 6 Sense &amp; Sensibility surrounding acquaintance ## 7 Sense &amp; Sensibility late owner ## 8 Sense &amp; Sensibility advanced age ## 9 Sense &amp; Sensibility constant companion ## 10 Sense &amp; Sensibility happened ten ## # ... with 44,774 more rows We can now count the most common pairs of words: bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) ## Source: local data frame [33,421 x 3] ## Groups: word1 [6,711] ## ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 sir thomas 287 ## 2 miss crawford 215 ## 3 captain wentworth 170 ## 4 miss woodhouse 162 ## 5 frank churchill 132 ## 6 lady russell 118 ## 7 lady bertram 114 ## 8 sir walter 113 ## 9 miss fairfax 109 ## 10 colonel brandon 108 ## # ... with 33,411 more rows We can see that names (whether first and last or with a salutation) are the most common pairs in Jane Austen books. We may want to work with the recombined words. tidyr’s unite() function is the inverse of separate(), and lets us recombine the columns into one. bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united ## # A tibble: 44,784 × 2 ## book bigram ## * &lt;fctr&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility jane austen ## 2 Sense &amp; Sensibility austen 1811 ## 3 Sense &amp; Sensibility 1811 chapter ## 4 Sense &amp; Sensibility chapter 1 ## 5 Sense &amp; Sensibility norland park ## 6 Sense &amp; Sensibility surrounding acquaintance ## 7 Sense &amp; Sensibility late owner ## 8 Sense &amp; Sensibility advanced age ## 9 Sense &amp; Sensibility constant companion ## 10 Sense &amp; Sensibility happened ten ## # ... with 44,774 more rows You could also easily work with trigrams (sequences of 3 words) by setting n = 3: austen_books() %&gt;% unnest_tokens(trigram, text, token = &quot;ngrams&quot;, n = 3) %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% count(word1, word2, word3, sort = TRUE) ## Source: local data frame [8,757 x 4] ## Groups: word1, word2 [7,462] ## ## word1 word2 word3 n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 dear miss woodhouse 23 ## 2 miss de bourgh 18 ## 3 lady catherine de 14 ## 4 catherine de bourgh 13 ## 5 poor miss taylor 11 ## 6 sir walter elliot 11 ## 7 ten thousand pounds 11 ## 8 dear sir thomas 10 ## 9 twenty thousand pounds 8 ## 10 replied miss crawford 7 ## # ... with 8,747 more rows 5.1.2 Analyzing bigrams A bigram can be treated as a term in a document in the same way that we treated individual words. For example, we can look at tf-idf of bigrams: bigram_tf_idf &lt;- bigrams_united %&gt;% count(book, bigram) %&gt;% bind_tf_idf(bigram, book, n) %&gt;% arrange(desc(tf_idf)) bigram_tf_idf ## Source: local data frame [36,217 x 6] ## Groups: book [6] ## ## book bigram n tf idf tf_idf ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Persuasion captain wentworth 170 0.02985599 1.791759 0.05349475 ## 2 Mansfield Park sir thomas 287 0.02873160 1.791759 0.05148012 ## 3 Mansfield Park miss crawford 215 0.02152368 1.791759 0.03856525 ## 4 Persuasion lady russell 118 0.02072357 1.791759 0.03713165 ## 5 Persuasion sir walter 113 0.01984545 1.791759 0.03555828 ## 6 Emma miss woodhouse 162 0.01700966 1.791759 0.03047722 ## 7 Northanger Abbey miss tilney 82 0.01594400 1.791759 0.02856782 ## 8 Sense &amp; Sensibility colonel brandon 108 0.01502086 1.791759 0.02691377 ## 9 Emma frank churchill 132 0.01385972 1.791759 0.02483329 ## 10 Pride &amp; Prejudice lady catherine 100 0.01380453 1.791759 0.02473439 ## # ... with 36,207 more rows This can be visualized within each book, just as we did for words: Much as we discovered in Chapter 4, the units that distinguish each Austen book are almost exclusively names. 5.1.3 Using bigrams to provide context in sentiment analysis Our sentiment analysis approch in Chapter 3 simply counted the appearance of positive or negative words, according to a reference lexicon. One of the problems with this approach is that a word’s context can matter nearly as much as its presence. For example, the words “happy” and “like” will be counted as positive, even in a sentence like “I’m not happy and I don’t like it!” bigrams_separated %&gt;% filter(word1 == &quot;not&quot;) %&gt;% count(word1, word2, sort = TRUE) ## Source: local data frame [1,246 x 3] ## Groups: word1 [1] ## ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 not be 610 ## 2 not to 355 ## 3 not have 327 ## 4 not know 252 ## 5 not a 189 ## 6 not think 176 ## 7 not been 160 ## 8 not the 147 ## 9 not at 129 ## 10 not in 118 ## # ... with 1,236 more rows By performing sentiment analysis on the bigram data, we can examine how often sentiment-associated words are preceded by “not” or other negating words. We could use this to ignore or even reverse their contribution to the sentiment score. Let’s use the AFINN lexicon for sentiment analysis, which gives a numeric sentiment score for each word: AFINN &lt;- sentiments %&gt;% filter(lexicon == &quot;AFINN&quot;) %&gt;% select(word, score) AFINN ## # A tibble: 2,476 × 2 ## word score ## &lt;chr&gt; &lt;int&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # ... with 2,466 more rows We can then examine the most frequent words that were preceded by “not” and were associated with a sentiment. not_words &lt;- bigrams_separated %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% count(word2, score, sort = TRUE) %&gt;% ungroup() not_words ## # A tibble: 245 × 3 ## word2 score n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 like 2 99 ## 2 help 2 82 ## 3 want 1 45 ## 4 wish 1 39 ## 5 allow 1 36 ## 6 care 2 23 ## 7 sorry -1 21 ## 8 leave -1 18 ## 9 pretend -1 18 ## 10 worth 2 17 ## # ... with 235 more rows It’s worth asking which words contributed the most in the “wrong” direction. To compute that, we can multiply their score by the number of times they appear (so that a word with a sentiment score of +3 occurring 10 times has as much impact as a word with a sentiment score of +1 occurring 30 times). not_words %&gt;% mutate(contribution = n * score) %&gt;% arrange(desc(abs(contribution))) %&gt;% head(20) %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% ggplot(aes(word2, n * score, fill = n * score &gt; 0)) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + ylab(&quot;Words preceded by \\&quot;not\\&quot;&quot;) + xlab(&quot;Sentiment score * # of occurrences&quot;) + coord_flip() The bigrams “not like” and “not help” were overwhelmingly the largest causes of misidentification, making the text seem much more positive than it is. But we can see phrases like “not afraid” and “not fail” sometimes suggest text is more negative than it is. “Not” isn’t the only word that provides context. We could make a vector of words that we suspect are used in negation, and use the same joining and counting approach to examine all of them at once. negation_words &lt;- c(&quot;not&quot;, &quot;no&quot;, &quot;never&quot;, &quot;without&quot;) negated_words &lt;- bigrams_separated %&gt;% filter(word1 %in% negation_words) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% count(word1, word2, score, sort = TRUE) %&gt;% ungroup() negated_words ## # A tibble: 531 × 4 ## word1 word2 score n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 no doubt -1 102 ## 2 not like 2 99 ## 3 not help 2 82 ## 4 no no -1 60 ## 5 not want 1 45 ## 6 not wish 1 39 ## 7 not allow 1 36 ## 8 not care 2 23 ## 9 no harm -2 22 ## 10 not sorry -1 21 ## # ... with 521 more rows ## TODO: make the ordering vary depending on each facet ## (not easy to fix) negated_words %&gt;% mutate(contribution = n * score) %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% group_by(word1) %&gt;% top_n(10, abs(contribution)) %&gt;% ggplot(aes(word2, contribution, fill = n * score &gt; 0)) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ word1, scales = &quot;free&quot;) + xlab(&quot;Words preceded by negation&quot;) + ylab(&quot;Sentiment score * # of occurrences&quot;) + coord_flip() 5.1.4 Visualizing a network of bigrams with igraph We may be interested in visualizing all of the relationships among words simultaneously, rather than just the top few at a time. bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) bigram_counts ## Source: local data frame [33,421 x 3] ## Groups: word1 [6,711] ## ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 sir thomas 287 ## 2 miss crawford 215 ## 3 captain wentworth 170 ## 4 miss woodhouse 162 ## 5 frank churchill 132 ## 6 lady russell 118 ## 7 lady bertram 114 ## 8 sir walter 113 ## 9 miss fairfax 109 ## 10 colonel brandon 108 ## # ... with 33,411 more rows As one powerful visualization, we can arrange the words into a network, or “graph.” Here we’ll be referring to a “graph” not in the sense of a visualization, but as a combination of connected nodes. A graph can be created from a tidy object because a graph has three variables: from: the node an edge is coming from to: the node an edge is going towards weight A numeric value associated with each edge The igraph package has many powerful functions for manipulating and analyzing networks. The most typical way to create an igraph object from tidy data is the graph_from_data_frame() function. library(igraph) # filter for only relatively common combinations bigram_graph &lt;- bigram_counts %&gt;% filter(n &gt; 20) %&gt;% graph_from_data_frame() bigram_graph ## IGRAPH DN-- 91 77 -- ## + attr: name (v/c), n (e/n) ## + edges (vertex names): ## [1] sir -&gt;thomas miss -&gt;crawford captain -&gt;wentworth miss -&gt;woodhouse ## [5] frank -&gt;churchill lady -&gt;russell lady -&gt;bertram sir -&gt;walter ## [9] miss -&gt;fairfax colonel -&gt;brandon miss -&gt;bates lady -&gt;catherine ## [13] sir -&gt;john jane -&gt;fairfax miss -&gt;tilney lady -&gt;middleton ## [17] miss -&gt;bingley thousand-&gt;pounds miss -&gt;dashwood miss -&gt;bennet ## [21] john -&gt;knightley miss -&gt;morland captain -&gt;benwick dear -&gt;miss ## [25] miss -&gt;smith miss -&gt;crawford&#39;s henry -&gt;crawford miss -&gt;elliot ## [29] dr -&gt;grant miss -&gt;bertram sir -&gt;thomas&#39;s ten -&gt;minutes ## + ... omitted several edges igraph has plotting functions built in, but they’re not what the package is designed to do. Many others have developed visualization methods for graphs. We recommend the ggraph package, because it implements these visualizations in terms of the grammar of graphics, which we are already familiar with from ggplot2. We can convert an igraph object into a ggraph with the ggraph function, after which we add layers to it, much like layers are added in ggplot2. For example, here we add nodes, edges, and text to construct the basics of a graph: library(ggraph) set.seed(2016) ggraph(bigram_graph, layout = &quot;fr&quot;) + geom_edge_link() + geom_node_point() + geom_node_text(aes(label = name), vjust = 1, hjust = 1) We now see more details of the network structure. For example, we see that salutations such as “miss”, “lady”, “sir”, “and”colonel&quot; form common centers of nodes, which are often followed by names. We also see pairs or triplets along the outside that form common short phrases (“half hour,” “ten minutes”, “thousand pounds”). As a few polishing operations: We add the edge_alpha aesthetic to the link layer to make links transparent based on how common or rare the bigram is We add directionality with an arrow, constructed using grid::arrow() We tinker with the options to the node layer to make the nodes more attractive (larger, blue points) We add a theme that’s useful for plotting networks, theme_void() set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) ggraph(bigram_graph, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() It may take a some experimentation with ggraph to get your networks into a presentable format like this, but the network structure is useful and flexible way to visualize relational tidy data. 5.1.5 Visualizing bigrams in other texts We went to a good amount of work in cleaning and visualizing bigrams on a text dataset. So let’s collect it into a function so that we can do it on other text datasets easily. count_bigrams &lt;- function(dataset) { dataset %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) %&gt;% count(word1, word2, sort = TRUE) } visualize_bigrams &lt;- function(bigrams) { set.seed(2016) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } At this point, we could visualize bigrams in other works, such as the King James Version of the Bible: # The King James version is book 10 on Project Gutenberg: library(gutenbergr) kjv &lt;- gutenberg_download(10) library(stringr) kjv_bigrams &lt;- kjv %&gt;% count_bigrams() kjv_bigrams ## Source: local data frame [47,551 x 3] ## Groups: word1 [7,265] ## ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 thou shalt 1250 ## 2 thou hast 768 ## 3 lord god 546 ## 4 thy god 356 ## 5 thou art 320 ## 6 lord thy 316 ## 7 lord hath 291 ## 8 shalt thou 258 ## 9 jesus christ 196 ## 10 burnt offering 184 ## # ... with 47,541 more rows # filter out rare combinations, as well as digits set.seed(2016) kjv_bigrams %&gt;% filter(n &gt; 40, !str_detect(word1, &quot;\\\\d&quot;), !str_detect(word2, &quot;\\\\d&quot;)) %&gt;% visualize_bigrams() TODO: Description of bible network 5.2 Counting and correlating pairs of words with the widyr package Tokenizing by n-gram is a useful way to explore pairs of adjacent words. However, we may also be interested in words that tend to co-occur within particular documents or particular chapters. Tidy data is a useful structure for comparing between variables or grouping by rows, but it can be challenging to compare between rows: for example, to count the number of times that two words appear within the same document. This is provided by the widyr package, which focuses on encapsulating the pattern of “widen data, perform an operation, then re-tidy data.” The philosophy behind the widyr package, which can operations such as counting and correlating on pairs of values in a tidy dataset. This makes certain operations for comparing words much easier. We’ll focus on a set of functions that make pairwise comparisons between groups of observations (for example, between documents, or sections). 5.2.1 Counting and correlating among sections Consider the book “Pride and Prejudice” divided into 10-line sections, as we did for sentiment analysis in Chapter 3. We may be interested in what words tend to appear within the same section. austen_section_words &lt;- austen_books() %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% mutate(section = row_number() %/% 10) %&gt;% filter(section &gt; 0) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) austen_section_words ## # A tibble: 37,240 × 3 ## book section word ## &lt;fctr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Pride &amp; Prejudice 1 truth ## 2 Pride &amp; Prejudice 1 universally ## 3 Pride &amp; Prejudice 1 acknowledged ## 4 Pride &amp; Prejudice 1 single ## 5 Pride &amp; Prejudice 1 possession ## 6 Pride &amp; Prejudice 1 fortune ## 7 Pride &amp; Prejudice 1 wife ## 8 Pride &amp; Prejudice 1 feelings ## 9 Pride &amp; Prejudice 1 views ## 10 Pride &amp; Prejudice 1 entering ## # ... with 37,230 more rows One example of the widyr pattern is the pairwise_count function. The prefix “pairwise” means it will result in one row for each pair of words in the word variable. This lets us count common pairs of words co-appearing within the same section: library(widyr) # count words co-occuring within sections word_pairs &lt;- austen_section_words %&gt;% pairwise_count(word, section, sort = TRUE) word_pairs ## # A tibble: 796,008 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 darcy elizabeth 144 ## 2 elizabeth darcy 144 ## 3 miss elizabeth 110 ## 4 elizabeth miss 110 ## 5 elizabeth jane 106 ## 6 jane elizabeth 106 ## 7 miss darcy 92 ## 8 darcy miss 92 ## 9 elizabeth bingley 91 ## 10 bingley elizabeth 91 ## # ... with 795,998 more rows For example, we discover that the most common pair of words in a section is “Elizabeth” and “Darcy” (the two main characters). word_pairs %&gt;% filter(item1 == &quot;darcy&quot;) ## # A tibble: 2,930 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 darcy elizabeth 144 ## 2 darcy miss 92 ## 3 darcy bingley 86 ## 4 darcy jane 46 ## 5 darcy bennet 45 ## 6 darcy sister 45 ## 7 darcy time 41 ## 8 darcy lady 38 ## 9 darcy friend 37 ## 10 darcy wickham 37 ## # ... with 2,920 more rows 5.2.2 Pairwise correlation Pairs like “Elizabeth” and “Darcy” are the most common co-occurring words, but that’s not particularly meaningful since they’re also the most common words. We instead want to examine correlation among words, which is how often they appear together relative to how often they appear separately. TODO: formula for Pearson correlation, explanation of phi coefficient The pairwise_cor() function in widyr lets us perform a Pearson correlation between words based on how often they appear in the same section. library(widyr) # We need to filter for at least relatively common words first word_cors &lt;- austen_section_words %&gt;% group_by(word) %&gt;% filter(n() &gt;= 20) %&gt;% pairwise_cor(word, section, sort = TRUE) word_cors ## # A tibble: 154,842 × 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 bourgh de 0.9508501 ## 2 de bourgh 0.9508501 ## 3 pounds thousand 0.7005808 ## 4 thousand pounds 0.7005808 ## 5 william sir 0.6644719 ## 6 sir william 0.6644719 ## 7 catherine lady 0.6633048 ## 8 lady catherine 0.6633048 ## 9 forster colonel 0.6220950 ## 10 colonel forster 0.6220950 ## # ... with 154,832 more rows For instance, we could find the words most correlated with a word like “pounds” by filtering: word_cors %&gt;% filter(item1 == &quot;pounds&quot;) ## # A tibble: 393 × 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 pounds thousand 0.70058081 ## 2 pounds ten 0.23057580 ## 3 pounds fortune 0.16386264 ## 4 pounds settled 0.14946049 ## 5 pounds wickham&#39;s 0.14152401 ## 6 pounds children 0.12900011 ## 7 pounds mother&#39;s 0.11905928 ## 8 pounds believed 0.09321518 ## 9 pounds estate 0.08896876 ## 10 pounds ready 0.08597038 ## # ... with 383 more rows This would let us examine the most-correlated words with any selection of words: word_cors %&gt;% filter(item1 %in% c(&quot;elizabeth&quot;, &quot;pounds&quot;, &quot;married&quot;, &quot;pride&quot;)) %&gt;% group_by(item1) %&gt;% top_n(6) %&gt;% mutate(item2 = reorder(item2, correlation)) %&gt;% ggplot(aes(item2, correlation)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ item1, scales = &quot;free&quot;) + coord_flip() Just as we used ggraph to visualize bigrams, we can use it to visualize the correlations and clusters of words that were found by the widyr package. set.seed(2016) word_cors %&gt;% filter(correlation &gt; .15) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() Note that unlike the bigram analysis, the relationship here are symmetric, rather than directional. We can also see that while pairings of names and titles that dominated bigram pairings are common, such as “colonel/fitzwilliam”, we can also see pairings of words that appear close to each other, such as “walk” and “park”. These network visualizations are a flexible tool for exploring relationships, and will play an important role in the case studies in later chapters. "],
["dtm.html", "6 Tidying and casting document-term matrices 6.1 Tidying a document-term matrix 6.2 Casting tidy text data into a DocumentTermMatrix 6.3 Tidying corpus objects with metadata", " 6 Tidying and casting document-term matrices So far, we’ve been analyzing data in a tidy text structure: a data frame with one-token-per-document-per-row. This lets us use the popular suite of tidy tools such as dplyr, tidyr, and ggplot2. We’ve demonstrated that many text analyses can be performed using these principles. But many of the existing tools for natural language processing don’t work with this kind of structure. The CRAN Task View for Natural Language Processing lists a large selection of packages that take other inputs. One of the most common is the document-term matrix, a sparse matrix with one row for each document in a collection and one column for each term or word. The value that goes into the matrix is usually a word count or sometimes tf-idf. These matrices are sparse (they consist mostly of zeroes), so special algorithms and data structures can be used to deal with them that are efficient and fast. The tidytext package can integrate these packages into an analysis while still relying on our tidy tools. The two key verbs are: tidy(): Constructs a data frame that summarizes a model’s statistical findings. cast_: Turns a tidy one-term-per-row data frame into a document-term matrix. This includes cast_sparse() (sparse Matrix), cast_dtm() (DocumentTermMatrix objects from tm), and cast_dfm() (dfm objects from quanteda). 6.1 Tidying a document-term matrix As we have discussed, many existing text mining datasets expect and provide a document-term matrix, or DTM. A DTM is a matrix where each row represents one document, each column represents one term, and each value typically contains the number of appearances of that term in that document. DTMs are usually implemented as sparse matrices, meaning the vast majority of values are 0. These objects can be interacted with as though they were matrices, but are stored in a more efficient format. One commonly used implementation of DTMs in R is the DocumentTermMatrix class in the tm package. For example, consider the corpus of 2246 Associated Press articles from the topicmodels package. library(tm) data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) class(AssociatedPress) ## [1] &quot;DocumentTermMatrix&quot; &quot;simple_triplet_matrix&quot; AssociatedPress ## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) We see that this dataset contains documents (each of them an AP article) and terms (words). Notice that this example DTM is 99% sparse. If we want to analyze this with tidy tools, we need to turn it into a one-token-per-document-per-row data frame first. The broom package (Robinson et al. 2015) introduced the tidy verb, which takes a non-tidy object and turns it into a data frame. The tidytext package implements that method for DocumentTermClass objects: library(dplyr) library(tidytext) ap_td &lt;- tidy(AssociatedPress) ap_td ## # A tibble: 302,031 × 3 ## document term count ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 adding 1 ## 2 1 adult 2 ## 3 1 ago 1 ## 4 1 alcohol 1 ## 5 1 allegedly 1 ## 6 1 allen 1 ## 7 1 apparently 2 ## 8 1 appeared 1 ## 9 1 arrested 1 ## 10 1 assault 1 ## # ... with 302,021 more rows Notice that we now have a tidy three-column tbl_df, with variables document, term, and count. This tidying operation is similar to the melt function from the reshape2 package (Wickham 2007) for non-sparse matrices. As we’ve seen in chapters 2-5, this form is convenient for analysis with the dplyr and tidytext packages. For example, you can perform sentiment analysis on these newspaper articles. bing &lt;- sentiments %&gt;% filter(lexicon == &quot;bing&quot;) %&gt;% select(word, sentiment) ap_sentiments &lt;- ap_td %&gt;% inner_join(bing, by = c(term = &quot;word&quot;)) ap_sentiments ## # A tibble: 30,094 × 4 ## document term count sentiment ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 assault 1 negative ## 2 1 complex 1 negative ## 3 1 death 1 negative ## 4 1 died 1 negative ## 5 1 good 2 positive ## 6 1 illness 1 negative ## 7 1 killed 2 negative ## 8 1 like 2 positive ## 9 1 liked 1 positive ## 10 1 miracle 1 positive ## # ... with 30,084 more rows This could, for example, let us visualize which words from these AP articles most often contributed to positive or negative sentiment: library(ggplot2) ap_sentiments %&gt;% count(sentiment, term, wt = count) %&gt;% ungroup() %&gt;% filter(n &gt;= 150) %&gt;% mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;% mutate(term = reorder(term, n)) %&gt;% ggplot(aes(term, n, fill = sentiment)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;) + ylab(&quot;Contribution to sentiment&quot;) + coord_flip() A tidier is also available for the dfm (document-feature matrix) class from the quanteda package (Benoit and Nulty 2016). Consider the corpus of presidential inauguration speeches that comes with the quanteda package: library(methods) data(&quot;inaugCorpus&quot;, package = &quot;quanteda&quot;) d &lt;- quanteda::dfm(inaugCorpus) d ## Document-feature matrix of: 57 documents, 9,215 features. tidy(d) ## # A tibble: 43,719 × 3 ## document term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1789-Washington fellow-citizens 1 ## 2 1797-Adams fellow-citizens 3 ## 3 1801-Jefferson fellow-citizens 2 ## 4 1809-Madison fellow-citizens 1 ## 5 1813-Madison fellow-citizens 1 ## 6 1817-Monroe fellow-citizens 5 ## 7 1821-Monroe fellow-citizens 1 ## 8 1841-Harrison fellow-citizens 11 ## 9 1845-Polk fellow-citizens 1 ## 10 1849-Taylor fellow-citizens 1 ## # ... with 43,709 more rows We could find the words most specific to several inaugural speeches using bind_tf_idf from chapter 4: speeches &lt;- c(&quot;1861-Lincoln&quot;, &quot;1945-Roosevelt&quot;, &quot;1961-Kennedy&quot;, &quot;2009-Obama&quot;) inaug_tf_idf &lt;- tidy(d) %&gt;% bind_tf_idf(term, document, count) %&gt;% arrange(desc(tf_idf)) %&gt;% filter(document %in% speeches) inaug_tf_idf ## # A tibble: 2,690 × 6 ## document term count tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1945-Roosevelt learned 5 0.009009009 1.845827 0.016629069 ## 2 1945-Roosevelt trend 2 0.003603604 3.349904 0.012071726 ## 3 1945-Roosevelt test 3 0.005405405 2.097141 0.011335898 ## 4 1961-Kennedy sides 8 0.005865103 1.845827 0.010825963 ## 5 1945-Roosevelt mistakes 2 0.003603604 2.944439 0.010610591 ## 6 1945-Roosevelt upward 2 0.003603604 2.656757 0.009573899 ## 7 1945-Roosevelt gain 2 0.003603604 2.433613 0.008769778 ## 8 1945-Roosevelt well-being 2 0.003603604 2.251292 0.008112763 ## 9 1945-Roosevelt faintness 1 0.001801802 4.043051 0.007284777 ## 10 1945-Roosevelt schoolmaster 1 0.001801802 4.043051 0.007284777 ## # ... with 2,680 more rows inaug_tf_idf %&gt;% group_by(document) %&gt;% top_n(8, tf_idf) %&gt;% ungroup() %&gt;% mutate(term = reorder(term, tf_idf)) %&gt;% ggplot(aes(term, tf_idf, fill = document)) + geom_bar(stat = &quot;identity&quot;, alpha = 0.8,show.legend = FALSE) + facet_wrap(~ document, scales = &quot;free&quot;) + coord_flip() 6.2 Casting tidy text data into a DocumentTermMatrix Some existing text mining tools or algorithms work only on sparse document-term matrices. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices. For example, we could take the tidied AP dataset and cast it back into a document-term matrix: ap_td ## # A tibble: 302,031 × 3 ## document term count ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 adding 1 ## 2 1 adult 2 ## 3 1 ago 1 ## 4 1 alcohol 1 ## 5 1 allegedly 1 ## 6 1 allen 1 ## 7 1 apparently 2 ## 8 1 appeared 1 ## 9 1 arrested 1 ## 10 1 assault 1 ## # ... with 302,021 more rows ap_td %&gt;% cast_dtm(document, term, count) ## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) Similarly, we could cast it into a Term-Document Matrix with cast_tdm, or quanteda’s dfm with cast_dfm: # cast into a Term-Document Matrix ap_td %&gt;% cast_tdm(term, document, count) ## &lt;&lt;TermDocumentMatrix (terms: 10473, documents: 2246)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) # cast into quanteda&#39;s dfm ap_td %&gt;% cast_dfm(term, document, count) ## Document-feature matrix of: 10,473 documents, 2,246 features. Some tools simply require a sparse matrix: library(Matrix) # cast into a Matrix object m &lt;- ap_td %&gt;% cast_sparse(document, term, count) class(m) ## [1] &quot;dgCMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;Matrix&quot; dim(m) ## [1] 2246 10473 This casting process allows for easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications. 6.3 Tidying corpus objects with metadata You can also tidy Corpus objects from the tm package. For example, consider a Corpus containing 20 documents: reut21578 &lt;- system.file(&quot;texts&quot;, &quot;crude&quot;, package = &quot;tm&quot;) reuters &lt;- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain)) reuters ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 20 The tidy verb creates a table with one row per document: reuters_td &lt;- tidy(reuters) reuters_td ## # A tibble: 20 × 17 ## author datetimestamp description ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 1987-02-26 17:00:56 ## 2 BY TED D&#39;AFFLISIO, Reuters 1987-02-26 17:34:11 ## 3 &lt;NA&gt; 1987-02-26 18:18:00 ## 4 &lt;NA&gt; 1987-02-26 18:21:01 ## 5 &lt;NA&gt; 1987-02-26 19:00:57 ## 6 &lt;NA&gt; 1987-03-01 03:25:46 ## 7 By Jeremy Clift, Reuters 1987-03-01 03:39:14 ## 8 &lt;NA&gt; 1987-03-01 05:27:27 ## 9 &lt;NA&gt; 1987-03-01 08:22:30 ## 10 &lt;NA&gt; 1987-03-01 18:31:44 ## 11 &lt;NA&gt; 1987-03-02 01:05:49 ## 12 &lt;NA&gt; 1987-03-02 07:39:23 ## 13 &lt;NA&gt; 1987-03-02 07:43:22 ## 14 &lt;NA&gt; 1987-03-02 07:43:41 ## 15 &lt;NA&gt; 1987-03-02 08:25:42 ## 16 &lt;NA&gt; 1987-03-02 11:20:05 ## 17 &lt;NA&gt; 1987-03-02 11:28:26 ## 18 &lt;NA&gt; 1987-03-02 12:13:46 ## 19 By BERNICE NAPACH, Reuters 1987-03-02 14:38:34 ## 20 &lt;NA&gt; 1987-03-02 14:49:06 ## heading id language origin ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES 127 en Reuters-21578 XML ## 2 OPEC MAY HAVE TO MEET TO FIRM PRICES - ANALYSTS 144 en Reuters-21578 XML ## 3 TEXACO CANADA &lt;TXC&gt; LOWERS CRUDE POSTINGS 191 en Reuters-21578 XML ## 4 MARATHON PETROLEUM REDUCES CRUDE POSTINGS 194 en Reuters-21578 XML ## 5 HOUSTON OIL &lt;HO&gt; RESERVES STUDY COMPLETED 211 en Reuters-21578 XML ## 6 KUWAIT SAYS NO PLANS FOR EMERGENCY OPEC TALKS 236 en Reuters-21578 XML ## 7 INDONESIA SEEN AT CROSSROADS OVER ECONOMIC CHANGE 237 en Reuters-21578 XML ## 8 SAUDI RIYAL DEPOSIT RATES REMAIN FIRM 242 en Reuters-21578 XML ## 9 QATAR UNVEILS BUDGET FOR FISCAL 1987/88 246 en Reuters-21578 XML ## 10 SAUDI ARABIA REITERATES COMMITMENT TO OPEC PACT 248 en Reuters-21578 XML ## 11 SAUDI FEBRUARY CRUDE OUTPUT PUT AT 3.5 MLN BPD 273 en Reuters-21578 XML ## 12 GULF ARAB DEPUTY OIL MINISTERS TO MEET IN BAHRAIN 349 en Reuters-21578 XML ## 13 SAUDI ARABIA REITERATES COMMITMENT TO OPEC ACCORD 352 en Reuters-21578 XML ## 14 KUWAIT MINISTER SAYS NO EMERGENCY OPEC TALKS SET 353 en Reuters-21578 XML ## 15 PHILADELPHIA PORT CLOSED BY TANKER CRASH 368 en Reuters-21578 XML ## 16 STUDY GROUP URGES INCREASED U.S. OIL RESERVES 489 en Reuters-21578 XML ## 17 STUDY GROUP URGES INCREASED U.S. OIL RESERVES 502 en Reuters-21578 XML ## 18 UNOCAL &lt;UCL&gt; UNIT CUTS CRUDE OIL POSTED PRICES 543 en Reuters-21578 XML ## 19 NYMEX WILL EXPAND OFF-HOUR TRADING APRIL ONE 704 en Reuters-21578 XML ## 20 ARGENTINE OIL PRODUCTION DOWN IN JANUARY 1987 708 en Reuters-21578 XML ## # ... with 10 more variables: topics &lt;chr&gt;, lewissplit &lt;chr&gt;, cgisplit &lt;chr&gt;, oldid &lt;chr&gt;, ## # topics_cat &lt;list&gt;, places &lt;list&gt;, people &lt;chr&gt;, orgs &lt;chr&gt;, exchanges &lt;chr&gt;, text &lt;chr&gt; Another variation of a corpus object is corpus from the quanteda package: library(quanteda) data(&quot;inaugCorpus&quot;) inaugCorpus ## Corpus consisting of 57 documents and 3 docvars. inaug_td &lt;- tidy(inaugCorpus) inaug_td ## # A tibble: 57 × 4 ## text ## * &lt;chr&gt; ## 1 Fellow-Citizens of the Senate and of the House of Representatives:\\n\\nAmong the vicissitudes incident to life no event could have filled me with greater ## 2 Fellow citizens, I am again called upon by the voice of my country to execute the functions of its Chief Magistrate. When the occasion proper for it s ## 3 When it was first perceived, in early times, that no middle course for America remained between unlimited submission to a foreign legislature and a to ## 4 Friends and Fellow Citizens:\\n\\nCalled upon to undertake the duties of the first executive office of our country, I avail myself of the presence of that ## 5 Proceeding, fellow citizens, to that qualification which the Constitution requires before my entrance on the charge again conferred on me, it is my du ## 6 Unwilling to depart from examples of the most revered authority, I avail myself of the occasion now presented to express the profound impression made ## 7 About to add the solemnity of an oath to the obligations imposed by a second call to the station in which my country heretofore placed me, I find in t ## 8 I should be destitute of feeling if I was not deeply affected by the strong proof which my fellow-citizens have given me of their confidence in callin ## 9 Fellow citizens, I shall not attempt to describe the grateful emotions which the new and very distinguished proof of the confidence of my fellow citiz ## 10 In compliance with an usage coeval with the existence of our Federal Constitution, and sanctioned by the example of my predecessors in the career upon ## # ... with 47 more rows, and 3 more variables: Year &lt;int&gt;, President &lt;chr&gt;, FirstName &lt;chr&gt; This lets us work with tidy tools like unnest_tokens to analyze the text alongside the metadata. inaug_words &lt;- inaug_td %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) inaug_words ## # A tibble: 49,621 × 4 ## Year President FirstName word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 Obama Barack waves ## 2 2013 Obama Barack realizes ## 3 2013 Obama Barack philadelphia ## 4 2013 Obama Barack 400 ## 5 2013 Obama Barack 40 ## 6 2013 Obama Barack absolutism ## 7 2013 Obama Barack contour ## 8 2013 Obama Barack newtown ## 9 2013 Obama Barack lanes ## 10 2013 Obama Barack appalachia ## # ... with 49,611 more rows We could then, for example, see how the appearance of a word changes over time: library(tidyr) inaug_freq &lt;- inaug_words %&gt;% count(Year, word) %&gt;% ungroup() %&gt;% complete(Year, word, fill = list(n = 0)) %&gt;% group_by(Year) %&gt;% mutate(year_total = sum(n), percent = n / year_total) %&gt;% ungroup() inaug_freq %&gt;% filter(word == &quot;america&quot;) ## # A tibble: 57 × 5 ## Year word n year_total percent ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1789 america 0 529 0.000000000 ## 2 1793 america 1 51 0.019607843 ## 3 1797 america 5 863 0.005793743 ## 4 1801 america 0 634 0.000000000 ## 5 1805 america 0 796 0.000000000 ## 6 1809 america 0 436 0.000000000 ## 7 1813 america 0 456 0.000000000 ## 8 1817 america 0 1197 0.000000000 ## 9 1821 america 2 1578 0.001267427 ## 10 1825 america 0 1153 0.000000000 ## # ... with 47 more rows For instance, we could display the top 6 terms that have changed in frequency over time. library(scales) inaug_freq %&gt;% filter(word %in% c(&quot;americans&quot;, &quot;century&quot;, &quot;foreign&quot;, &quot;god&quot;, &quot;union&quot;, &quot;constitution&quot;)) %&gt;% ggplot(aes(Year, percent)) + geom_point(alpha = 0.8) + geom_smooth() + facet_wrap(~ word, scales = &quot;free_y&quot;) + scale_y_continuous(labels = percent_format()) + ylab(&quot;Frequency of word in speech&quot;) References "],
["topicmodeling.html", "7 Topic modeling 7.1 The great library heist 7.2 Latent Dirichlet allocation with the topicmodels package 7.3 Per-document classification 7.4 By word assignments: augment", " 7 Topic modeling Topic modeling is a method for unsupervised classification of documents, by modeling each document as a mixture of topics and each topic as a mixture of words. Latent Dirichlet allocation is a particularly popular method for fitting a topic model. We can use tidy text principles, as described in Chapter 2, to approach topic modeling using consistent and effective tools. In particular, we’ll be using tidying functions for LDA objects from the topicmodels package. 7.1 The great library heist Suppose a vandal has broken into your study and torn apart four of your books: Great Expectations by Charles Dickens The War of the Worlds by H.G. Wells Twenty Thousand Leagues Under the Sea by Jules Verne Pride and Prejudice by Jane Austen This vandal has torn the books into individual chapters, and left them in one large pile. How can we restore these disorganized chapters to their original books? We’ll use topic modeling to discover how chapters are distinguished into distinct topics. We’ll retrieve these four books using the gutenbergr package: library(dplyr) titles &lt;- c(&quot;Twenty Thousand Leagues under the Sea&quot;, &quot;The War of the Worlds&quot;, &quot;Pride and Prejudice&quot;, &quot;Great Expectations&quot;) library(gutenbergr) books &lt;- gutenberg_works(title %in% titles) %&gt;% gutenberg_download(meta_fields = &quot;title&quot;) As pre-processing, we divide these into chapters, use tidytext’s unnest_tokens to separate them into words, then remove stop_words. We’re treating every chapter as a separate “document”, each with a name like Great Expectations_1 or Pride and Prejudice_11. library(tidytext) library(stringr) library(tidyr) by_chapter &lt;- books %&gt;% group_by(title) %&gt;% mutate(chapter = cumsum(str_detect(text, regex(&quot;^chapter &quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% filter(chapter &gt; 0) by_chapter_word &lt;- by_chapter %&gt;% unite(title_chapter, title, chapter) %&gt;% unnest_tokens(word, text) word_counts &lt;- by_chapter_word %&gt;% anti_join(stop_words) %&gt;% count(title_chapter, word, sort = TRUE) %&gt;% ungroup() word_counts ## # A tibble: 104,721 × 3 ## title_chapter word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations_57 joe 88 ## 2 Great Expectations_7 joe 70 ## 3 Great Expectations_17 biddy 63 ## 4 Great Expectations_27 joe 58 ## 5 Great Expectations_38 estella 58 ## 6 Great Expectations_2 joe 56 ## 7 Great Expectations_23 pocket 53 ## 8 Great Expectations_15 joe 50 ## 9 Great Expectations_18 joe 50 ## 10 The War of the Worlds_16 brother 50 ## # ... with 104,711 more rows 7.2 Latent Dirichlet allocation with the topicmodels package Right now this data frame is in a tidy form, with one-term-per-document-per-row. However, the topicmodels package requires a DocumentTermMatrix (from the tm package). As described in Chapter 6, we can cast a one-token-per-row table into a DocumentTermMatrix with tidytext’s cast_dtm: chapters_dtm &lt;- word_counts %&gt;% cast_dtm(title_chapter, word, n) chapters_dtm ## &lt;&lt;DocumentTermMatrix (documents: 193, terms: 18215)&gt;&gt; ## Non-/sparse entries: 104721/3410774 ## Sparsity : 97% ## Maximal term length: 19 ## Weighting : term frequency (tf) Now we are ready to use the topicmodels package to create a four topic LDA model. library(topicmodels) chapters_lda &lt;- LDA(chapters_dtm, k = 4, control = list(seed = 1234)) chapters_lda ## A LDA_VEM topic model with 4 topics. (In this case we know there are four topics because there are four books; in practice we may need to try a few different values of k). Now tidytext gives us the option of returning to a tidy analysis, using the tidy and augment verbs borrowed from the broom package. In particular, we start with the tidy verb. library(tidytext) chapters_lda_td &lt;- tidytext:::tidy.LDA(chapters_lda) chapters_lda_td ## # A tibble: 72,860 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 joe 5.830326e-17 ## 2 2 joe 3.194447e-57 ## 3 3 joe 4.162676e-24 ## 4 4 joe 1.445030e-02 ## 5 1 biddy 7.846976e-27 ## 6 2 biddy 4.672244e-69 ## 7 3 biddy 2.259711e-46 ## 8 4 biddy 4.767972e-03 ## 9 1 estella 3.827272e-06 ## 10 2 estella 5.316964e-65 ## # ... with 72,850 more rows Notice that this has turned the model into a one-topic-per-term-per-row format. For each combination the model has \\(\\beta\\), the probability of that term being generated from that topic. We could use dplyr’s top_n to find the top 5 terms within each topic: top_terms &lt;- chapters_lda_td %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms ## # A tibble: 20 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 elizabeth 0.014107538 ## 2 1 darcy 0.008814258 ## 3 1 miss 0.008706741 ## 4 1 bennet 0.006947431 ## 5 1 jane 0.006497512 ## 6 2 captain 0.015507696 ## 7 2 nautilus 0.013050048 ## 8 2 sea 0.008850073 ## 9 2 nemo 0.008708397 ## 10 2 ned 0.008030799 ## 11 3 people 0.006797400 ## 12 3 martians 0.006512569 ## 13 3 time 0.005347115 ## 14 3 black 0.005278302 ## 15 3 night 0.004483143 ## 16 4 joe 0.014450300 ## 17 4 time 0.006847574 ## 18 4 pip 0.006817363 ## 19 4 looked 0.006365257 ## 20 4 miss 0.006228387 This model lends itself to a visualization: library(ggplot2) top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() These topics are pretty clearly associated with the four books! There’s no question that the topic of “nemo”, “sea”, and “nautilus” belongs to Twenty Thousand Leagues Under the Sea, and that “jane”, “darcy”, and “elizabeth” belongs to Pride and Prejudice. We see “pip” and “joe” from Great Expectations and “martians”, “black”, and “night” from The War of the Worlds. 7.3 Per-document classification Each chapter was a “document” in this analysis. Thus, we may want to know which topics are associated with each document. Can we put the chapters back together in the correct books? chapters_lda_gamma &lt;- tidytext:::tidy.LDA(chapters_lda, matrix = &quot;gamma&quot;) chapters_lda_gamma ## # A tibble: 772 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations_57 1 1.351886e-05 ## 2 Great Expectations_7 1 1.470726e-05 ## 3 Great Expectations_17 1 2.117127e-05 ## 4 Great Expectations_27 1 1.919746e-05 ## 5 Great Expectations_38 1 3.544403e-01 ## 6 Great Expectations_2 1 1.723723e-05 ## 7 Great Expectations_23 1 5.507241e-01 ## 8 Great Expectations_15 1 1.682503e-02 ## 9 Great Expectations_18 1 1.272044e-05 ## 10 The War of the Worlds_16 1 1.084337e-05 ## # ... with 762 more rows Setting matrix = &quot;gamma&quot; returns a tidied version with one-document-per-topic-per-row. Now that we have these document classifiations, we can see how well our unsupervised learning did at distinguishing the four books. First we re-separate the document name into title and chapter: chapters_lda_gamma &lt;- chapters_lda_gamma %&gt;% separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE) chapters_lda_gamma ## # A tibble: 772 × 4 ## title chapter topic gamma ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations 57 1 1.351886e-05 ## 2 Great Expectations 7 1 1.470726e-05 ## 3 Great Expectations 17 1 2.117127e-05 ## 4 Great Expectations 27 1 1.919746e-05 ## 5 Great Expectations 38 1 3.544403e-01 ## 6 Great Expectations 2 1 1.723723e-05 ## 7 Great Expectations 23 1 5.507241e-01 ## 8 Great Expectations 15 1 1.682503e-02 ## 9 Great Expectations 18 1 1.272044e-05 ## 10 The War of the Worlds 16 1 1.084337e-05 ## # ... with 762 more rows Then we examine what fraction of chapters we got right for each: ggplot(chapters_lda_gamma, aes(gamma, fill = factor(topic))) + geom_histogram() + facet_wrap(~ title, nrow = 2) We notice that almost all of the chapters from Pride and Prejudice, War of the Worlds, and Twenty Thousand Leagues Under the Sea were uniquely identified as a single topic each. chapter_classifications &lt;- chapters_lda_gamma %&gt;% group_by(title, chapter) %&gt;% top_n(1, gamma) %&gt;% ungroup() %&gt;% arrange(gamma) chapter_classifications ## # A tibble: 193 × 4 ## title chapter topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations 54 3 0.4803234 ## 2 Great Expectations 22 4 0.5356506 ## 3 Great Expectations 31 4 0.5464851 ## 4 Great Expectations 23 1 0.5507241 ## 5 Great Expectations 33 4 0.5700737 ## 6 Great Expectations 47 4 0.5802089 ## 7 Great Expectations 56 4 0.5984806 ## 8 Great Expectations 38 4 0.6455341 ## 9 Great Expectations 11 4 0.6689600 ## 10 Great Expectations 44 4 0.6777974 ## # ... with 183 more rows We can determine this by finding the consensus book for each, which we note is correct based on our earlier visualization: book_topics &lt;- chapter_classifications %&gt;% count(title, topic) %&gt;% top_n(1, n) %&gt;% ungroup() %&gt;% transmute(consensus = title, topic) book_topics ## # A tibble: 4 × 2 ## consensus topic ## &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations 4 ## 2 Pride and Prejudice 1 ## 3 The War of the Worlds 3 ## 4 Twenty Thousand Leagues under the Sea 2 Then we see which chapters were misidentified: chapter_classifications %&gt;% inner_join(book_topics, by = &quot;topic&quot;) %&gt;% count(title, consensus) ## Source: local data frame [6 x 3] ## Groups: title [?] ## ## title consensus n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations Great Expectations 57 ## 2 Great Expectations Pride and Prejudice 1 ## 3 Great Expectations The War of the Worlds 1 ## 4 Pride and Prejudice Pride and Prejudice 61 ## 5 The War of the Worlds The War of the Worlds 27 ## 6 Twenty Thousand Leagues under the Sea Twenty Thousand Leagues under the Sea 46 We see that only a few chapters from Great Expectations were misclassified. Not bad for unsupervised clustering! 7.4 By word assignments: augment One important step in the topic modeling expectation-maximization algorithm is assigning each word in each document to a topic. The more words in a document are assigned to that topic, generally, the more weight (gamma) will go on that document-topic classification. We may want to take the original document-word pairs and find which words in each document were assigned to which topic. This is the job of the augment verb. assignments &lt;- tidytext:::augment.LDA(chapters_lda, data = chapters_dtm) We can combine this with the consensus book titles to find which words were incorrectly classified. assignments &lt;- assignments %&gt;% separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% inner_join(book_topics, by = c(&quot;.topic&quot; = &quot;topic&quot;)) assignments ## # A tibble: 104,721 × 6 ## title chapter term count .topic consensus ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Great Expectations 57 joe 88 4 Great Expectations ## 2 Great Expectations 7 joe 70 4 Great Expectations ## 3 Great Expectations 17 joe 5 4 Great Expectations ## 4 Great Expectations 27 joe 58 4 Great Expectations ## 5 Great Expectations 2 joe 56 4 Great Expectations ## 6 Great Expectations 23 joe 1 4 Great Expectations ## 7 Great Expectations 15 joe 50 4 Great Expectations ## 8 Great Expectations 18 joe 50 4 Great Expectations ## 9 Great Expectations 9 joe 44 4 Great Expectations ## 10 Great Expectations 13 joe 40 4 Great Expectations ## # ... with 104,711 more rows We can, for example, create a “confusion matrix” using dplyr’s count and tidyr’s spread: assignments %&gt;% count(title, consensus, wt = count) %&gt;% spread(consensus, n, fill = 0) ## Source: local data frame [4 x 5] ## Groups: title [4] ## ## title `Great Expectations` `Pride and Prejudice` ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Great Expectations 49770 3876 ## 2 Pride and Prejudice 1 37229 ## 3 The War of the Worlds 0 0 ## 4 Twenty Thousand Leagues under the Sea 0 5 ## `The War of the Worlds` `Twenty Thousand Leagues under the Sea` ## * &lt;dbl&gt; &lt;dbl&gt; ## 1 1845 77 ## 2 7 5 ## 3 22561 7 ## 4 0 39629 We notice that almost all the words for Pride and Prejudice, Twenty Thousand Leagues Under the Sea, and War of the Worlds were correctly assigned, while Great Expectations had a fair amount of misassignment. What were the most commonly mistaken words? wrong_words &lt;- assignments %&gt;% filter(title != consensus) wrong_words ## # A tibble: 4,535 × 6 ## title chapter term count .topic ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Great Expectations 38 brother 2 1 ## 2 Great Expectations 22 brother 4 1 ## 3 Great Expectations 23 miss 2 1 ## 4 Great Expectations 22 miss 23 1 ## 5 Twenty Thousand Leagues under the Sea 8 miss 1 1 ## 6 Great Expectations 31 miss 1 1 ## 7 Great Expectations 5 sergeant 37 1 ## 8 Great Expectations 46 captain 1 2 ## 9 Great Expectations 32 captain 1 2 ## 10 The War of the Worlds 17 captain 5 2 ## consensus ## &lt;chr&gt; ## 1 Pride and Prejudice ## 2 Pride and Prejudice ## 3 Pride and Prejudice ## 4 Pride and Prejudice ## 5 Pride and Prejudice ## 6 Pride and Prejudice ## 7 Pride and Prejudice ## 8 Twenty Thousand Leagues under the Sea ## 9 Twenty Thousand Leagues under the Sea ## 10 Twenty Thousand Leagues under the Sea ## # ... with 4,525 more rows wrong_words %&gt;% count(title, consensus, term, wt = count) %&gt;% ungroup() %&gt;% arrange(desc(n)) ## # A tibble: 3,500 × 4 ## title consensus term n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Great Expectations Pride and Prejudice love 44 ## 2 Great Expectations Pride and Prejudice sergeant 37 ## 3 Great Expectations Pride and Prejudice lady 32 ## 4 Great Expectations Pride and Prejudice miss 26 ## 5 Great Expectations The War of the Worlds boat 25 ## 6 Great Expectations Pride and Prejudice father 19 ## 7 Great Expectations The War of the Worlds water 19 ## 8 Great Expectations Pride and Prejudice baby 18 ## 9 Great Expectations Pride and Prejudice flopson 18 ## 10 Great Expectations Pride and Prejudice family 16 ## # ... with 3,490 more rows Notice the word “flopson” here; these wrong words do not necessarily appear in the novels they were misassigned to. Indeed, we can confirm “flopson” appears only in Great Expectations: word_counts %&gt;% filter(word == &quot;flopson&quot;) ## # A tibble: 3 × 3 ## title_chapter word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations_22 flopson 10 ## 2 Great Expectations_23 flopson 7 ## 3 Great Expectations_33 flopson 1 The algorithm is stochastic and iterative, and it can accidentally land on a topic that spans multiple books. "],
["twitter.html", "8 Case study: comparing Twitter archives 8.1 Getting the data and distribution of tweets 8.2 Word frequencies", " 8 Case study: comparing Twitter archives One type of text that has received its share of attention in recent years is text shared online via Twitter. In fact, several of the sentiment lexicons used in this book (and commonly used in general) were designed for use with and validated on tweets. Both of the authors of this book are on Twitter and are fairly regular users of it so in this case study, let’s compare the entire Twitter archives of Julia and David. 8.1 Getting the data and distribution of tweets An individual can download their own Twitter archive by following directions available here. We each downloaded ours and will now open them up. Let’s use the lubridate package to convert the string timestamps to date-time objects and initially take a look at our tweeting patterns overall. library(lubridate) library(ggplot2) library(dplyr) tweets_julia &lt;- read.csv(&quot;data/tweets_julia.csv&quot;, stringsAsFactors = FALSE) tweets_dave &lt;- read.csv(&quot;data/tweets_dave.csv&quot;, stringsAsFactors = FALSE) # take out the timezone thing if we don&#39;t do anything with times of day tweets_julia$timestamp &lt;- with_tz(ymd_hms(tweets_julia$timestamp), &quot;America/Denver&quot;) tweets_dave$timestamp &lt;- with_tz(ymd_hms(tweets_dave$timestamp), &quot;America/New_York&quot;) tweets &lt;- bind_rows(tweets_julia %&gt;% mutate(person = &quot;Julia&quot;), tweets_dave %&gt;% mutate(person = &quot;David&quot;)) ggplot(tweets, aes(x = timestamp, fill = person)) + geom_histogram(alpha = 0.5, position = &quot;identity&quot;) David and Julia tweet at about the same rate currently and joined Twitter about a year apart from each other, but there about 5 years where David was not active on Twitter and Julia was. In total, Julia has about 4 times as many tweets as David. 8.2 Word frequencies Let’s use unnest_tokens to make a tidy dataframe of all the words in our tweets, and remove the common English stop words. There are certain conventions in how people use text on Twitter, so we will do a bit more work with our text here than, for example, we did with the narrative text from Project Gutenberg. The first mutate line below removes links and cleans out some characters that we don’t want. In the call to unnest_tokens, we unnest using a regex pattern, instead of just looking for single unigrams (words). This regex pattern is very useful for dealing with Twitter text; it retains hashtags and mentions of usernames with the @ symbol. Because we have kept these types of symbols in the text, we can’t use a simple anti_join to remove stop words. Instead, we can take the approach shown in the filter line that uses str_detect from the stringr library. library(tidytext) library(stringr) reg &lt;- &quot;([^A-Za-z_\\\\d#@&#39;]|&#39;(?![A-Za-z_\\\\d#@]))&quot; tidy_tweets &lt;- tweets %&gt;% mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT&quot;, &quot;&quot;)) %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = reg) %&gt;% filter(!word %in% stop_words$word, str_detect(word, &quot;[a-z]&quot;)) Now we can calculate word frequencies for each person frequency &lt;- tidy_tweets %&gt;% group_by(person) %&gt;% count(word, sort = TRUE) %&gt;% left_join(tidy_tweets %&gt;% group_by(person) %&gt;% summarise(total = n())) %&gt;% mutate(freq = n/total) frequency ## Source: local data frame [23,086 x 5] ## Groups: person [2] ## ## person word n total freq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Julia time 567 76504 0.007411377 ## 2 Julia @selkie1970 565 76504 0.007385235 ## 3 Julia @skedman 518 76504 0.006770888 ## 4 Julia day 470 76504 0.006143470 ## 5 Julia baby 410 76504 0.005359197 ## 6 David #rstats 359 22074 0.016263477 ## 7 Julia @doctormac 342 76504 0.004470354 ## 8 David @hadleywickham 306 22074 0.013862463 ## 9 Julia love 303 76504 0.003960577 ## 10 Julia @haleynburke 291 76504 0.003803723 ## # ... with 23,076 more rows This is a lovely, tidy data frame but we would actually like to plot those frequencies on the x- and y-axes of a plot, so we will need to use an inner_join and make a different dataframe. frequency &lt;- inner_join(frequency %&gt;% filter(person == &quot;Julia&quot;) %&gt;% rename(Julia = freq), frequency %&gt;% filter(person == &quot;David&quot;) %&gt;% rename(David = freq), by = &quot;word&quot;) %&gt;% ungroup() %&gt;% select(word, Julia, David) frequency ## # A tibble: 3,520 × 3 ## word Julia David ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 time 0.007411377 0.0043490079 ## 2 day 0.006143470 0.0014949715 ## 3 baby 0.005359197 0.0001812087 ## 4 love 0.003960577 0.0020385974 ## 5 house 0.003790651 0.0001359065 ## 6 morning 0.003659939 0.0004077195 ## 7 people 0.003372373 0.0033523602 ## 8 feel 0.003124020 0.0013137628 ## 9 pretty 0.002954094 0.0010872520 ## 10 school 0.002875667 0.0002265108 ## # ... with 3,510 more rows library(scales) ggplot(frequency, aes(Julia, David)) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + geom_abline(color = &quot;red&quot;) This may not even need to be pointed out, but David and Julia have used their Twitter accounts rather differently over the course of the past several years. David has used his Twitter account almost exclusively for professional purposes since he became more active, while Julia used it for entirely personal purposes until late 2015. We see these differences immediately in this plot exploring word frequencies, and they will continue to be obvious in the rest of this chapter. Words near the red line in this plot are used with about equal frequencies by David and Julia, while words far away from the line are used much more by one person compared to the other. Because of the inner join we did above, words, hashtags, and usernames that appear in this plot are ones that we have both used at least once. TODO: lots "],
["nasa.html", "9 Case study: mining NASA metadata 9.1 Getting the metadata 9.2 Wrangling and tidying the data 9.3 Some initial simple exploration 9.4 Word co-ocurrences", " 9 Case study: mining NASA metadata There are 32,000+ datasets at NASA, and we can use the metadata for these datasets to understand the connections between them. What is metadata? Metadata is data that gives information about other data, in this case, data about what is in these numerous NASA datasets (but not the datasets themselves). It includes information like the title of the dataset, description fields, what organization(s) within NASA is responsible for the dataset, and so forth. NASA places a high priority on making its data accessible, even requiring all NASA-funded research to be openly accessible online, and the metadata for all its datasets is publicly available online in JSON format. Let’s take a look at this metadata and see what is there. 9.1 Getting the metadata First, let’s download the JSON file and take a look at the names. library(jsonlite) metadata &lt;- fromJSON(&quot;https://data.nasa.gov/data.json&quot;) names(metadata$dataset) ## [1] &quot;_id&quot; &quot;@type&quot; &quot;accessLevel&quot; &quot;accrualPeriodicity&quot; ## [5] &quot;bureauCode&quot; &quot;contactPoint&quot; &quot;description&quot; &quot;distribution&quot; ## [9] &quot;identifier&quot; &quot;issued&quot; &quot;keyword&quot; &quot;landingPage&quot; ## [13] &quot;language&quot; &quot;modified&quot; &quot;programCode&quot; &quot;publisher&quot; ## [17] &quot;spatial&quot; &quot;temporal&quot; &quot;theme&quot; &quot;title&quot; ## [21] &quot;license&quot; &quot;isPartOf&quot; &quot;references&quot; &quot;rights&quot; ## [25] &quot;describedBy&quot; What kind of data is available here? sapply(metadata$dataset, class) ## _id @type accessLevel accrualPeriodicity bureauCode ## &quot;data.frame&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;list&quot; ## contactPoint description distribution identifier issued ## &quot;data.frame&quot; &quot;character&quot; &quot;list&quot; &quot;character&quot; &quot;character&quot; ## keyword landingPage language modified programCode ## &quot;list&quot; &quot;character&quot; &quot;list&quot; &quot;character&quot; &quot;list&quot; ## publisher spatial temporal theme title ## &quot;data.frame&quot; &quot;character&quot; &quot;character&quot; &quot;list&quot; &quot;character&quot; ## license isPartOf references rights describedBy ## &quot;character&quot; &quot;character&quot; &quot;list&quot; &quot;character&quot; &quot;character&quot; It seems likely that the title, description, and keywords for each dataset may be most fruitful for drawing connections between datasets. It’s a place to start anyway! Let’s check them out. class(metadata$dataset$title) ## [1] &quot;character&quot; class(metadata$dataset$description) ## [1] &quot;character&quot; class(metadata$dataset$keyword) ## [1] &quot;list&quot; 9.2 Wrangling and tidying the data Let’s set up tidy data frames for title, description, and keyword and keep the dataset ids. library(dplyr) nasa_title &lt;- data_frame(id = metadata$dataset$`_id`$`$oid`, title = metadata$dataset$title) nasa_title ## # A tibble: 32,089 × 2 ## id title ## &lt;chr&gt; &lt;chr&gt; ## 1 55942a57c63a7fe59b495a77 15 Minute Stream Flow Data: USGS (FIFE) ## 2 55942a57c63a7fe59b495a78 15 Minute Stream Flow Data: USGS (FIFE) ## 3 55942a58c63a7fe59b495a79 15 Minute Stream Flow Data: USGS (FIFE) ## 4 55942a58c63a7fe59b495a7a 2000 Pilot Environmental Sustainability Index (ESI) ## 5 55942a58c63a7fe59b495a7b 2000 Pilot Environmental Sustainability Index (ESI) ## 6 55942a58c63a7fe59b495a7c 2000 Pilot Environmental Sustainability Index (ESI) ## 7 55942a58c63a7fe59b495a7d 2001 Environmental Sustainability Index (ESI) ## 8 55942a58c63a7fe59b495a7e 2001 Environmental Sustainability Index (ESI) ## 9 55942a58c63a7fe59b495a7f 2001 Environmental Sustainability Index (ESI) ## 10 55942a58c63a7fe59b495a80 2001 Environmental Sustainability Index (ESI) ## # ... with 32,079 more rows nasa_desc &lt;- data_frame(id = metadata$dataset$`_id`$`$oid`, desc = metadata$dataset$description) nasa_desc ## # A tibble: 32,089 × 2 ## id ## &lt;chr&gt; ## 1 55942a57c63a7fe59b495a77 ## 2 55942a57c63a7fe59b495a78 ## 3 55942a58c63a7fe59b495a79 ## 4 55942a58c63a7fe59b495a7a ## 5 55942a58c63a7fe59b495a7b ## 6 55942a58c63a7fe59b495a7c ## 7 55942a58c63a7fe59b495a7d ## 8 55942a58c63a7fe59b495a7e ## 9 55942a58c63a7fe59b495a7f ## 10 55942a58c63a7fe59b495a80 ## # ... with 32,079 more rows, and 1 more variables: desc &lt;chr&gt; These are having a hard time printing out; let’s print out part of a few. nasa_desc %&gt;% select(desc) %&gt;% sample_n(5) ## # A tibble: 5 × 1 ## desc ## &lt;chr&gt; ## 1 In a large network of computers, wireless sensors, or mobile devices, each of the components (hence, ## 2 LAT solar gamma-ray flux &gt; 100 MeV, one point per solar exposure (i.e. average of the 20-40 minutes ## 3 ML1OA is the EOS Aura Microwave Limb Sounder (MLS) product containing the level 1 orbit attitude and ## 4 This data set reports the oxygen isotope signatures of water extracted from plant tissue (xylem from ## 5 MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the\\nTerra (EOS A Now we can do the keywords, which must be unnested since they are in a list-column. library(tidyr) nasa_keyword &lt;- data_frame(id = metadata$dataset$`_id`$`$oid`, keyword = metadata$dataset$keyword) %&gt;% unnest(keyword) nasa_keyword ## # A tibble: 126,814 × 2 ## id keyword ## &lt;chr&gt; &lt;chr&gt; ## 1 55942a57c63a7fe59b495a77 EARTH SCIENCE ## 2 55942a57c63a7fe59b495a77 HYDROSPHERE ## 3 55942a57c63a7fe59b495a77 SURFACE WATER ## 4 55942a57c63a7fe59b495a78 EARTH SCIENCE ## 5 55942a57c63a7fe59b495a78 HYDROSPHERE ## 6 55942a57c63a7fe59b495a78 SURFACE WATER ## 7 55942a58c63a7fe59b495a79 EARTH SCIENCE ## 8 55942a58c63a7fe59b495a79 HYDROSPHERE ## 9 55942a58c63a7fe59b495a79 SURFACE WATER ## 10 55942a58c63a7fe59b495a7a EARTH SCIENCE ## # ... with 126,804 more rows Now let’s use tidytext’s unnest_tokens for the title and description fields so we can do the text analysis. Let’s also remove common English words. library(tidytext) nasa_title &lt;- nasa_title %&gt;% unnest_tokens(word, title) %&gt;% anti_join(stop_words) nasa_desc &lt;- nasa_desc %&gt;% unnest_tokens(word, desc) %&gt;% anti_join(stop_words) 9.3 Some initial simple exploration What are the most common words in the NASA dataset titles? nasa_title %&gt;% count(word, sort = TRUE) ## # A tibble: 11,614 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 project 7735 ## 2 data 3354 ## 3 1 2841 ## 4 level 2400 ## 5 global 1809 ## 6 v1 1478 ## 7 daily 1397 ## 8 3 1364 ## 9 aura 1363 ## 10 l2 1311 ## # ... with 11,604 more rows What about the descriptions? nasa_desc %&gt;% count(word, sort = TRUE) ## # A tibble: 35,940 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 data 68871 ## 2 modis 24420 ## 3 global 23028 ## 4 2 16599 ## 5 1 15770 ## 6 system 15480 ## 7 product 14780 ## 8 aqua 14738 ## 9 earth 14373 ## 10 resolution 13879 ## # ... with 35,930 more rows It looks like we might want to remove digits and some “words” like “v1” from these dataframes before approaching something more meaningful like topic modeling. mystopwords &lt;- data_frame(word = c(as.character(1:10), &quot;v1&quot;, &quot;v03&quot;, &quot;l2&quot;, &quot;l3&quot;, &quot;v5.2.0&quot;, &quot;v003&quot;, &quot;v004&quot;, &quot;v005&quot;, &quot;v006&quot;)) nasa_title &lt;- nasa_title %&gt;% anti_join(mystopwords) nasa_desc &lt;- nasa_desc %&gt;% anti_join(mystopwords) What are the most common keywords? nasa_keyword %&gt;% group_by(keyword) %&gt;% count(sort = TRUE) ## # A tibble: 1,774 × 2 ## keyword n ## &lt;chr&gt; &lt;int&gt; ## 1 EARTH SCIENCE 14362 ## 2 Project 7452 ## 3 ATMOSPHERE 7321 ## 4 Ocean Color 7268 ## 5 Ocean Optics 7268 ## 6 Oceans 7268 ## 7 completed 6452 ## 8 ATMOSPHERIC WATER VAPOR 3142 ## 9 OCEANS 2765 ## 10 LAND SURFACE 2720 ## # ... with 1,764 more rows It is possible that “Project completed” may not be a useful set of keywords to keep around for some purposes, and we may want to change all of these to lower or upper case to get rid of duplicates like “OCEANS” and “Oceans”. Let’s do that, actually. nasa_keyword &lt;- nasa_keyword %&gt;% mutate(keyword = toupper(keyword)) 9.4 Word co-ocurrences Let’s examine which words commonly occur together in the titles and descriptions of NASA datasets. We can then examine a word network in titles/descriptions; this may help us decide, for example, how many topics to look at in topic modeling. library(widyr) title_words &lt;- nasa_title %&gt;% pairwise_count(word, id, sort = TRUE) title_words ## # A tibble: 313,774 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 project system 796 ## 2 system project 796 ## 3 eco lba 683 ## 4 lba eco 683 ## 5 aqua airs 641 ## 6 airs aqua 641 ## 7 aqua level 623 ## 8 level aqua 623 ## 9 airs level 612 ## 10 level airs 612 ## # ... with 313,764 more rows desc_words &lt;- nasa_desc %&gt;% pairwise_count(word, id, sort = TRUE) desc_words ## # A tibble: 21,779,288 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 global data 9864 ## 2 data global 9864 ## 3 resolution data 9302 ## 4 data resolution 9302 ## 5 resolution instrument 8189 ## 6 instrument resolution 8189 ## 7 surface data 8180 ## 8 data surface 8180 ## 9 resolution global 8139 ## 10 global resolution 8139 ## # ... with 21,779,278 more rows Let’s plot networks of these co-occurring words. library(ggplot2) library(igraph) library(ggraph) set.seed(1234) title_words %&gt;% filter(n &gt;= 250) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1.8) + ggtitle(&quot;Word Network in NASA Dataset Titles&quot;) + theme_void() This is a good start, although it looks like there may still a bit more cleaning to be done. Let’s look at the words in descriptions. set.seed(2016) desc_words %&gt;% filter(n &gt;= 5000) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;indianred4&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1.8) + ggtitle(&quot;Word Network in NASA Dataset Descriptions&quot;) + theme_void() Here there are such strong connections between the top dozen or so words (words like “data”, “resolution”, and “instrument”) that we may do better if we exclude these very highly connected words. Also, this may mean that tf-idf (as described in detail in Chapter 4) will be a good option to explore. But for now, let’s add a few more stop words and look at one more word network for the description fields. Notice how we use bind_rows to add more custom stop words to the words we are already using; this approach can be used in many instances. mystopwords &lt;- bind_rows(mystopwords, data_frame(word = c(&quot;data&quot;, &quot;global&quot;, &quot;instrument&quot;, &quot;resolution&quot;, &quot;product&quot;, &quot;level&quot;))) nasa_desc &lt;- nasa_desc %&gt;% anti_join(mystopwords) desc_words &lt;- nasa_desc %&gt;% pairwise_count(word, id, sort = TRUE) set.seed(1234) desc_words %&gt;% filter(n &gt;= 4600) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;indianred4&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1.8) + ggtitle(&quot;Word Network in NASA Dataset Descriptions&quot;) + theme_void() We still are not seeing clusters the way we did with the titles (the descriptions appear to use very similar words compared to each other), so using tf-idf may be a better way to go when approaching the description fields. Let’s make a network of the keywords to see which keywords commonly occur together in the same datasets. keyword_counts &lt;- nasa_keyword %&gt;% pairwise_count(keyword, id, sort = TRUE) set.seed(1234) keyword_counts %&gt;% filter(n &gt;= 700) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;royalblue3&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1.8) + ggtitle(&quot;Co-occurrence Network in NASA Dataset Keywords&quot;) + theme_void() These are the most commonly co-occurring words, but also just the most common keywords in general. To more meaningfully examine which keywords are likely to appear together instead of separately, we need to find the correlation among the keywords as described in Chapter 5. TODO: correlation of keywords, tf-idf, and topic modeling "],
["usenet.html", "10 Case study: analyzing usenet text 10.1 Wrangling the data 10.2 Term frequency and inverse document frequency: tf-idf 10.3 Sentiment analysis 10.4 Sentiment analysis by word 10.5 Sentiment analysis by message 10.6 N-grams", " 10 Case study: analyzing usenet text In our final chapter, we’ll use what we’ve learned in this book to perform a start-to-finish analysis of a set of 20,000 messages sent to 20 Usenet bulletin boards in 1993. The Usenet bulletin boards in this data set include boards for topics like politics, autos, “for sale”, atheism, etc. This data set is publicly available and has become popular for testing and exercises in text analysis and machine learning. 10.1 Wrangling the data We’ll start by reading in all the messages. (Note that this step takes several minutes). library(dplyr) library(tidyr) library(purrr) library(readr) library(stringr) training_folder &lt;- &quot;data/20news-bydate/20news-bydate-train/&quot; read_folder &lt;- function(infolder) { print(infolder) data_frame(file = dir(infolder, full.names = TRUE)) %&gt;% mutate(text = map(file, read_lines)) %&gt;% transmute(id = basename(file), text) %&gt;% unnest(text) } raw_text &lt;- data_frame(folder = dir(training_folder, full.names = TRUE)) %&gt;% unnest(map(folder, read_folder)) %&gt;% transmute(board = basename(folder), id, text) Each email has structure we need to remove. For starters: Every email has one or more headers (e.g. “from:”, “in_reply_to:”) Many have signatures, which (since they’re constant for each user) we wouldn’t want to examine alongside the content We need to remove headers and signatures. # remove headers and signatures cleaned_text &lt;- raw_text %&gt;% group_by(id) %&gt;% filter(cumsum(text == &quot;&quot;) &gt; 0, cumsum(str_detect(text, &quot;^--&quot;)) == 0) %&gt;% ungroup() # remove nested text (starting with &quot;&gt;&quot;) and lines that note the author # of those cleaned_text &lt;- cleaned_text %&gt;% filter(str_detect(text, &quot;^[^&gt;]+[A-Za-z\\\\d]&quot;) | text == &quot;&quot;, !str_detect(text, &quot;writes(:|\\\\.\\\\.\\\\.)$&quot;), !str_detect(text, &quot;^In article &lt;&quot;), !id %in% c(9704, 9985)) Now it is time to use unnest_tokens to identify the words in this data set. library(tidytext) usenet_words &lt;- cleaned_text %&gt;% unnest_tokens(word, text) %&gt;% filter(str_detect(word, &quot;^[a-z]&quot;), str_detect(word, &quot;[a-z]$&quot;), !word %in% stop_words$word) What are the most common words? usenet_words %&gt;% count(word, sort = TRUE) ## # A tibble: 63,937 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 people 3397 ## 2 time 2569 ## 3 god 1611 ## 4 system 1571 ## 5 subject 1312 ## 6 lines 1188 ## 7 program 1086 ## 8 windows 1085 ## 9 bit 1070 ## 10 space 1062 ## # ... with 63,927 more rows Or perhaps more sensibly, we could examine the most common words by board. words_by_board &lt;- usenet_words %&gt;% count(board, word) %&gt;% ungroup() words_by_board %&gt;% group_by(board) %&gt;% top_n(3) ## Source: local data frame [60 x 3] ## Groups: board [20] ## ## board word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 alt.atheism god 268 ## 2 alt.atheism jesus 129 ## 3 alt.atheism people 276 ## 4 comp.graphics graphics 217 ## 5 comp.graphics image 169 ## 6 comp.graphics program 134 ## 7 comp.os.ms-windows.misc dos 194 ## 8 comp.os.ms-windows.misc file 232 ## 9 comp.os.ms-windows.misc windows 625 ## 10 comp.sys.ibm.pc.hardware card 237 ## # ... with 50 more rows These look sensible and illuminating so far; let’s move on to some more sophisticated analysis! 10.2 Term frequency and inverse document frequency: tf-idf Some words are likely to be more common on particular boards. Let’s try quantifying this using the tf-idf metric we learned in Chapter 4. tf_idf &lt;- words_by_board %&gt;% bind_tf_idf(word, board, n) %&gt;% arrange(desc(tf_idf)) tf_idf ## # A tibble: 166,528 × 6 ## board word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 comp.sys.ibm.pc.hardware scsi 483 0.018138801 1.203973 0.02183862 ## 2 rec.motorcycles bike 321 0.013750268 1.386294 0.01906192 ## 3 talk.politics.mideast armenian 440 0.007348275 2.302585 0.01692003 ## 4 sci.crypt encryption 410 0.008311878 1.897120 0.01576863 ## 5 talk.politics.mideast armenians 396 0.006613447 2.302585 0.01522803 ## 6 rec.sport.hockey nhl 151 0.004291114 2.995732 0.01285503 ## 7 comp.sys.ibm.pc.hardware ide 208 0.007811326 1.609438 0.01257184 ## 8 talk.politics.misc stephanopoulos 158 0.004175145 2.995732 0.01250762 ## 9 rec.motorcycles bikes 97 0.004155065 2.995732 0.01244746 ## 10 rec.sport.hockey hockey 265 0.007530762 1.609438 0.01212029 ## # ... with 166,518 more rows We can visualize this for a few select boards. First, let’s look at all the sci. boards. library(ggplot2) tf_idf %&gt;% filter(str_detect(board, &quot;^sci\\\\.&quot;)) %&gt;% group_by(board) %&gt;% top_n(12, tf_idf) %&gt;% mutate(word = reorder(word, tf_idf)) %&gt;% ggplot(aes(word, tf_idf, fill = board)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ board, scales = &quot;free&quot;) + ylab(&quot;tf-idf&quot;) + coord_flip() We could use almost the same code (not shown) to compare the “rec.” (recreation) or “talk.” boards: We see lots of characteristic words for these boards, from “pitching” and “hitter” for the baseball board to “firearm” and “militia” on the guns board. Notice how high tf-idf is for words like “Stephanopoulos” or “Armenian”; this means that these words are very unique among the documents as a whole and important to those particular boards. 10.3 Sentiment analysis We can use the sentiment analysis techniques we explored in Chapter 3 to examine how positive and negative words were used in these Usenet posts. Which boards used the most positive and negative words? AFINN &lt;- sentiments %&gt;% filter(lexicon == &quot;AFINN&quot;) word_board_sentiments &lt;- words_by_board %&gt;% inner_join(AFINN, by = &quot;word&quot;) board_sentiments &lt;- word_board_sentiments %&gt;% group_by(board) %&gt;% summarize(score = sum(score * n) / sum(n)) board_sentiments %&gt;% mutate(board = reorder(board, score)) %&gt;% ggplot(aes(board, score, fill = score &gt; 0)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + coord_flip() + ylab(&quot;Average sentiment score&quot;) 10.4 Sentiment analysis by word It’s worth looking deeper to understand why some boards ended up more positive than others. For that, we can examine the total positive and negative contributions of each word. contributions &lt;- usenet_words %&gt;% inner_join(AFINN, by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% summarize(occurences = n(), contribution = sum(score)) contributions ## # A tibble: 1,891 × 3 ## word occurences contribution ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 abandon 12 -24 ## 2 abandoned 18 -36 ## 3 abandons 3 -6 ## 4 abduction 1 -2 ## 5 abhor 3 -9 ## 6 abhorred 1 -3 ## 7 abhorrent 2 -6 ## 8 abilities 16 32 ## 9 ability 160 320 ## 10 aboard 8 8 ## # ... with 1,881 more rows Which words had the most effect? contributions %&gt;% top_n(25, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + coord_flip() These words look generally reasonable as indicators of each message’s sentiment, but we can spot possible problems with the approach. “True” could just as easily be a part of “not true” or a similar negative expression, and the words “God” and “Jesus” are apparently very common on Usenet but could easily be used in many contexts, positive or negative. The important point is that we may also care about which words contributed the most within each board. We can calculate each word’s contribution to each board’s sentiment score from our word_board_sentiments variable: top_sentiment_words &lt;- word_board_sentiments %&gt;% mutate(contribution = score * n / sum(n)) top_sentiment_words %&gt;% group_by(board) %&gt;% top_n(8, abs(contribution)) %&gt;% ungroup() %&gt;% mutate(board = reorder(board, contribution), word = reorder(word, contribution)) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ board, scales = &quot;free&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) We can see here how much sentiment is confounded with topic in this particular approach. An atheism board is likely to discuss “god” in detail even in a negative context, and we can see it makes the board look more positive. Similarly, the negative contribution of the word “gun” to the “talk.politics.guns” board would occur even if the board members were discussing guns positively. 10.5 Sentiment analysis by message We can also try finding the most positive and negative messages. sentiment_messages &lt;- usenet_words %&gt;% inner_join(AFINN, by = &quot;word&quot;) %&gt;% group_by(board, id) %&gt;% summarize(sentiment = mean(score), words = n()) %&gt;% ungroup() %&gt;% filter(words &gt;= 5) As a simple measure to reduce the role of randomness, we filtered out messages that had fewer than five words that contributed to sentiment. What were the most positive messages? sentiment_messages %&gt;% arrange(desc(sentiment)) ## # A tibble: 3,385 × 4 ## board id sentiment words ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 rec.sport.hockey 53560 3.888889 18 ## 2 rec.sport.hockey 53602 3.833333 30 ## 3 rec.sport.hockey 53822 3.833333 6 ## 4 rec.sport.hockey 53645 3.230769 13 ## 5 rec.autos 102768 3.200000 5 ## 6 misc.forsale 75965 3.000000 5 ## 7 misc.forsale 76037 3.000000 5 ## 8 rec.sport.baseball 104458 2.916667 12 ## 9 comp.os.ms-windows.misc 9620 2.857143 7 ## 10 misc.forsale 74787 2.833333 6 ## # ... with 3,375 more rows Let’s check this by looking at the most positive message in the whole data set. print_message &lt;- function(message_id) { cleaned_text %&gt;% filter(id == message_id) %&gt;% filter(text != &quot;&quot;) %&gt;% .$text %&gt;% cat(sep = &quot;\\n&quot;) } print_message(53560) ## Everybody. Please send me your predictions for the Stanley Cup Playoffs! ## I want to see who people think will win.!!!!!!! ## Please Send them in this format, or something comparable: ## 1. Winner of Buffalo-Boston ## 2. Winner of Montreal-Quebec ## 3. Winner of Pittsburgh-New York ## 4. Winner of New Jersey-Washington ## 5. Winner of Chicago-(Minnesota/St.Louis) ## 6. Winner of Toronto-Detroit ## 7. Winner of Vancouver-Winnipeg ## 8. Winner of Calgary-Los Angeles ## 9. Winner of Adams Division (1-2 above) ## 10. Winner of Patrick Division (3-4 above) ## 11. Winner of Norris Division (5-6 above) ## 12. Winner of Smythe Division (7-8 above) ## 13. Winner of Wales Conference (9-10 above) ## 14. Winner of Campbell Conference (11-12 above) ## 15. Winner of Stanley Cup (13-14 above) ## I will summarize the predictions, and see who is the biggest ## INTERNET GURU PREDICTING GUY/GAL. ## Send entries to Richard Madison ## rrmadiso@napier.uwaterloo.ca ## PS: I will send my entries to one of you folks so you know when I say ## I won, that I won!!!!! ## From: sknapp@iastate.edu (Steven M. Knapp) ## Subject: Re: Radar detector DETECTORS? ## Organization: Iowa State University, Ames, IA ## Lines: 16 ## Yes some radar detectors are less detectable by radar detector ## detectors. ;-) ## Look in Car and Driver (last 6 months should do), they had a big ## review of the &quot;better&quot; detectors, and stealth was a factor. ## Steven M. Knapp Computer Engineering Student ## sknapp@iastate.edu President Cyclone Amateur Radio Club ## Iowa State University; Ames, IA; USA Durham Center Operations Staff Looks like it’s because the message uses the word “winner” a lot! How about the most negative message? Turns out it’s also from the hockey site, but has a very different attitude. sentiment_messages %&gt;% arrange(sentiment) ## # A tibble: 3,385 × 4 ## board id sentiment words ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 rec.sport.hockey 53907 -3.000000 6 ## 2 sci.electronics 53899 -3.000000 5 ## 3 rec.autos 101627 -2.833333 6 ## 4 comp.graphics 37948 -2.800000 5 ## 5 comp.windows.x 67204 -2.700000 10 ## 6 talk.politics.guns 53362 -2.666667 6 ## 7 alt.atheism 51309 -2.600000 5 ## 8 comp.sys.mac.hardware 51513 -2.600000 5 ## 9 rec.autos 102883 -2.600000 5 ## 10 rec.motorcycles 72052 -2.600000 5 ## # ... with 3,375 more rows print_message(53907) ## Losers like us? You are the fucking moron who has never heard of the Western ## Business School, or the University of Western Ontario for that matter. Why ## don&#39;t you pull your head out of your asshole and smell something other than ## shit for once so you can look on a map to see where UWO is! Back to hockey, ## the North Stars should be moved because for the past few years they have ## just been SHIT. A real team like Toronto would never be moved!!! ## Andrew-- Well then. 10.6 N-grams We can also examine the effect of words that are used in negation, like we did in Chapter 5. Let’s start by finding all the bigrams in the Usenet posts. usenet_bigrams &lt;- cleaned_text %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) usenet_bigrams ## # A tibble: 1,762,089 × 3 ## board id bigram ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 alt.atheism 49960 archive name ## 2 alt.atheism 49960 name atheism ## 3 alt.atheism 49960 atheism resources ## 4 alt.atheism 49960 resources alt ## 5 alt.atheism 49960 alt atheism ## 6 alt.atheism 49960 atheism archive ## 7 alt.atheism 49960 archive name ## 8 alt.atheism 49960 name resources ## 9 alt.atheism 49960 resources last ## 10 alt.atheism 49960 last modified ## # ... with 1,762,079 more rows Now let’s count how many of these bigrams are used in each board. usenet_bigram_counts &lt;- usenet_bigrams %&gt;% count(board, bigram) usenet_bigram_counts %&gt;% arrange(desc(n)) ## Source: local data frame [1,006,415 x 3] ## Groups: board [20] ## ## board bigram n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 soc.religion.christian of the 1141 ## 2 talk.politics.mideast of the 1135 ## 3 talk.politics.mideast in the 857 ## 4 sci.space of the 684 ## 5 sci.crypt of the 671 ## 6 talk.politics.misc of the 645 ## 7 soc.religion.christian in the 637 ## 8 talk.religion.misc of the 630 ## 9 talk.politics.guns of the 618 ## 10 alt.atheism of the 474 ## # ... with 1,006,405 more rows Next, we can calculate tf-idf for the bigrams to find the ones that are important for each board. bigram_tf_idf &lt;- usenet_bigram_counts %&gt;% bind_tf_idf(bigram, board, n) bigram_tf_idf %&gt;% arrange(desc(tf_idf)) ## Source: local data frame [1,006,415 x 6] ## Groups: board [20] ## ## board bigram n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 talk.politics.misc mr stephanopoulos 155 0.001477344 2.995732 0.004425728 ## 2 comp.windows.x n x 177 0.001917577 2.302585 0.004415384 ## 3 comp.windows.x x printf 130 0.001408390 2.995732 0.004219158 ## 4 rec.motorcycles the bike 104 0.001675663 2.302585 0.003858356 ## 5 comp.sys.ibm.pc.hardware scsi 2 107 0.001478983 2.302585 0.003405485 ## 6 comp.windows.x file x 104 0.001126712 2.995732 0.003375327 ## 7 talk.politics.mideast the armenians 169 0.001111988 2.995732 0.003331220 ## 8 rec.sport.hockey 1 0 256 0.002733816 1.203973 0.003291440 ## 9 comp.windows.x output oname 100 0.001083377 2.995732 0.003245506 ## 10 comp.windows.x x char 98 0.001061709 2.995732 0.003180596 ## # ... with 1,006,405 more rows Now we come back to the words used in negation that we are interested in examining. Let’s define a vector of words that we suspect are used in negation, and use the same joining and counting approach from Chapter 5 to examine all of them at once. negate_words &lt;- c(&quot;not&quot;, &quot;without&quot;, &quot;no&quot;, &quot;can&#39;t&quot;, &quot;don&#39;t&quot;, &quot;won&#39;t&quot;) usenet_bigram_counts %&gt;% ungroup() %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 %in% negate_words) %&gt;% count(word1, word2, wt = n, sort = TRUE) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% mutate(contribution = score * nn) %&gt;% top_n(10, abs(contribution)) %&gt;% ungroup() %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% ggplot(aes(word2, contribution, fill = contribution &gt; 0)) + geom_bar(alpha = 0.8, stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ word1, scales = &quot;free&quot;, nrow = 3) + xlab(&quot;Words preceded by negation&quot;) + ylab(&quot;Sentiment score * # of occurrences&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) These words are the ones that contribute the most to the sentiment scores in the wrong direction, because they are being used with negation words before them. Phrases like “no problem” and “don’t want” are important sources of misidentification. "],
["references.html", "11 References", " 11 References "]
]
