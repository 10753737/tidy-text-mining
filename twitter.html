<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Tidy Text Mining with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Tidy Text Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/tidycover.png" />
  <meta property="og:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="github-repo" content="dgrtwo/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Tidy Text Mining with R" />
  
  <meta name="twitter:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="twitter:image" content="images/tidycover.png" />

<meta name="author" content="Julia Silge and David Robinson">


<meta name="date" content="2016-12-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="topicmodeling.html">
<link rel="next" href="nasa.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68765210-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Tidy Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Tidy Text Mining with R</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-tidy-text"><i class="fa fa-check"></i><b>1.1</b> What is tidy text?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#outline"><i class="fa fa-check"></i><b>1.3</b> Outline</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#topics-this-book-does-not-cover"><i class="fa fa-check"></i><b>1.4</b> Topics this book does not cover</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#acknowledgements"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tidytext.html"><a href="tidytext.html"><i class="fa fa-check"></i><b>2</b> The tidy text format</a><ul>
<li class="chapter" data-level="2.1" data-path="tidytext.html"><a href="tidytext.html#the-unnest_tokens-function"><i class="fa fa-check"></i><b>2.1</b> The <code>unnest_tokens</code> function</a></li>
<li class="chapter" data-level="2.2" data-path="tidytext.html"><a href="tidytext.html#tidying-the-works-of-jane-austen"><i class="fa fa-check"></i><b>2.2</b> Tidying the works of Jane Austen</a></li>
<li class="chapter" data-level="2.3" data-path="tidytext.html"><a href="tidytext.html#the-gutenbergr-package"><i class="fa fa-check"></i><b>2.3</b> The gutenbergr package</a></li>
<li class="chapter" data-level="2.4" data-path="tidytext.html"><a href="tidytext.html#word-frequencies"><i class="fa fa-check"></i><b>2.4</b> Word frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sentiment.html"><a href="sentiment.html"><i class="fa fa-check"></i><b>3</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="3.1" data-path="sentiment.html"><a href="sentiment.html#the-sentiments-dataset"><i class="fa fa-check"></i><b>3.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="3.2" data-path="sentiment.html"><a href="sentiment.html#sentiment-analysis-with-inner-join"><i class="fa fa-check"></i><b>3.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="3.3" data-path="sentiment.html"><a href="sentiment.html#comparing-the-three-sentiment-dictionaries"><i class="fa fa-check"></i><b>3.3</b> Comparing the three sentiment dictionaries</a></li>
<li class="chapter" data-level="3.4" data-path="sentiment.html"><a href="sentiment.html#most-common-positive-and-negative-words"><i class="fa fa-check"></i><b>3.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="3.5" data-path="sentiment.html"><a href="sentiment.html#wordclouds"><i class="fa fa-check"></i><b>3.5</b> Wordclouds</a></li>
<li class="chapter" data-level="3.6" data-path="sentiment.html"><a href="sentiment.html#looking-at-units-beyond-just-words"><i class="fa fa-check"></i><b>3.6</b> Looking at units beyond just words</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tfidf.html"><a href="tfidf.html"><i class="fa fa-check"></i><b>4</b> Analyzing word and document frequency: tf-idf</a><ul>
<li class="chapter" data-level="4.1" data-path="tfidf.html"><a href="tfidf.html#term-frequency-and-inverse-document-frequency"><i class="fa fa-check"></i><b>4.1</b> Term frequency and inverse document frequency</a></li>
<li class="chapter" data-level="4.2" data-path="tfidf.html"><a href="tfidf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>4.2</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="4.3" data-path="tfidf.html"><a href="tfidf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>4.3</b> The <code>bind_tf_idf</code> function</a></li>
<li class="chapter" data-level="4.4" data-path="tfidf.html"><a href="tfidf.html#a-corpus-of-physics-texts"><i class="fa fa-check"></i><b>4.4</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ngrams.html"><a href="ngrams.html"><i class="fa fa-check"></i><b>5</b> Working with combinations of words using n-grams and widyr</a><ul>
<li class="chapter" data-level="5.1" data-path="ngrams.html"><a href="ngrams.html#tokenizing-by-n-gram"><i class="fa fa-check"></i><b>5.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="5.1.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-filtering-n-grams"><i class="fa fa-check"></i><b>5.1.1</b> Counting and filtering n-grams</a></li>
<li class="chapter" data-level="5.1.2" data-path="ngrams.html"><a href="ngrams.html#analyzing-bigrams"><i class="fa fa-check"></i><b>5.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="5.1.3" data-path="ngrams.html"><a href="ngrams.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>5.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="5.1.4" data-path="ngrams.html"><a href="ngrams.html#visualizing-a-network-of-bigrams-with-igraph"><i class="fa fa-check"></i><b>5.1.4</b> Visualizing a network of bigrams with igraph</a></li>
<li class="chapter" data-level="5.1.5" data-path="ngrams.html"><a href="ngrams.html#visualizing-bigrams-in-other-texts"><i class="fa fa-check"></i><b>5.1.5</b> Visualizing bigrams in other texts</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-pairs-of-words-with-the-widyr-package"><i class="fa fa-check"></i><b>5.2</b> Counting and correlating pairs of words with the widyr package</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>5.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="5.2.2" data-path="ngrams.html"><a href="ngrams.html#pairwise-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="dtm.html"><a href="dtm.html"><i class="fa fa-check"></i><b>6</b> Tidying and casting document-term matrices</a><ul>
<li class="chapter" data-level="6.1" data-path="dtm.html"><a href="dtm.html#tidying-a-document-term-matrix"><i class="fa fa-check"></i><b>6.1</b> Tidying a document-term matrix</a></li>
<li class="chapter" data-level="6.2" data-path="dtm.html"><a href="dtm.html#casting-tidy-text-data-into-a-documenttermmatrix"><i class="fa fa-check"></i><b>6.2</b> Casting tidy text data into a DocumentTermMatrix</a></li>
<li class="chapter" data-level="6.3" data-path="dtm.html"><a href="dtm.html#tidying-corpus-objects-with-metadata"><i class="fa fa-check"></i><b>6.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="topicmodeling.html"><a href="topicmodeling.html"><i class="fa fa-check"></i><b>7</b> Topic modeling</a><ul>
<li class="chapter" data-level="7.1" data-path="topicmodeling.html"><a href="topicmodeling.html#the-great-library-heist"><i class="fa fa-check"></i><b>7.1</b> The great library heist</a></li>
<li class="chapter" data-level="7.2" data-path="topicmodeling.html"><a href="topicmodeling.html#latent-dirichlet-allocation-with-the-topicmodels-package"><i class="fa fa-check"></i><b>7.2</b> Latent Dirichlet allocation with the topicmodels package</a></li>
<li class="chapter" data-level="7.3" data-path="topicmodeling.html"><a href="topicmodeling.html#per-document-classification"><i class="fa fa-check"></i><b>7.3</b> Per-document classification</a></li>
<li class="chapter" data-level="7.4" data-path="topicmodeling.html"><a href="topicmodeling.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>7.4</b> By word assignments: <code>augment</code></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="twitter.html"><a href="twitter.html"><i class="fa fa-check"></i><b>8</b> Case study: comparing Twitter archives</a><ul>
<li class="chapter" data-level="8.1" data-path="twitter.html"><a href="twitter.html#getting-the-data-and-distribution-of-tweets"><i class="fa fa-check"></i><b>8.1</b> Getting the data and distribution of tweets</a></li>
<li class="chapter" data-level="8.2" data-path="twitter.html"><a href="twitter.html#word-frequencies-1"><i class="fa fa-check"></i><b>8.2</b> Word frequencies</a></li>
<li class="chapter" data-level="8.3" data-path="twitter.html"><a href="twitter.html#comparing-word-usage"><i class="fa fa-check"></i><b>8.3</b> Comparing word usage</a></li>
<li class="chapter" data-level="8.4" data-path="twitter.html"><a href="twitter.html#sentiment-analysis"><i class="fa fa-check"></i><b>8.4</b> Sentiment analysis</a></li>
<li class="chapter" data-level="8.5" data-path="twitter.html"><a href="twitter.html#words-that-contribute-to-sentiment-in-tweets"><i class="fa fa-check"></i><b>8.5</b> Words that contribute to sentiment in tweets</a></li>
<li class="chapter" data-level="8.6" data-path="twitter.html"><a href="twitter.html#favorites-and-retweets"><i class="fa fa-check"></i><b>8.6</b> Favorites and retweets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nasa.html"><a href="nasa.html"><i class="fa fa-check"></i><b>9</b> Case study: mining NASA metadata</a><ul>
<li class="chapter" data-level="9.1" data-path="nasa.html"><a href="nasa.html#getting-the-metadata"><i class="fa fa-check"></i><b>9.1</b> Getting the metadata</a></li>
<li class="chapter" data-level="9.2" data-path="nasa.html"><a href="nasa.html#wrangling-and-tidying-the-data"><i class="fa fa-check"></i><b>9.2</b> Wrangling and tidying the data</a></li>
<li class="chapter" data-level="9.3" data-path="nasa.html"><a href="nasa.html#some-initial-simple-exploration"><i class="fa fa-check"></i><b>9.3</b> Some initial simple exploration</a></li>
<li class="chapter" data-level="9.4" data-path="nasa.html"><a href="nasa.html#word-co-ocurrences-and-correlations"><i class="fa fa-check"></i><b>9.4</b> Word co-ocurrences and correlations</a></li>
<li class="chapter" data-level="9.5" data-path="nasa.html"><a href="nasa.html#calculating-tf-idf-for-the-description-fields"><i class="fa fa-check"></i><b>9.5</b> Calculating tf-idf for the description fields</a></li>
<li class="chapter" data-level="9.6" data-path="nasa.html"><a href="nasa.html#topic-modeling"><i class="fa fa-check"></i><b>9.6</b> Topic modeling</a></li>
<li class="chapter" data-level="9.7" data-path="nasa.html"><a href="nasa.html#interpreting-the-topic-model"><i class="fa fa-check"></i><b>9.7</b> Interpreting the topic model</a></li>
<li class="chapter" data-level="9.8" data-path="nasa.html"><a href="nasa.html#connecting-topic-modeling-with-keywords"><i class="fa fa-check"></i><b>9.8</b> Connecting topic modeling with keywords</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="usenet.html"><a href="usenet.html"><i class="fa fa-check"></i><b>10</b> Case study: analyzing usenet text</a><ul>
<li class="chapter" data-level="10.1" data-path="usenet.html"><a href="usenet.html#wrangling-the-data"><i class="fa fa-check"></i><b>10.1</b> Wrangling the data</a></li>
<li class="chapter" data-level="10.2" data-path="usenet.html"><a href="usenet.html#term-frequency-and-inverse-document-frequency-tf-idf"><i class="fa fa-check"></i><b>10.2</b> Term frequency and inverse document frequency: tf-idf</a></li>
<li class="chapter" data-level="10.3" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-1"><i class="fa fa-check"></i><b>10.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="10.4" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-word"><i class="fa fa-check"></i><b>10.4</b> Sentiment analysis by word</a></li>
<li class="chapter" data-level="10.5" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-message"><i class="fa fa-check"></i><b>10.5</b> Sentiment analysis by message</a></li>
<li class="chapter" data-level="10.6" data-path="usenet.html"><a href="usenet.html#n-grams"><i class="fa fa-check"></i><b>10.6</b> N-grams</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Tidy Text Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="twitter" class="section level1">
<h1><span class="header-section-number">8</span> Case study: comparing Twitter archives</h1>
<p>One type of text that has received its share of attention in recent years is text shared online via Twitter. In fact, several of the sentiment lexicons used in this book (and commonly used in general) were designed for use with and validated on tweets. Both of the authors of this book are on Twitter and are fairly regular users of it so in this case study, let’s compare the entire Twitter archives of <a href="https://twitter.com/juliasilge">Julia</a> and <a href="https://twitter.com/drob">David</a>.</p>
<div id="getting-the-data-and-distribution-of-tweets" class="section level2">
<h2><span class="header-section-number">8.1</span> Getting the data and distribution of tweets</h2>
<p>An individual can download their own Twitter archive by following <a href="https://support.twitter.com/articles/20170160">directions available here</a>. We each downloaded ours and will now open them up. Let’s use the lubridate package to convert the string timestamps to date-time objects and initially take a look at our tweeting patterns overall.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lubridate)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(readr)

tweets_julia &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/tweets_julia.csv&quot;</span>)
tweets_dave &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/tweets_dave.csv&quot;</span>)
tweets &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(tweets_julia %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;Julia&quot;</span>),
                    tweets_dave %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;David&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">timestamp =</span> <span class="kw">ymd_hms</span>(timestamp))

<span class="kw">ggplot</span>(tweets, <span class="kw">aes</span>(<span class="dt">x =</span> timestamp, <span class="dt">fill =</span> person)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">position =</span> <span class="st">&quot;identity&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/setup-1.png" width="768" /></p>
<p>David and Julia tweet at about the same rate currently and joined Twitter about a year apart from each other, but there about 5 years where David was not active on Twitter and Julia was. In total, Julia has about 4 times as many tweets as David.</p>
</div>
<div id="word-frequencies-1" class="section level2">
<h2><span class="header-section-number">8.2</span> Word frequencies</h2>
<p>Let’s use <code>unnest_tokens</code> to make a tidy dataframe of all the words in our tweets, and remove the common English stop words. There are certain conventions in how people use text on Twitter, so we will do a bit more work with our text here than, for example, we did with the narrative text from Project Gutenberg. The first <code>mutate</code> line below removes links and cleans out some characters that we don’t want. In the call to <code>unnest_tokens</code>, we unnest using a regex pattern, instead of just looking for single unigrams (words). This regex pattern is very useful for dealing with Twitter text; it retains hashtags and mentions of usernames with the <code>@</code> symbol. Because we have kept these types of symbols in the text, we can’t use a simple <code>anti_join</code> to remove stop words. Instead, we can take the approach shown in the <code>filter</code> line that uses <code>str_detect</code> from the stringr library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidytext)
<span class="kw">library</span>(stringr)

reg &lt;-<span class="st"> &quot;([^A-Za-z_</span><span class="ch">\\</span><span class="st">d#@&#39;]|&#39;(?![A-Za-z_</span><span class="ch">\\</span><span class="st">d#@]))&quot;</span>
tidy_tweets &lt;-<span class="st"> </span>tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_replace_all</span>(text, <span class="st">&quot;https://t.co/[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|http://[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT&quot;</span>, <span class="st">&quot;&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text, <span class="dt">token =</span> <span class="st">&quot;regex&quot;</span>, <span class="dt">pattern =</span> reg) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!word %in%<span class="st"> </span>stop_words$word,
         <span class="kw">str_detect</span>(word, <span class="st">&quot;[a-z]&quot;</span>))</code></pre></div>
<p>Now we can calculate word frequencies for each person</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">frequency &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">              </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">              </span><span class="kw">summarise</span>(<span class="dt">total =</span> <span class="kw">n</span>())) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">freq =</span> n/total)

frequency</code></pre></div>
<pre><code>## Source: local data frame [23,086 x 5]
## Groups: person [2]
## 
##    person           word     n total        freq
##     &lt;chr&gt;          &lt;chr&gt; &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;
## 1   Julia           time   567 76504 0.007411377
## 2   Julia    @selkie1970   565 76504 0.007385235
## 3   Julia       @skedman   518 76504 0.006770888
## 4   Julia            day   470 76504 0.006143470
## 5   Julia           baby   410 76504 0.005359197
## 6   David        #rstats   359 22074 0.016263477
## 7   Julia     @doctormac   342 76504 0.004470354
## 8   David @hadleywickham   306 22074 0.013862463
## 9   Julia           love   303 76504 0.003960577
## 10  Julia   @haleynburke   291 76504 0.003803723
## # ... with 23,076 more rows</code></pre>
<p>This is a lovely, tidy data frame but we would actually like to plot those frequencies on the x- and y-axes of a plot, so we will need to use an <code>inner_join</code> and make a different dataframe.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">frequency &lt;-<span class="st"> </span><span class="kw">inner_join</span>(frequency %&gt;%<span class="st"> </span>
<span class="st">                          </span><span class="kw">filter</span>(person ==<span class="st"> &quot;Julia&quot;</span>) %&gt;%<span class="st"> </span>
<span class="st">                          </span><span class="kw">rename</span>(<span class="dt">Julia =</span> freq),
                        frequency %&gt;%<span class="st"> </span>
<span class="st">                          </span><span class="kw">filter</span>(person ==<span class="st"> &quot;David&quot;</span>) %&gt;%<span class="st"> </span>
<span class="st">                          </span><span class="kw">rename</span>(<span class="dt">David =</span> freq),
                        <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(word, Julia, David)

frequency</code></pre></div>
<pre><code>## # A tibble: 3,520 × 3
##       word       Julia        David
##      &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;
## 1     time 0.007411377 0.0043490079
## 2      day 0.006143470 0.0014949715
## 3     baby 0.005359197 0.0001812087
## 4     love 0.003960577 0.0020385974
## 5    house 0.003790651 0.0001359065
## 6  morning 0.003659939 0.0004077195
## 7   people 0.003372373 0.0033523602
## 8     feel 0.003124020 0.0013137628
## 9   pretty 0.002954094 0.0010872520
## 10  school 0.002875667 0.0002265108
## # ... with 3,510 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scales)
<span class="kw">ggplot</span>(frequency, <span class="kw">aes</span>(Julia, David)) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">alpha =</span> <span class="fl">0.1</span>, <span class="dt">size =</span> <span class="fl">2.5</span>, <span class="dt">width =</span> <span class="fl">0.25</span>, <span class="dt">height =</span> <span class="fl">0.25</span>) +
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> word), <span class="dt">check_overlap =</span> <span class="ot">TRUE</span>, <span class="dt">vjust =</span> <span class="fl">1.5</span>) +
<span class="st">  </span><span class="kw">scale_x_log10</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/spread-1.png" width="672" /></p>
<p>This may not even need to be pointed out, but David and Julia have used their Twitter accounts rather differently over the course of the past several years. David has used his Twitter account almost exclusively for professional purposes since he became more active, while Julia used it for entirely personal purposes until late 2015. We see these differences immediately in this plot exploring word frequencies, and they will continue to be obvious in the rest of this chapter. Words near the red line in this plot are used with about equal frequencies by David and Julia, while words far away from the line are used much more by one person compared to the other. Because of the inner join we did above, words, hashtags, and usernames that appear in this plot are ones that we have both used at least once.</p>
</div>
<div id="comparing-word-usage" class="section level2">
<h2><span class="header-section-number">8.3</span> Comparing word usage</h2>
<p>We just made a plot comparing raw word frequencies, but now let’s find which words are more or less likely to come from each person’s account using the log odds ratio. First, let’s use <code>str_detect</code> to remove Twitter usernames from the <code>word</code> column, because otherwise, the results here are dominated only by people who Julia or David know and the other does not. After removing these, we count how many times each person uses each word and keep only the words used more than 5 times. After a <code>spread</code> operation, we can calculate the log odds ratio for each word, using</p>
<p><span class="math display">\[\text{log odds ratio} = \ln{\left(\frac{\left[\frac{n+1}{\text{total}+1}\right]_\text{David}}{\left[\frac{n+1}{\text{total}+1}\right]_\text{Julia}}\right)}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of times the word in question is used by each person and the total indicates the total words for each person.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyr)
word_ratios &lt;-<span class="st"> </span>tidy_tweets %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">str_detect</span>(word, <span class="st">&quot;^@&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">count</span>(word, person) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">sum</span>(n) &gt;=<span class="st"> </span><span class="dv">5</span>) %&gt;%
<span class="st">  </span><span class="kw">spread</span>(person, n, <span class="dt">fill =</span> <span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate_each</span>(<span class="kw">funs</span>((. +<span class="st"> </span><span class="dv">1</span>) /<span class="st"> </span><span class="kw">sum</span>(. +<span class="st"> </span><span class="dv">1</span>)), -word) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">logratio =</span> <span class="kw">log</span>(David /<span class="st"> </span>Julia)) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(logratio))</code></pre></div>
<p>What are some words that are about equally likely to come from David or Julia’s account?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_ratios %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">abs</span>(logratio))</code></pre></div>
<pre><code>## # A tibble: 3,483 × 4
##           word        David        Julia    logratio
##          &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
## 1        alive 0.0001932118 0.0001920358 0.006105552
## 2       fallen 0.0001932118 0.0001920358 0.006105552
## 3        focus 0.0001932118 0.0001920358 0.006105552
## 4    forgotten 0.0001932118 0.0001920358 0.006105552
## 5  information 0.0001932118 0.0001920358 0.006105552
## 6         nice 0.0023185419 0.0023044290 0.006105552
## 7     painting 0.0001932118 0.0001920358 0.006105552
## 8        phone 0.0007728473 0.0007681430 0.006105552
## 9       random 0.0001932118 0.0001920358 0.006105552
## 10      system 0.0003864236 0.0003840715 0.006105552
## # ... with 3,473 more rows</code></pre>
<p>Which words are most likely to be from Julia’s account or from David’s account? Let’s just take the top 15 most distinctive words for each account and plot them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_ratios %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(logratio &lt;<span class="st"> </span><span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">15</span>, <span class="kw">abs</span>(logratio)) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, logratio)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, logratio, <span class="dt">fill =</span> logratio &lt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;log odds ratio (David/Julia)&quot;</span>) +
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;David&quot;</span>, <span class="st">&quot;Julia&quot;</span>))</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/plotratios-1.png" width="672" /></p>
<p>So David has tweeted about bioinformatics and Stack Overflow while Julia has been tweeting about preschool, naps, and the snow.</p>
</div>
<div id="sentiment-analysis" class="section level2">
<h2><span class="header-section-number">8.4</span> Sentiment analysis</h2>
<p>David and Julia have tweeted using different words, reflecting the difference of a professional vs. a personal Twitter account over the bulk of our tweets, but what about the sentiment of each person’s tweets? Let’s look for similarities and differences in the emotional content of these tweets.</p>
<p>First, let’s find the total number of words from each person; Julia has a much larger total number of tweets than David.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals &lt;-<span class="st"> </span>tidy_tweets %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">total_words =</span> <span class="kw">n</span>())

totals</code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   person total_words
##    &lt;chr&gt;       &lt;int&gt;
## 1  David       22074
## 2  Julia       76504</code></pre>
<p>Now let’s measure the sentiment for each word in our tweet archives. First we use <code>inner_join</code> and the NRC sentiment lexicon to identify words that are associated with joy, anger, surprise, etc. Then we count how many of these words each person used. Next, we join the data frame with the total words so that we will be able to look at proportions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sentiment_by_person &lt;-<span class="st"> </span>tidy_tweets %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;nrc&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">count</span>(sentiment, person) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">left_join</span>(totals) %&gt;%
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">words =</span> n)

sentiment_by_person</code></pre></div>
<pre><code>## # A tibble: 20 × 4
##       sentiment person words total_words
##           &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;       &lt;int&gt;
## 1         anger  David   447       22074
## 2         anger  Julia  2235       76504
## 3  anticipation  David   744       22074
## 4  anticipation  Julia  4683       76504
## 5       disgust  David   304       22074
## 6       disgust  Julia  2271       76504
## 7          fear  David   497       22074
## 8          fear  Julia  2667       76504
## 9           joy  David   520       22074
## 10          joy  Julia  5202       76504
## 11     negative  David  1078       22074
## 12     negative  Julia  5007       76504
## 13     positive  David  1650       22074
## 14     positive  Julia  9080       76504
## 15      sadness  David   447       22074
## 16      sadness  Julia  2862       76504
## 17     surprise  David   311       22074
## 18     surprise  Julia  2022       76504
## 19        trust  David   914       22074
## 20        trust  Julia  4660       76504</code></pre>
<p>This is count data, so let’s use a Poisson test to measure the difference between David’s word use and Julia’s. We want to test each sentiment separately, so first let’s <code>nest</code> the data so that we have a list-column with the data for each sentiment. Then let’s use <code>map</code> from the purrr library to apply the Poisson test to each little data frame individually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(purrr)

sentiment_differences &lt;-<span class="st"> </span>sentiment_by_person %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">nest</span>(-sentiment) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">tests =</span> <span class="kw">map</span>(data, ~<span class="st"> </span><span class="kw">poisson.test</span>(.$words, .$total_words)))

sentiment_differences</code></pre></div>
<pre><code>## # A tibble: 10 × 3
##       sentiment             data       tests
##           &lt;chr&gt;           &lt;list&gt;      &lt;list&gt;
## 1         anger &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 2  anticipation &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 3       disgust &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 4          fear &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 5           joy &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 6      negative &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 7      positive &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 8       sadness &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 9      surprise &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;
## 10        trust &lt;tibble [2 × 3]&gt; &lt;S3: htest&gt;</code></pre>
<p>Now we can use <code>tidy</code> from the broom library and <code>unnest</code> to get back to a data frame without list-columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)

sentiment_differences &lt;-<span class="st"> </span>sentiment_differences %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>(<span class="kw">map</span>(tests, tidy)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(-data, -tests)

sentiment_differences</code></pre></div>
<pre><code>## # A tibble: 10 × 9
##       sentiment  estimate statistic       p.value parameter  conf.low conf.high
##           &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1         anger 0.6931594       447  2.231075e-13  600.5647 0.6248241 0.7675962
## 2  anticipation 0.5506199       744  4.746477e-59 1215.2366 0.5089522 0.5950353
## 3       disgust 0.4639376       304  1.948674e-43  576.6048 0.4102417 0.5231585
## 4          fear 0.6458572       497  7.419529e-21  708.4962 0.5857010 0.7110327
## 5           joy 0.3464464       520 8.526864e-156 1281.2943 0.3159762 0.3792032
## 6      negative 0.7461812      1078  2.789775e-19 1362.5788 0.6980139 0.7971034
## 7      positive 0.6297979      1650  5.922101e-74 2402.7067 0.5972567 0.6637915
## 8       sadness 0.5413037       447  2.145804e-38  740.9652 0.4888538 0.5982540
## 9      surprise 0.5330677       311  1.091952e-28  522.4152 0.4715559 0.6009547
## 10        trust 0.6797722       914  1.103873e-28 1248.1535 0.6325550 0.7298829
## # ... with 2 more variables: method &lt;fctr&gt;, alternative &lt;fctr&gt;</code></pre>
<p>We have measured the differences in sentiment between our two Twitter feeds; now let’s plot them!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scales)

sentiment_differences %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sentiment =</span> <span class="kw">reorder</span>(sentiment, estimate)) %&gt;%
<span class="st">  </span><span class="kw">mutate_each</span>(<span class="kw">funs</span>(<span class="dv">1</span> -<span class="st"> </span>.), estimate, conf.low, conf.high) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(estimate, sentiment)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">2.5</span>) +
<span class="st">  </span><span class="kw">geom_errorbarh</span>(<span class="kw">aes</span>(<span class="dt">xmin =</span> conf.low, <span class="dt">xmax =</span> conf.high)) +
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;% increase for Julia relative to David&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Sentiment&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/plot_unnest-1.png" width="672" /></p>
<p>We see here that Julia uses about 65% more words related to joy and about 55% more words related to disgust than David; in fact, she uses more of <em>all</em> words associated with sentiments, both negative and positive. David’s tweets are more neutral in sentiment while Julia’s involve more dramatic word choices that communicate more emotions. This, again, reflects the different ways we have used our Twitter accounts.</p>
</div>
<div id="words-that-contribute-to-sentiment-in-tweets" class="section level2">
<h2><span class="header-section-number">8.5</span> Words that contribute to sentiment in tweets</h2>
<p>Which specific words are driving this difference in sentiment between Julia’s and David’s accounts? Let’s go back to the data frame that contains the odds ratios for each word. We can use the same <code>inner_join</code> with the NRC sentiment lexicon to identify the words that are associated with emotions like joy, disgust, and so forth; these are exactly the words that are driving the differences in measured sentiment between our two accounts. Now let’s find the words with the largest odds ratios. When we did a similar operation above, we took the <code>top_n</code> for each individual person to see the words that were more likely to come from each Twitter feed, but now we will take the <code>top_n</code> for each sentiment to see which word choices are the cause of the marked difference in sentiment.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_ratios %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;nrc&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!sentiment %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;positive&quot;</span>, <span class="st">&quot;negative&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sentiment =</span> <span class="kw">reorder</span>(sentiment, -logratio),
         <span class="dt">word =</span> <span class="kw">reorder</span>(word, -logratio)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(sentiment) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>, <span class="kw">abs</span>(logratio)) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, logratio, <span class="dt">fill =</span> logratio &lt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>sentiment, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">nrow =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>)) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;log odds ratio (David/Julia)&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;David&quot;</span>, <span class="st">&quot;Julia&quot;</span>))</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/plotsentiments-1.png" width="768" /></p>
<p>We see here that Julia’s tweets have such a higher joy score because she has tweeted about things like birthdays, babies, and things that are delicious and sweet (probably sweet babies even!). Her disgust score is higher because she has tweeted about pregnancy, diapers, and being sick with stomach bugs. We see that there are some misidentifications visible in this plot; David’s only joy word should actually be Excel, the Microsoft product (not excel, the verb) and he uses “wrangling” in the sense of “data wrangling”, which is not fraught with disgust, anger, or fear (or is it?!). These misidentifications due to domain-specific word use mean that the difference between Julia and David for joy, digust, and some others should actually be even larger.</p>
</div>
<div id="favorites-and-retweets" class="section level2">
<h2><span class="header-section-number">8.6</span> Favorites and retweets</h2>
<p>Another important characteristic of tweets is how many times they are favorited or retweeted. Let’s explore which words are more likely to be retweeted or favorited for Julia’s and David’s tweets. When a user downloads their own Twitter archive, favorites and retweets are not included, so we constructed another dataset of the author’s tweets that includes this information. We accessed our own tweets via the Twitter API and downloaded about 3200 tweets for each person. In both cases, that is about the last 18 months worth of Twitter activity. This corresponds to a period of increasing activity and increasing numbers of followers for both of us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tweets_julia &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/juliasilge_tweets.csv&quot;</span>)
tweets_dave &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/drob_tweets.csv&quot;</span>)
tweets &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(tweets_julia %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;Julia&quot;</span>),
                    tweets_dave %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;David&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">created_at =</span> <span class="kw">ymd_hms</span>(created_at))</code></pre></div>
<p>Now that we have this second, smaller set of only recent tweets, let’s use <code>unnest_tokens</code> to transform these tweets to a tidy data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tidy_tweets &lt;-<span class="st"> </span>tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">str_detect</span>(text, <span class="st">&quot;^RT&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_replace_all</span>(text, <span class="st">&quot;https://t.co/[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|http://[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT&quot;</span>, <span class="st">&quot;&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text, <span class="dt">token =</span> <span class="st">&quot;regex&quot;</span>, <span class="dt">pattern =</span> reg) %&gt;%
<span class="st">  </span><span class="kw">anti_join</span>(stop_words)

tidy_tweets</code></pre></div>
<pre><code>## # A tibble: 30,725 × 7
##              id          created_at             source retweets favorites person        word
##           &lt;dbl&gt;              &lt;dttm&gt;              &lt;chr&gt;    &lt;int&gt;     &lt;int&gt;  &lt;chr&gt;       &lt;chr&gt;
## 1  8.043967e+17 2016-12-01 18:48:07 Twitter Web Client        4         6  David         j&#39;s
## 2  8.043611e+17 2016-12-01 16:26:39 Twitter Web Client        8        12  David   bangalore
## 3  8.043611e+17 2016-12-01 16:26:39 Twitter Web Client        8        12  David      london
## 4  8.043435e+17 2016-12-01 15:16:48 Twitter for iPhone        0         1  David @rodneyfort
## 5  8.043120e+17 2016-12-01 13:11:37 Twitter for iPhone        0         1  Julia         sho
## 6  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David       arbor
## 7  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David       arbor
## 8  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David         ann
## 9  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David         ann
## 10 8.040582e+17 2016-11-30 20:23:14 Twitter Web Client       30        41  David          sf
## # ... with 30,715 more rows</code></pre>
<p>To start with, let’s look at retweets. Let’s find the total number of retweets for each person.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person, id) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">rts =</span> <span class="kw">sum</span>(retweets)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">total_rts =</span> <span class="kw">sum</span>(rts))

totals</code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   person total_rts
##    &lt;chr&gt;     &lt;int&gt;
## 1  David    111863
## 2  Julia     12906</code></pre>
<p>Now let’s find the number of retweets for each word and person (making sure to count each tweet only once) and join to this data frame of retweet totals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_by_rts &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(id, word, person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">rts =</span> <span class="kw">sum</span>(retweets)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person, word) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">retweets =</span> <span class="kw">sum</span>(rts)) %&gt;%
<span class="st">  </span><span class="kw">left_join</span>(totals) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(retweets !=<span class="st"> </span><span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>()

word_by_rts %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(retweets))</code></pre></div>
<pre><code>## # A tibble: 3,519 × 4
##    person     word retweets total_rts
##     &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt;     &lt;int&gt;
## 1   David  #rstats     6833    111863
## 2   David    trump     3778    111863
## 3   David     post     2936    111863
## 4   David analysis     2732    111863
## 5   David   tweets     2067    111863
## 6   David  android     1820    111863
## 7   David overflow     1768    111863
## 8   David    stack     1768    111863
## 9   David  angrier     1757    111863
## 10  David confirms     1757    111863
## # ... with 3,509 more rows</code></pre>
<p>Now we can plot the words that have contributed the most to each of our retweets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plot_rts &lt;-<span class="st"> </span>word_by_rts %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ratio =</span> retweets /<span class="st"> </span>total_rts) %&gt;%<span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">8</span>, ratio) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(person, ratio) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">order =</span> <span class="kw">row_number</span>())

<span class="kw">ggplot</span>(plot_rts, <span class="kw">aes</span>(order, ratio, <span class="dt">fill =</span> person)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>person, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> plot_rts$order, <span class="dt">labels =</span> plot_rts$word) +
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;proportion of total RTs due to each word&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/plot_rts-1.png" width="960" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="topicmodeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nasa.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dgrtwo/tidy-text-mining/edit/master/08-tweet-archives.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
