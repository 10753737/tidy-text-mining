<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Tidy Text Mining with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Tidy Text Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/tidycover.png" />
  <meta property="og:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="github-repo" content="dgrtwo/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Tidy Text Mining with R" />
  
  <meta name="twitter:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="twitter:image" content="images/tidycover.png" />

<meta name="author" content="Julia Silge and David Robinson">


<meta name="date" content="2017-01-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="topicmodeling.html">
<link rel="next" href="nasa.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68765210-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Tidy Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Tidy Text Mining with R</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-tidy-text"><i class="fa fa-check"></i><b>1.1</b> What is tidy text?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#outline"><i class="fa fa-check"></i><b>1.3</b> Outline</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#topics-this-book-does-not-cover"><i class="fa fa-check"></i><b>1.4</b> Topics this book does not cover</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#acknowledgements"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tidytext.html"><a href="tidytext.html"><i class="fa fa-check"></i><b>2</b> The tidy text format</a><ul>
<li class="chapter" data-level="2.1" data-path="tidytext.html"><a href="tidytext.html#the-unnest_tokens-function"><i class="fa fa-check"></i><b>2.1</b> The <code>unnest_tokens</code> function</a></li>
<li class="chapter" data-level="2.2" data-path="tidytext.html"><a href="tidytext.html#tidying-the-works-of-jane-austen"><i class="fa fa-check"></i><b>2.2</b> Tidying the works of Jane Austen</a></li>
<li class="chapter" data-level="2.3" data-path="tidytext.html"><a href="tidytext.html#the-gutenbergr-package"><i class="fa fa-check"></i><b>2.3</b> The gutenbergr package</a></li>
<li class="chapter" data-level="2.4" data-path="tidytext.html"><a href="tidytext.html#word-frequencies"><i class="fa fa-check"></i><b>2.4</b> Word frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sentiment.html"><a href="sentiment.html"><i class="fa fa-check"></i><b>3</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="3.1" data-path="sentiment.html"><a href="sentiment.html#the-sentiments-dataset"><i class="fa fa-check"></i><b>3.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="3.2" data-path="sentiment.html"><a href="sentiment.html#sentiment-analysis-with-inner-join"><i class="fa fa-check"></i><b>3.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="3.3" data-path="sentiment.html"><a href="sentiment.html#comparing-the-three-sentiment-dictionaries"><i class="fa fa-check"></i><b>3.3</b> Comparing the three sentiment dictionaries</a></li>
<li class="chapter" data-level="3.4" data-path="sentiment.html"><a href="sentiment.html#most-common-positive-and-negative-words"><i class="fa fa-check"></i><b>3.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="3.5" data-path="sentiment.html"><a href="sentiment.html#wordclouds"><i class="fa fa-check"></i><b>3.5</b> Wordclouds</a></li>
<li class="chapter" data-level="3.6" data-path="sentiment.html"><a href="sentiment.html#looking-at-units-beyond-just-words"><i class="fa fa-check"></i><b>3.6</b> Looking at units beyond just words</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tfidf.html"><a href="tfidf.html"><i class="fa fa-check"></i><b>4</b> Analyzing word and document frequency: tf-idf</a><ul>
<li class="chapter" data-level="4.1" data-path="tfidf.html"><a href="tfidf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>4.1</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="4.2" data-path="tfidf.html"><a href="tfidf.html#zipfs-law"><i class="fa fa-check"></i><b>4.2</b> Zipf’s law</a></li>
<li class="chapter" data-level="4.3" data-path="tfidf.html"><a href="tfidf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>4.3</b> The <code>bind_tf_idf</code> function</a></li>
<li class="chapter" data-level="4.4" data-path="tfidf.html"><a href="tfidf.html#a-corpus-of-physics-texts"><i class="fa fa-check"></i><b>4.4</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ngrams.html"><a href="ngrams.html"><i class="fa fa-check"></i><b>5</b> Working with combinations of words using n-grams and widyr</a><ul>
<li class="chapter" data-level="5.1" data-path="ngrams.html"><a href="ngrams.html#tokenizing-by-n-gram"><i class="fa fa-check"></i><b>5.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="5.1.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-filtering-n-grams"><i class="fa fa-check"></i><b>5.1.1</b> Counting and filtering n-grams</a></li>
<li class="chapter" data-level="5.1.2" data-path="ngrams.html"><a href="ngrams.html#analyzing-bigrams"><i class="fa fa-check"></i><b>5.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="5.1.3" data-path="ngrams.html"><a href="ngrams.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>5.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="5.1.4" data-path="ngrams.html"><a href="ngrams.html#visualizing-a-network-of-bigrams-with-igraph"><i class="fa fa-check"></i><b>5.1.4</b> Visualizing a network of bigrams with igraph</a></li>
<li class="chapter" data-level="5.1.5" data-path="ngrams.html"><a href="ngrams.html#visualizing-bigrams-in-other-texts"><i class="fa fa-check"></i><b>5.1.5</b> Visualizing bigrams in other texts</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-pairs-of-words-with-the-widyr-package"><i class="fa fa-check"></i><b>5.2</b> Counting and correlating pairs of words with the widyr package</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>5.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="5.2.2" data-path="ngrams.html"><a href="ngrams.html#pairwise-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="dtm.html"><a href="dtm.html"><i class="fa fa-check"></i><b>6</b> Tidying and casting document-term matrices</a><ul>
<li class="chapter" data-level="6.1" data-path="dtm.html"><a href="dtm.html#tidying-a-document-term-matrix"><i class="fa fa-check"></i><b>6.1</b> Tidying a document-term matrix</a><ul>
<li class="chapter" data-level="6.1.1" data-path="dtm.html"><a href="dtm.html#tidying-documenttermmatrix-objects"><i class="fa fa-check"></i><b>6.1.1</b> Tidying DocumentTermMatrix objects</a></li>
<li class="chapter" data-level="6.1.2" data-path="dtm.html"><a href="dtm.html#tidying-dfm-objects"><i class="fa fa-check"></i><b>6.1.2</b> Tidying dfm objects</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="dtm.html"><a href="dtm.html#casting-tidy-text-data-into-a-matrix"><i class="fa fa-check"></i><b>6.2</b> Casting tidy text data into a matrix</a></li>
<li class="chapter" data-level="6.3" data-path="dtm.html"><a href="dtm.html#tidying-corpus-objects-with-metadata"><i class="fa fa-check"></i><b>6.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="topicmodeling.html"><a href="topicmodeling.html"><i class="fa fa-check"></i><b>7</b> Topic modeling</a><ul>
<li class="chapter" data-level="7.1" data-path="topicmodeling.html"><a href="topicmodeling.html#the-great-library-heist"><i class="fa fa-check"></i><b>7.1</b> The great library heist</a></li>
<li class="chapter" data-level="7.2" data-path="topicmodeling.html"><a href="topicmodeling.html#latent-dirichlet-allocation-with-the-topicmodels-package"><i class="fa fa-check"></i><b>7.2</b> Latent Dirichlet allocation with the topicmodels package</a></li>
<li class="chapter" data-level="7.3" data-path="topicmodeling.html"><a href="topicmodeling.html#per-document-classification"><i class="fa fa-check"></i><b>7.3</b> Per-document classification</a></li>
<li class="chapter" data-level="7.4" data-path="topicmodeling.html"><a href="topicmodeling.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>7.4</b> By word assignments: <code>augment</code></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="twitter.html"><a href="twitter.html"><i class="fa fa-check"></i><b>8</b> Case study: comparing Twitter archives</a><ul>
<li class="chapter" data-level="8.1" data-path="twitter.html"><a href="twitter.html#getting-the-data-and-distribution-of-tweets"><i class="fa fa-check"></i><b>8.1</b> Getting the data and distribution of tweets</a></li>
<li class="chapter" data-level="8.2" data-path="twitter.html"><a href="twitter.html#word-frequencies-1"><i class="fa fa-check"></i><b>8.2</b> Word frequencies</a></li>
<li class="chapter" data-level="8.3" data-path="twitter.html"><a href="twitter.html#comparing-word-usage"><i class="fa fa-check"></i><b>8.3</b> Comparing word usage</a></li>
<li class="chapter" data-level="8.4" data-path="twitter.html"><a href="twitter.html#changes-in-word-use"><i class="fa fa-check"></i><b>8.4</b> Changes in word use</a></li>
<li class="chapter" data-level="8.5" data-path="twitter.html"><a href="twitter.html#favorites-and-retweets"><i class="fa fa-check"></i><b>8.5</b> Favorites and retweets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nasa.html"><a href="nasa.html"><i class="fa fa-check"></i><b>9</b> Case study: mining NASA metadata</a><ul>
<li class="chapter" data-level="9.1" data-path="nasa.html"><a href="nasa.html#how-data-is-organized-at-nasa"><i class="fa fa-check"></i><b>9.1</b> How data is organized at NASA</a><ul>
<li class="chapter" data-level="9.1.1" data-path="nasa.html"><a href="nasa.html#wrangling-and-tidying-the-data"><i class="fa fa-check"></i><b>9.1.1</b> Wrangling and tidying the data</a></li>
<li class="chapter" data-level="9.1.2" data-path="nasa.html"><a href="nasa.html#some-initial-simple-exploration"><i class="fa fa-check"></i><b>9.1.2</b> Some initial simple exploration</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="nasa.html"><a href="nasa.html#word-co-ocurrences-and-correlations"><i class="fa fa-check"></i><b>9.2</b> Word co-ocurrences and correlations</a><ul>
<li class="chapter" data-level="9.2.1" data-path="nasa.html"><a href="nasa.html#networks-of-description-and-title-words"><i class="fa fa-check"></i><b>9.2.1</b> Networks of Description and Title Words</a></li>
<li class="chapter" data-level="9.2.2" data-path="nasa.html"><a href="nasa.html#networks-of-keywords"><i class="fa fa-check"></i><b>9.2.2</b> Networks of Keywords</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="nasa.html"><a href="nasa.html#calculating-tf-idf-for-the-description-fields"><i class="fa fa-check"></i><b>9.3</b> Calculating tf-idf for the description fields</a><ul>
<li class="chapter" data-level="9.3.1" data-path="nasa.html"><a href="nasa.html#what-is-tf-idf-for-the-description-field-words"><i class="fa fa-check"></i><b>9.3.1</b> What is tf-idf for the description field words?</a></li>
<li class="chapter" data-level="9.3.2" data-path="nasa.html"><a href="nasa.html#connecting-description-fields-to-keywords"><i class="fa fa-check"></i><b>9.3.2</b> Connecting description fields to keywords</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="nasa.html"><a href="nasa.html#topic-modeling"><i class="fa fa-check"></i><b>9.4</b> Topic modeling</a><ul>
<li class="chapter" data-level="9.4.1" data-path="nasa.html"><a href="nasa.html#casting-to-a-document-term-matrix"><i class="fa fa-check"></i><b>9.4.1</b> Casting to a document-term matrix</a></li>
<li class="chapter" data-level="9.4.2" data-path="nasa.html"><a href="nasa.html#ready-for-topic-modeling"><i class="fa fa-check"></i><b>9.4.2</b> Ready for topic modeling</a></li>
<li class="chapter" data-level="9.4.3" data-path="nasa.html"><a href="nasa.html#interpreting-the-topic-model"><i class="fa fa-check"></i><b>9.4.3</b> Interpreting the topic model</a></li>
<li class="chapter" data-level="9.4.4" data-path="nasa.html"><a href="nasa.html#connecting-topic-modeling-with-keywords"><i class="fa fa-check"></i><b>9.4.4</b> Connecting topic modeling with keywords</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="usenet.html"><a href="usenet.html"><i class="fa fa-check"></i><b>10</b> Case study: analyzing usenet text</a><ul>
<li class="chapter" data-level="10.1" data-path="usenet.html"><a href="usenet.html#wrangling-the-data"><i class="fa fa-check"></i><b>10.1</b> Wrangling the data</a></li>
<li class="chapter" data-level="10.2" data-path="usenet.html"><a href="usenet.html#term-frequency-and-inverse-document-frequency-tf-idf"><i class="fa fa-check"></i><b>10.2</b> Term frequency and inverse document frequency: tf-idf</a></li>
<li class="chapter" data-level="10.3" data-path="usenet.html"><a href="usenet.html#sentiment-analysis"><i class="fa fa-check"></i><b>10.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="10.4" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-word"><i class="fa fa-check"></i><b>10.4</b> Sentiment analysis by word</a></li>
<li class="chapter" data-level="10.5" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-message"><i class="fa fa-check"></i><b>10.5</b> Sentiment analysis by message</a></li>
<li class="chapter" data-level="10.6" data-path="usenet.html"><a href="usenet.html#n-grams"><i class="fa fa-check"></i><b>10.6</b> N-grams</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Tidy Text Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="twitter" class="section level1">
<h1><span class="header-section-number">8</span> Case study: comparing Twitter archives</h1>
<p>One type of text that has risen to attention in recent years is text shared online via Twitter. In fact, several of the sentiment lexicons used in this book (and commonly used in general) were designed for use with and validated on tweets. Both of the authors of this book are on Twitter and are fairly regular users of it so in this case study, let’s compare the entire Twitter archives of <a href="https://twitter.com/juliasilge">Julia</a> and <a href="https://twitter.com/drob">David</a>.</p>
<p>TODO: download Twitter archives again</p>
<div id="getting-the-data-and-distribution-of-tweets" class="section level2">
<h2><span class="header-section-number">8.1</span> Getting the data and distribution of tweets</h2>
<p>An individual can download their own Twitter archive by following <a href="https://support.twitter.com/articles/20170160">directions available on Twitter’s website</a>. We each downloaded ours and will now open them up. Let’s use the lubridate package to convert the string timestamps to date-time objects and initially take a look at our tweeting patterns overall.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lubridate)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(readr)

tweets_julia &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/tweets_julia.csv&quot;</span>)
tweets_dave &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/tweets_dave.csv&quot;</span>)
tweets &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(tweets_julia %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;Julia&quot;</span>),
                    tweets_dave %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;David&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">timestamp =</span> <span class="kw">ymd_hms</span>(timestamp))

<span class="kw">ggplot</span>(tweets, <span class="kw">aes</span>(<span class="dt">x =</span> timestamp, <span class="dt">fill =</span> person)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">position =</span> <span class="st">&quot;identity&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/setup-1.png" width="768" /></p>
<p>David and Julia tweet at about the same rate currently and joined Twitter about a year apart from each other, but there were about 5 years where David was not active on Twitter and Julia was. In total, Julia has about 4 times as many tweets as David.</p>
</div>
<div id="word-frequencies-1" class="section level2">
<h2><span class="header-section-number">8.2</span> Word frequencies</h2>
<p>Let’s use <code>unnest_tokens</code> to make a tidy dataframe of all the words in our tweets, and remove the common English stop words. There are certain conventions in how people use text on Twitter, so we will do a bit more work with our text here than, for example, we did with the narrative text from Project Gutenberg. The first <code>mutate</code> line below removes links and cleans out some characters that we don’t want like ampersands. In the call to <code>unnest_tokens</code>, we unnest using a regex pattern, instead of just looking for single unigrams (words). This regex pattern is very useful for dealing with Twitter text; it retains hashtags and mentions of usernames with the <code>@</code> symbol. Because we have kept these types of symbols in the text, we can’t use a simple <code>anti_join</code> to remove stop words. Instead, we can take the approach shown in the <code>filter</code> line that uses <code>str_detect</code> from the stringr library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidytext)
<span class="kw">library</span>(stringr)

reg &lt;-<span class="st"> &quot;([^A-Za-z_</span><span class="ch">\\</span><span class="st">d#@&#39;]|&#39;(?![A-Za-z_</span><span class="ch">\\</span><span class="st">d#@]))&quot;</span>
tidy_tweets &lt;-<span class="st"> </span>tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_replace_all</span>(text, <span class="st">&quot;https://t.co/[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|http://[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT|https&quot;</span>, <span class="st">&quot;&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text, <span class="dt">token =</span> <span class="st">&quot;regex&quot;</span>, <span class="dt">pattern =</span> reg) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!word %in%<span class="st"> </span>stop_words$word,
         <span class="kw">str_detect</span>(word, <span class="st">&quot;[a-z]&quot;</span>))</code></pre></div>
<p>Now we can calculate word frequencies for each person. First, we group by person and count how many times each person used each word. Then we use <code>left_join</code> to add a column of the total number of words used by each person. (This is higher for Julia than David since she has more tweets than David.) Finally, we calculate a frequency for each person and word.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">frequency &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">              </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">              </span><span class="kw">summarise</span>(<span class="dt">total =</span> <span class="kw">n</span>())) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">freq =</span> n/total)

frequency</code></pre></div>
<pre><code>## Source: local data frame [23,084 x 5]
## Groups: person [2]
## 
##    person           word     n total        freq
##     &lt;chr&gt;          &lt;chr&gt; &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;
## 1   Julia           time   567 76439 0.007417679
## 2   Julia    @selkie1970   565 76439 0.007391515
## 3   Julia       @skedman   518 76439 0.006776645
## 4   Julia            day   470 76439 0.006148694
## 5   Julia           baby   410 76439 0.005363754
## 6   David        #rstats   359 21984 0.016330058
## 7   Julia     @doctormac   342 76439 0.004474156
## 8   David @hadleywickham   306 21984 0.013919214
## 9   Julia           love   303 76439 0.003963945
## 10  Julia   @haleynburke   291 76439 0.003806957
## # ... with 23,074 more rows</code></pre>
<p>This is a nice, tidy data frame but we would actually like to plot those frequencies on the x- and y-axes of a plot, so we will need to use <code>spread</code> from tidyr make a differently shaped dataframe.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyr)

frequency &lt;-<span class="st"> </span>frequency %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(person, word, freq) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(person, freq) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(Julia, David)

frequency</code></pre></div>
<pre><code>## # A tibble: 19,565 × 3
##              word        David        Julia
##             &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;
## 1    @_aaronmiles 4.548763e-05 1.308233e-05
## 2     @_cingraham 4.548763e-05 1.308233e-05
## 3      @amandacox 4.548763e-05 1.308233e-05
## 4      @antoviral 4.548763e-05 1.308233e-05
## 5    @astrobiased 4.548763e-05 1.308233e-05
## 6     @astrokatie 4.548763e-05 1.308233e-05
## 7           @b0rk 4.548763e-05 1.308233e-05
## 8      @benhamner 4.548763e-05 1.308233e-05
## 9       @calli993 4.548763e-05 1.308233e-05
## 10 @clarecorthell 4.548763e-05 1.308233e-05
## # ... with 19,555 more rows</code></pre>
<p>Now this is ready for us to plot. Let’s use <code>geom_jitter</code> so that we don’t see the discreteness at the low end of frequency as much.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scales)

<span class="kw">ggplot</span>(frequency, <span class="kw">aes</span>(Julia, David)) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">alpha =</span> <span class="fl">0.1</span>, <span class="dt">size =</span> <span class="fl">2.5</span>, <span class="dt">width =</span> <span class="fl">0.25</span>, <span class="dt">height =</span> <span class="fl">0.25</span>) +
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> word), <span class="dt">check_overlap =</span> <span class="ot">TRUE</span>, <span class="dt">vjust =</span> <span class="fl">1.5</span>) +
<span class="st">  </span><span class="kw">scale_x_log10</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>This may not even need to be pointed out, but David and Julia have used their Twitter accounts rather differently over the course of the past several years. David has used his Twitter account almost exclusively for professional purposes since he became more active, while Julia used it for entirely personal purposes until late 2015. We see these differences immediately in this plot exploring word frequencies, and they will continue to be obvious in the rest of this chapter. Words near the red line in this plot are used with about equal frequencies by David and Julia, while words far away from the line are used much more by one person compared to the other. Words, hashtags, and usernames that appear in this plot are ones that we have both used at least once in tweets or retweets.</p>
</div>
<div id="comparing-word-usage" class="section level2">
<h2><span class="header-section-number">8.3</span> Comparing word usage</h2>
<p>We just made a plot comparing raw word frequencies over our whole Twitter histories; now let’s find which words are more or less likely to come from each person’s account using the log odds ratio. First, let’s restrict the analysis moving forward to tweets from David and Julia sent during 2016. David was consistently active on Twitter for all of 2016 and this was about when Julia transitioned into data science as a career.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tidy_tweets &lt;-<span class="st"> </span>tidy_tweets %&gt;%
<span class="st">  </span><span class="kw">filter</span>(timestamp &gt;=<span class="st"> </span><span class="kw">as.Date</span>(<span class="st">&quot;2016-01-01&quot;</span>))</code></pre></div>
<p>Next, let’s use <code>str_detect</code> to remove Twitter usernames from the <code>word</code> column, because otherwise, the results here are dominated only by people who Julia or David know and the other does not. After removing these, we count how many times each person uses each word and keep only the words used more than 10 times. After a <code>spread</code> operation, we can calculate the log odds ratio for each word, using</p>
<p><span class="math display">\[\text{log odds ratio} = \ln{\left(\frac{\left[\frac{n+1}{\text{total}+1}\right]_\text{David}}{\left[\frac{n+1}{\text{total}+1}\right]_\text{Julia}}\right)}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of times the word in question is used by each person and the total indicates the total words for each person.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_ratios &lt;-<span class="st"> </span>tidy_tweets %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">str_detect</span>(word, <span class="st">&quot;^@&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">count</span>(word, person) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">sum</span>(n) &gt;=<span class="st"> </span><span class="dv">10</span>) %&gt;%
<span class="st">  </span><span class="kw">spread</span>(person, n, <span class="dt">fill =</span> <span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate_each</span>(<span class="kw">funs</span>((. +<span class="st"> </span><span class="dv">1</span>) /<span class="st"> </span><span class="kw">sum</span>(. +<span class="st"> </span><span class="dv">1</span>)), -word) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">logratio =</span> <span class="kw">log</span>(David /<span class="st"> </span>Julia)) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(logratio))</code></pre></div>
<p>What are some words that are about equally likely to come from David or Julia’s account?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_ratios %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">abs</span>(logratio))</code></pre></div>
<pre><code>## # A tibble: 275 × 4
##           word       David       Julia     logratio
##          &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;
## 1         nice 0.005979499 0.005938634  0.006857602
## 2     hamilton 0.001993166 0.001979545  0.006857602
## 3        table 0.001993166 0.001979545  0.006857602
## 4     tomorrow 0.001993166 0.001979545  0.006857602
## 5         word 0.002562642 0.002639393 -0.029510042
## 6          yep 0.002562642 0.002639393 -0.029510042
## 7         true 0.003416856 0.003299241  0.035028479
## 8        words 0.003416856 0.003299241  0.035028479
## 9  programming 0.002847380 0.002969317 -0.041932562
## 10      python 0.002847380 0.002969317 -0.041932562
## # ... with 265 more rows</code></pre>
<p>We are about equally likely to tweet about programming, words, and the Broadway musical <em>Hamilton</em>.</p>
<p>Which words are most likely to be from Julia’s account or from David’s account? Let’s just take the top 15 most distinctive words for each account and plot them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_ratios %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(logratio &lt;<span class="st"> </span><span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">15</span>, <span class="kw">abs</span>(logratio)) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, logratio)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, logratio, <span class="dt">fill =</span> logratio &lt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;log odds ratio (David/Julia)&quot;</span>) +
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;David&quot;</span>, <span class="st">&quot;Julia&quot;</span>))</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/plotratios-1.png" width="672" /></p>
<p>So David has tweeted about matrices, models, and his R packages like broom and gganimate while Julia tweeted about Salt Lake City, the U.S. Census, and maps.</p>
</div>
<div id="changes-in-word-use" class="section level2">
<h2><span class="header-section-number">8.4</span> Changes in word use</h2>
<p>REDO THIS SECTION WITH UPDATED ARCHIVES</p>
<p>The section above looked at overall word use, but now let’s ask a different question. Which words’ frequencies have changed the fastest in our Twitter feeds? Or to state this another way, which words have we tweeted about at a higher or lower rate as time has passed? To do this, we will define a new time variable in the dataframe that defines which unit of time each tweet was posted in. We can use <code>floor_date()</code> from lubridate to do this, with a unit of our choosing; using 15 days seems to work well for this year of tweets from both of us.</p>
<p>After we have the time bins defined, we count how many times each of us used each word in each time bin. After that, we add columns to the dataframe for the total number of words used in each time bin by each person and the total number of times each word was used by each person. We can then <code>filter</code> to only keep words used at least some minimum number of times (30, in this case).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">words_by_time &lt;-<span class="st"> </span>tidy_tweets %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">str_detect</span>(word, <span class="st">&quot;^@&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">time_floor =</span> <span class="kw">floor_date</span>(timestamp, <span class="dt">unit =</span> <span class="st">&quot;15 days&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">count</span>(time_floor, person, word) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(person, time_floor) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">time_total =</span> <span class="kw">sum</span>(n)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(word) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word_total =</span> <span class="kw">sum</span>(n)) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">count =</span> n) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(word_total &gt;<span class="st"> </span><span class="dv">30</span>)

words_by_time</code></pre></div>
<pre><code>## # A tibble: 719 × 6
##    time_floor person    word count time_total word_total
##        &lt;dttm&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;      &lt;int&gt;      &lt;int&gt;
## 1  2016-01-01  David #rstats     4        243        385
## 2  2016-01-01  David   broom     1        243         37
## 3  2016-01-01  David    cran     1        243         35
## 4  2016-01-01  David    data     2        243        251
## 5  2016-01-01  David     day     2        243         53
## 6  2016-01-01  David    feel     1        243         36
## 7  2016-01-01  David ggplot2     1        243         55
## 8  2016-01-01  David     job     2        243         33
## 9  2016-01-01  David    love     1        243         52
## 10 2016-01-01  David  people     1        243         80
## # ... with 709 more rows</code></pre>
<p>Each row in this dataframe corresponds to one person using one word in a given time bin. The <code>count</code> column tells us how many times that person used that word in that time bin, the <code>time_total</code> column tells us how many words that person used during that time bin, and the <code>word_total</code> column tells us how many times that person used that word over the whole year. This is the data set we can use for modeling.</p>
<p>We can use <code>nest</code> from tidyr to make a data frame with a list column that contains little miniature data frames for each word. Let’s do that now and take a look at the resulting structure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nested_models &lt;-<span class="st"> </span>words_by_time %&gt;%
<span class="st">  </span><span class="kw">nest</span>(-word, -person) 

nested_models</code></pre></div>
<pre><code>## # A tibble: 70 × 3
##    person    word              data
##     &lt;chr&gt;   &lt;chr&gt;            &lt;list&gt;
## 1   David #rstats &lt;tibble [18 × 4]&gt;
## 2   David   broom &lt;tibble [15 × 4]&gt;
## 3   David    cran &lt;tibble [11 × 4]&gt;
## 4   David    data &lt;tibble [15 × 4]&gt;
## 5   David     day  &lt;tibble [9 × 4]&gt;
## 6   David    feel  &lt;tibble [8 × 4]&gt;
## 7   David ggplot2 &lt;tibble [13 × 4]&gt;
## 8   David     job  &lt;tibble [8 × 4]&gt;
## 9   David    love &lt;tibble [11 × 4]&gt;
## 10  David  people &lt;tibble [13 × 4]&gt;
## # ... with 60 more rows</code></pre>
<p>This data frame has one row for each person-word combination; the <code>data</code> column is a list column that contains data frames, one for each combination of person and word. Let’s use <code>map</code> from the purrr library to apply our modeling procedure to each of those little data frames inside our big data frame. This is count data so let’s use <code>glm</code> with <code>family = &quot;binomial&quot;</code> for modeling. We can think about this modeling procedure answering a question like, “Was a given word mentioned in a given time bin? Yes or no? How does the count of word mentions depend on time?”</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(purrr)

nested_models &lt;-<span class="st"> </span>nested_models %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">models =</span> <span class="kw">map</span>(data, ~<span class="st"> </span><span class="kw">glm</span>(<span class="kw">cbind</span>(count, time_total) ~<span class="st"> </span>time_floor, ., 
                                  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)))

nested_models</code></pre></div>
<p>Now notice that we have a new column for the modeling results; it is another list column and contains <code>glm</code> objects. The next step is to use <code>map</code> and <code>tidy</code> from the broom package to pull out the slopes for each of these models and find the important ones. We are comparing many slopes here and some of them are not statistically significant, so let’s apply an adjustment to the p-values for multiple comparisons.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)

slopes &lt;-<span class="st"> </span>nested_models %&gt;%
<span class="st">  </span><span class="kw">unnest</span>(<span class="kw">map</span>(models, tidy)) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(term ==<span class="st"> &quot;time_floor&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">adjusted.p.value =</span> <span class="kw">p.adjust</span>(p.value))</code></pre></div>
<p>Now let’s find the most important slopes. Which words have changed in frequency at a significant level in our tweets?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">top_slopes &lt;-<span class="st"> </span>slopes %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(adjusted.p.value &lt;<span class="st"> </span><span class="fl">0.05</span>)

top_slopes</code></pre></div>
<p>To visualize our results, we can plot these words’ use for both David and Julia over this year of tweets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">words_by_time %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(top_slopes, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;word&quot;</span>, <span class="st">&quot;person&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(time_floor, count/time_total, <span class="dt">color =</span> word)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>person, <span class="dt">nrow =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">size =</span> <span class="fl">1.3</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;Word frequency&quot;</span>,
       <span class="dt">title =</span> <span class="st">&quot;Trending words in our tweets&quot;</span>)</code></pre></div>
</div>
<div id="favorites-and-retweets" class="section level2">
<h2><span class="header-section-number">8.5</span> Favorites and retweets</h2>
<p>Another important characteristic of tweets is how many times they are favorited or retweeted. Let’s explore which words are more likely to be retweeted or favorited for Julia’s and David’s tweets. When a user downloads their own Twitter archive, favorites and retweets are not included, so we constructed another dataset of the author’s tweets that includes this information. We accessed our own tweets via the Twitter API and downloaded about 3200 tweets for each person. In both cases, that is about the last 18 months worth of Twitter activity. This corresponds to a period of increasing activity and increasing numbers of followers for both of us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tweets_julia &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/juliasilge_tweets.csv&quot;</span>)
tweets_dave &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/drob_tweets.csv&quot;</span>)
tweets &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(tweets_julia %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;Julia&quot;</span>),
                    tweets_dave %&gt;%<span class="st"> </span>
<span class="st">                      </span><span class="kw">mutate</span>(<span class="dt">person =</span> <span class="st">&quot;David&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">created_at =</span> <span class="kw">ymd_hms</span>(created_at))</code></pre></div>
<p>Now that we have this second, smaller set of only recent tweets, let’s use <code>unnest_tokens</code> to transform these tweets to a tidy data set. (Let’s also remove any retweets from this data set so we only look at tweets that David and Julia have posted directly.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tidy_tweets &lt;-<span class="st"> </span>tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">str_detect</span>(text, <span class="st">&quot;^RT&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_replace_all</span>(text, <span class="st">&quot;https://t.co/[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|http://[A-Za-z</span><span class="ch">\\</span><span class="st">d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT|https&quot;</span>, <span class="st">&quot;&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text, <span class="dt">token =</span> <span class="st">&quot;regex&quot;</span>, <span class="dt">pattern =</span> reg) %&gt;%
<span class="st">  </span><span class="kw">anti_join</span>(stop_words)

tidy_tweets</code></pre></div>
<pre><code>## # A tibble: 30,725 × 7
##              id          created_at             source retweets favorites person        word
##           &lt;dbl&gt;              &lt;dttm&gt;              &lt;chr&gt;    &lt;int&gt;     &lt;int&gt;  &lt;chr&gt;       &lt;chr&gt;
## 1  8.043967e+17 2016-12-01 18:48:07 Twitter Web Client        4         6  David         j&#39;s
## 2  8.043611e+17 2016-12-01 16:26:39 Twitter Web Client        8        12  David   bangalore
## 3  8.043611e+17 2016-12-01 16:26:39 Twitter Web Client        8        12  David      london
## 4  8.043435e+17 2016-12-01 15:16:48 Twitter for iPhone        0         1  David @rodneyfort
## 5  8.043120e+17 2016-12-01 13:11:37 Twitter for iPhone        0         1  Julia         sho
## 6  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David       arbor
## 7  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David       arbor
## 8  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David         ann
## 9  8.040632e+17 2016-11-30 20:43:03 Twitter Web Client        0         2  David         ann
## 10 8.040582e+17 2016-11-30 20:23:14 Twitter Web Client       30        41  David          sf
## # ... with 30,715 more rows</code></pre>
<p>To start with, let’s look at retweets. Let’s find the total number of retweets for each person.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person, id) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">rts =</span> <span class="kw">sum</span>(retweets)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">total_rts =</span> <span class="kw">sum</span>(rts))

totals</code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   person total_rts
##    &lt;chr&gt;     &lt;int&gt;
## 1  David    111863
## 2  Julia     12906</code></pre>
<p>Now let’s find the median number of retweets for each word and person; we probably want to count each tweet/word combination only once, so we will use <code>group_by</code> and <code>summarise</code> twice, one right after the other. Next, we can join this to the data frame of retweet totals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_by_rts &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(id, word, person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">rts =</span> <span class="kw">first</span>(retweets)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person, word) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">retweets =</span> <span class="kw">median</span>(rts)) %&gt;%
<span class="st">  </span><span class="kw">left_join</span>(totals) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(retweets !=<span class="st"> </span><span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>()

word_by_rts %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(retweets))</code></pre></div>
<pre><code>## # A tibble: 2,422 × 4
##    person         word retweets total_rts
##     &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt;
## 1   David      angrier   1757.0    111863
## 2   David     confirms    878.5    111863
## 3   David       writes    878.5    111863
## 4   David        voted    611.0    111863
## 5   David       county    534.0    111863
## 6   David      teacher    390.5    111863
## 7   David      command    344.0    111863
## 8   David       revert    344.0    111863
## 9   David  dimensional    320.5    111863
## 10  David eigenvectors    320.5    111863
## # ... with 2,412 more rows</code></pre>
<p>At the top of this sorted data frame, we see David’s tweet about <a href="http://varianceexplained.org/r/trump-tweets/">his blog post on Donald Trump’s own tweets</a> that went viral. A search tells us that this is the only time David has ever used the word “angrier” in his tweets, so that word has an extremely high median retweet rate.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
New post: Analysis of Trump tweets confirms he writes only the angrier Android half <a href="https://t.co/HRr4yj30hx">https://t.co/HRr4yj30hx</a> <a href="https://twitter.com/hashtag/rstats?src=hash">#rstats</a> <a href="https://t.co/cmwdNeYSE7">pic.twitter.com/cmwdNeYSE7</a>
</p>
— David Robinson (<span class="citation">(<span class="citeproc-not-found" data-reference-id="drob"><strong>???</strong></span>)</span>) <a href="https://twitter.com/drob/status/763048283531055104">August 9, 2016</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Now we can plot the words that have contributed the most to each of our retweets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_by_rts %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ratio =</span> retweets /<span class="st"> </span>total_rts) %&gt;%<span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>, ratio) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(ratio) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">factor</span>(word, <span class="kw">unique</span>(word))) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, ratio, <span class="dt">fill =</span> person)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>person, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, 
       <span class="dt">y =</span> <span class="st">&quot;proportion of total RTs due to each word&quot;</span>,
       <span class="dt">title =</span> <span class="st">&quot;Words with highest median retweets&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/unnamed-chunk-6-1.png" width="960" /></p>
<p>We see more words from David’s tweet about his Trump blog post, and words from Julia making announcements about blog posts and new package releases. These are some pretty good tweets; we can see why people retweeted them.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
NEW POST: Mapping ghost sightings in Kentucky using Leaflet 👻👻👻 <a href="https://twitter.com/hashtag/rstats?src=hash">#rstats</a> <a href="https://t.co/rRFTSsaKWQ">https://t.co/rRFTSsaKWQ</a> <a href="https://t.co/codPf3gy6O">pic.twitter.com/codPf3gy6O</a>
</p>
— Julia Silge (<span class="citation">(<span class="citeproc-not-found" data-reference-id="juliasilge"><strong>???</strong></span>)</span>) <a href="https://twitter.com/juliasilge/status/761667180148641793">August 5, 2016</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
Me: Git makes it easy to revert your local changes<br><br>Them: Great! So what command do I use?<br><br>Me: I said it was easy not that I knew how
</p>
— David Robinson (<span class="citation">(<span class="citeproc-not-found" data-reference-id="drob"><strong>???</strong></span>)</span>) <a href="https://twitter.com/drob/status/770706647585095680">August 30, 2016</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>We can follow a similar procedure to see which words led to more favorites. Are they different than the words that lead to more retweets?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">totals &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person, id) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">favs =</span> <span class="kw">sum</span>(favorites)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">total_favs =</span> <span class="kw">sum</span>(favs))

word_by_favs &lt;-<span class="st"> </span>tidy_tweets %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(id, word, person) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">favs =</span> <span class="kw">first</span>(favorites)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(person, word) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">favorites =</span> <span class="kw">median</span>(favs)) %&gt;%
<span class="st">  </span><span class="kw">left_join</span>(totals) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(favorites !=<span class="st"> </span><span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_by_favs %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ratio =</span> favorites /<span class="st"> </span>total_favs) %&gt;%<span class="st">  </span>
<span class="st">  </span><span class="kw">group_by</span>(person) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>, ratio) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(ratio) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">factor</span>(word, <span class="kw">unique</span>(word))) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, ratio, <span class="dt">fill =</span> person)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>person, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, 
       <span class="dt">y =</span> <span class="st">&quot;proportion of total favorites due to each word&quot;</span>,
       <span class="dt">title =</span> <span class="st">&quot;Words with highest median favorites&quot;</span>)</code></pre></div>
<p><img src="08-tweet-archives_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
<p>We see some minor differences, especially near the bottom of the top 10 list, but these are largely the same words as for favorites. In general, the same words that lead to retweets lead to favorites. There are some exceptions, though.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
🎶 I am writing a Shiny app for my joooooooob 🎶🎶 I am living the dreeeeeeeeeam… 🎶🎶 <a href="https://twitter.com/hashtag/rstats?src=hash">#rstats</a>
</p>
— Julia Silge (<span class="citation">(<span class="citeproc-not-found" data-reference-id="juliasilge"><strong>???</strong></span>)</span>) <a href="https://twitter.com/juliasilge/status/732645241610600448">May 17, 2016</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="topicmodeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nasa.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dgrtwo/tidy-text-mining/edit/master/08-tweet-archives.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
