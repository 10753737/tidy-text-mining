[
["intro.html", "Tidy Text Mining Chapter 1 Introduction 1.1 What is tidy text? 1.2 Outline", " Tidy Text Mining Julia Silge and David Robinson 2016-07-07 Chapter 1 Introduction This intro will be changed a lot to serve as a general and friendly intro to the topic. 1.1 What is tidy text? As described by Hadley Wickham(Wickham 2014), tidy data has a specific structure: each variable is a column each observation is a row each type of observational unit is a table Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr(???), ggplot2(???), and broom(Robinson et al. 2015). These tools do not yet, however, have the infrastructure to work fluently with text data and natural language processing tools. In developing this package, we provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages. We define the tidy text format as being one-token-per-document-per-row, and provide functionality to tokenize by commonly used units of text including words, n-grams, and sentences. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy objects (see the broom package(Robinson et al. 2015)) from popular text mining R packages such as tm(Ingo Feinerer and Meyer 2008) and quanteda(???). This allows, for example, a workflow with easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. 1.2 Outline We start by introducing the tidy text format, and some of the ways dplyr, tidyr and tidytext allow informative analyses of this structure. Chapter 2 outlines the tidy text format and the unnest_tokens function. It also introduces the gutenbergr and janeaustenr packages, which provide useful literary text datasets that we’ll use throughout this book. Chapter 3 shows how to perform sentiment analysis on a tidy text dataset, using the sentiments dataset from tidytext and inner_join from dplyr Chapter 4 describes the method of TF-IDF (term frequency times inverse document frequency), for identifying terms that are especially specific to a particular document. (Other document stuff in this chapter perhaps?) Text won’t be tidy at all stages of an analysis. Chapter 5 introduces methods for tidying document-term matrices and Corpus objects from the tm and quanteda packages, as well as for casting tidy text datasets into those formats. Chapter 6 introduces the concept of topic modeling, and uses the tidy method for interpreting and visualizing the output of the topicmodels package. Chapter 7 (TODO) introduces tidying methods for the glove package, which offer an interface to word2vec models. (These methods are still being implemented so this chapter is far from written!) We conclude with two tidy text analyses that bring together multiple text-mining approaches we’ve learned. Chapter 8 demonstrates an application of a tidy text analysis on the Yelp restaurant review dataset. We show a few approaches to predicting a star rating from a review’s text, and see how well sentiment analysis (from Chapter 3) does at this task. Chapter 9 TODO: find at least one other in-depth exploration of text data. Optional but I think would conclude the book well. References "],
["tidytext.html", "Chapter 2 The Tidy Text Format 2.1 The unnest_tokens function 2.2 Example: the works of Jane Austen", " Chapter 2 The Tidy Text Format Intro text may go here about the one-token-per-document-per-row and about what is explored in the chapter. 2.1 The unnest_tokens function text &lt;- c(&quot;Because I could not stop for Death -&quot;, &quot;He kindly stopped for me -&quot;, &quot;The Carriage held but just Ourselves -&quot;, &quot;and Immortality&quot;) text ## [1] &quot;Because I could not stop for Death -&quot; &quot;He kindly stopped for me -&quot; ## [3] &quot;The Carriage held but just Ourselves -&quot; &quot;and Immortality&quot; This is a typical character vector that we might want to analyze. In order to turn it into a tidy text dataset, we first need to put it into a data frame: library(dplyr) text_df &lt;- data_frame(line = 1:4, text = text) text_df ## # A tibble: 4 x 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 Because I could not stop for Death - ## 2 2 He kindly stopped for me - ## 3 3 The Carriage held but just Ourselves - ## 4 4 and Immortality Notice that this data frame isn’t yet compatible with tidy tools. We can’t filter out words or count which occur most frequently, since each row is made up of multiple coimbined tokens. We need to turn this into one-token-per-document-per-row. To do this, we use tidytext’s unnest_tokens function: library(tidytext) text_df %&gt;% unnest_tokens(word, text) ## # A tibble: 20 x 2 ## line word ## &lt;int&gt; &lt;chr&gt; ## 1 1 because ## 2 1 i ## 3 1 could ## 4 1 not ## 5 1 stop ## 6 1 for ## 7 1 death ## 8 2 he ## 9 2 kindly ## 10 2 stopped ## # ... with 10 more rows We’ve now split each row so that there’s one token (word) in each row of the new data frame. Also notice: Other columns, such as the line number each word came from, are retained Punctuation has been stripped By default, unnest_tokens turns the tokens lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior). Having the text data in this format lets us manipulate, process, and visualize the text using the standard set of tidy tools; namely dplyr, tidyr, ggplot2, and broom. 2.2 Example: the works of Jane Austen Let’s use the text of Jane Austen’s 6 completed, published novels from the janeaustenr package, and transform them into a tidy format. janeaustenr provides them as a one-row-per-line format: library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 x 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 SENSE AND SENSIBILITY Sense &amp; Sensibility 1 0 ## 2 Sense &amp; Sensibility 2 0 ## 3 by Jane Austen Sense &amp; Sensibility 3 0 ## 4 Sense &amp; Sensibility 4 0 ## 5 (1811) Sense &amp; Sensibility 5 0 ## 6 Sense &amp; Sensibility 6 0 ## 7 Sense &amp; Sensibility 7 0 ## 8 Sense &amp; Sensibility 8 0 ## 9 Sense &amp; Sensibility 9 0 ## 10 CHAPTER 1 Sense &amp; Sensibility 10 1 ## # ... with 73,412 more rows To work with this as a tidy dataset, we need to restructure it as one-token-per-row format. The unnest_tokens function is a way to convert a dataframe with a text column to be one-token-per-row: library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,054 x 4 ## book linenumber chapter word ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # ... with 725,044 more rows This function uses the tokenizers package to separate each line into words. The default tokenizing is for words, but other options include characters, ngrams, sentences, lines, paragraphs, or separation around a regex pattern. Now that the data is in one-word-per-row format, we can manipulate it with tidy tools like dplyr. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join. data(&quot;stop_words&quot;) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) We can also use count to find the most common words in all the books as a whole. tidy_books %&gt;% count(word, sort = TRUE) ## # A tibble: 13,914 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 1855 ## 2 time 1337 ## 3 fanny 862 ## 4 dear 822 ## 5 lady 817 ## 6 sir 806 ## 7 day 797 ## 8 emma 787 ## 9 sister 727 ## 10 house 699 ## # ... with 13,904 more rows For example, this allows us to visualize the popular words using ggplot2: library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) ## # A tibble: 13,914 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 1855 ## 2 time 1337 ## 3 fanny 862 ## 4 dear 822 ## 5 lady 817 ## 6 sir 806 ## 7 day 797 ## 8 emma 787 ## 9 sister 727 ## 10 house 699 ## # ... with 13,904 more rows 2.2.1 The gutenbergr package TODO: Now that we’ve introduced the janeaustenr package, also include a brief intro to the gutenberg package. "],
["sentiment.html", "Chapter 3 Sentiment Analysis with Tidy Data 3.1 The sentiments dataset 3.2 Sentiment analysis with inner join", " Chapter 3 Sentiment Analysis with Tidy Data 3.1 The sentiments dataset The sentiments ## # A tibble: 23,165 x 4 ## word sentiment lexicon score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 abacus trust nrc NA ## 2 abandon fear nrc NA ## 3 abandon negative nrc NA ## 4 abandon sadness nrc NA ## 5 abandoned anger nrc NA ## 6 abandoned fear nrc NA ## 7 abandoned negative nrc NA ## 8 abandoned sadness nrc NA ## 9 abandonment anger nrc NA ## 10 abandonment fear nrc NA ## # ... with 23,155 more rows 3.2 Sentiment analysis with inner join Sentiment analysis can be done as an inner join. Three sentiment lexicons are in the tidytext package in the sentiment dataset. Let’s look at the words with a joy score from the NRC lexicon. What are the most common joy words in Emma? nrcjoy &lt;- sentiments %&gt;% filter(lexicon == &quot;nrc&quot;, sentiment == &quot;joy&quot;) tidy_books %&gt;% filter(book == &quot;Emma&quot;) %&gt;% semi_join(nrcjoy) %&gt;% count(word, sort = TRUE) ## # A tibble: 298 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 friend 166 ## 2 hope 143 ## 3 happy 125 ## 4 love 117 ## 5 deal 92 ## 6 found 92 ## 7 happiness 76 ## 8 pretty 68 ## 9 true 66 ## 10 comfort 65 ## # ... with 288 more rows Or instead we could examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel. library(tidyr) bing &lt;- sentiments %&gt;% filter(lexicon == &quot;bing&quot;) %&gt;% select(-score) janeaustensentiment &lt;- tidy_books %&gt;% inner_join(bing) %&gt;% count(book, index = linenumber %/% 80, sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative) Now we can plot these sentiment scores across the plot trajectory of each novel. library(ggplot2) ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) 3.2.1 Most common positive and negative words One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment. bing_word_counts &lt;- tidy_books %&gt;% inner_join(bing) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts ## # A tibble: 2,555 x 3 ## word sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 miss negative 1855 ## 2 happy positive 534 ## 3 love positive 495 ## 4 pleasure positive 462 ## 5 poor negative 424 ## 6 happiness positive 369 ## 7 comfort positive 292 ## 8 doubt negative 281 ## 9 affection positive 272 ## 10 perfectly positive 271 ## # ... with 2,545 more rows This can be shown visually, and we can pipe straight into ggplot2 because of the way we are consistently using tools built for handling tidy data frames. bing_word_counts %&gt;% filter(n &gt; 150) %&gt;% mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_bar(stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Contribution to sentiment&quot;) This lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using bind_rows. 3.2.2 Wordclouds We’ve seen that this tidy text mining approach works well with ggplot2, but having our data in a tidy format is useful for other plots as well. For example, consider the wordcloud package. Let’s look at the most common words in Jane Austen’s works as a whole again. library(wordcloud) tidy_books %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100)) In other functions, such as comparison.cloud, you may need to turn it into a matrix with reshape2’s acast. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud, this can all be done with joins, piping, and dplyr because our data is in tidy format. library(reshape2) tidy_books %&gt;% inner_join(bing) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;#F8766D&quot;, &quot;#00BFC4&quot;), max.words = 100) 3.2.3 Looking at units beyond just words Lots of useful work can be done by tokenizing at the word level, but sometimes it is useful or necessary to look at different units of text. For example, some sentiment analysis algorithms look beyond only unigrams (i.e. single words) to try to understand the sentiment of a sentence as a whole. These algorithms try to understand that I am not having a good day. is a sad sentence, not a happy one, because of negation. The Stanford CoreNLP tools and the sentimentr R package (currently available on Github but not CRAN) are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences. PandP_sentences &lt;- data_frame(text = prideprejudice) %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) Let’s look at just one. PandP_sentences$sentence[2] ## [1] &quot;however little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.&quot; The sentence tokenizing does seem to have a bit of trouble with UTF-8 encoded text, especially with sections of dialogue; it does much better with punctuation in ASCII. Another option in unnest_tokens is to split into tokens using a regex pattern. We could use this, for example, to split the text of Jane Austen’s novels into a data frame by chapter. austen_chapters &lt;- austen_books() %&gt;% group_by(book) %&gt;% unnest_tokens(chapter, text, token = &quot;regex&quot;, pattern = &quot;Chapter|CHAPTER [\\\\dIVXLC]&quot;) %&gt;% ungroup() austen_chapters %&gt;% group_by(book) %&gt;% summarise(chapters = n()) ## # A tibble: 6 x 2 ## book chapters ## &lt;fctr&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility 51 ## 2 Pride &amp; Prejudice 62 ## 3 Mansfield Park 49 ## 4 Emma 56 ## 5 Northanger Abbey 32 ## 6 Persuasion 25 We have recovered the correct number of chapters in each novel (plus an “extra” row for each novel title). In this data frame, each row corresponds to one chapter. Near the beginning of this vignette, we used a similar regex to find where all the chapters were in Austen’s novels for a tidy data frame organized by one-word-per-row. We can use tidy text analysis to ask questions such as what are the most negative chapters in each of Jane Austen’s novels? First, let’s get the list of negative words from the Bing lexicon. Second, let’s make a dataframe of how many words are in each chapter so we can normalize for the length of chapters. Then, let’s find the number of negative words in each chapter and divide by the total words in each chapter. Which chapter has the highest proportion of negative words? bingnegative &lt;- sentiments %&gt;% filter(lexicon == &quot;bing&quot;, sentiment == &quot;negative&quot;) wordcounts &lt;- tidy_books %&gt;% group_by(book, chapter) %&gt;% summarize(words = n()) tidy_books %&gt;% semi_join(bingnegative) %&gt;% group_by(book, chapter) %&gt;% summarize(negativewords = n()) %&gt;% left_join(wordcounts, by = c(&quot;book&quot;, &quot;chapter&quot;)) %&gt;% mutate(ratio = negativewords/words) %&gt;% filter(chapter != 0) %&gt;% top_n(1) ## Source: local data frame [6 x 5] ## Groups: book [6] ## ## book chapter negativewords words ratio ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Sense &amp; Sensibility 29 172 1135 0.1515419 ## 2 Pride &amp; Prejudice 34 108 646 0.1671827 ## 3 Mansfield Park 45 132 884 0.1493213 ## 4 Emma 15 147 1012 0.1452569 ## 5 Northanger Abbey 27 55 337 0.1632047 ## 6 Persuasion 21 215 1948 0.1103696 These are the chapters with the most negative words in each book, normalized for number of words in the chapter. What is happening in these chapters? In Chapter 29 of Sense and Sensibility Marianne finds out what an awful person Willoughby is by letter, and in Chapter 34 of Pride and Prejudice Mr. Darcy proposes for the first time (so badly!). Chapter 45 of Mansfield Park is almost the end, when Tom is sick with consumption and Mary is revealed as mercenary and uncaring, Chapter 15 of Emma is when horrifying Mr. Elton proposes, and Chapter 27 of Northanger Abbey is a short chapter where Catherine gets a terrible letter from her inconstant friend Isabella. Chapter 21 of Persuasion is when Anne’s friend tells her all about Mr. Elliott’s immoral past. "],
["tfidf.html", "Chapter 4 TF-IDF: Analyzing word and document frequency 4.1 Term frequency and inverse document frequency 4.2 The bind_tf_idf function", " Chapter 4 TF-IDF: Analyzing word and document frequency A central question in text mining and natural language processing is how to quantify what a document is about. Can we do this by looking at the words that make up the document? One measure of how important a word may be is its term frequency (tf), how frequently a word occurs in a document. There are words in a document, however, that occur many times but may not be important; in English, these are probably words like “the”, “is”, “of”, and so forth. We might take the approach of adding words like these to a list of stop words and removing them before analysis, but it is possible that some of these words might be more important in some documents than others. A list of stop words is not a sophisticated approach to adjusting term frequency for commonly used words. 4.1 Term frequency and inverse document frequency Another approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf, the frequency of a term adjusted for how rarely it is used. It is intended to measure how important a word is to a document in a collection (or corpus) of documents. It is a rule-of-thumb or heuristic quantity; while it has proved useful in text mining, search engines, etc., its theoretical foundations are considered less than firm by information theory experts. The inverse document frequency for any given term is defined as \\[idf(\\text{term}) = \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)}\\] We can use tidy data principles, as described in the main vignette, to approach tf-idf analysis and use consistent, effective tools to quantify how important various terms are in a document that is part of a collection. Let’s look at the published novels of Jane Austen and examine first term frequency, then tf-idf. We can start just by using dplyr verbs such as group_by and join. What are the most commonly used words in Jane Austen’s novels? (Let’s also calculate the total words in each novel here, for later use.) library(dplyr) library(janeaustenr) library(tidytext) book_words &lt;- austen_books() %&gt;% unnest_tokens(word, text) %&gt;% count(book, word, sort = TRUE) %&gt;% ungroup() total_words &lt;- book_words %&gt;% group_by(book) %&gt;% summarize(total = sum(n)) book_words &lt;- left_join(book_words, total_words) book_words ## # A tibble: 40,379 x 4 ## book word n total ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Mansfield Park the 6206 160460 ## 2 Mansfield Park to 5475 160460 ## 3 Mansfield Park and 5438 160460 ## 4 Emma to 5239 160996 ## 5 Emma the 5201 160996 ## 6 Emma and 4896 160996 ## 7 Mansfield Park of 4778 160460 ## 8 Pride &amp; Prejudice the 4331 122204 ## 9 Emma of 4291 160996 ## 10 Pride &amp; Prejudice to 4162 122204 ## # ... with 40,369 more rows The usual suspects are here, “the”, “and”, “to”, and so forth. Let’s look at the distribution of n/total for each novel, the number of times a word appears in a novel divided by the total number of terms (words) in that novel. This is exactly what term frequency is. library(ggplot2) ggplot(book_words, aes(n/total, fill = book)) + geom_histogram(show.legend = FALSE) + xlim(NA, 0.0009) + facet_wrap(~book, ncol = 2, scales = &quot;free_y&quot;) There are very long tails to the right for these novels (those extremely common words!) that we have not shown in these plots. These plots exhibit similar distributions for all the novels, with many words that occur rarely and fewer words that occur frequently. 4.2 The bind_tf_idf function The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the group of Jane Austen’s novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. Let’s do that now. book_words &lt;- book_words %&gt;% bind_tf_idf(word, book, n) book_words ## # A tibble: 40,379 x 7 ## book word n total tf idf tf_idf ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mansfield Park the 6206 160460 0.03867631 0 0 ## 2 Mansfield Park to 5475 160460 0.03412065 0 0 ## 3 Mansfield Park and 5438 160460 0.03389007 0 0 ## 4 Emma to 5239 160996 0.03254118 0 0 ## 5 Emma the 5201 160996 0.03230515 0 0 ## 6 Emma and 4896 160996 0.03041069 0 0 ## 7 Mansfield Park of 4778 160460 0.02977689 0 0 ## 8 Pride &amp; Prejudice the 4331 122204 0.03544074 0 0 ## 9 Emma of 4291 160996 0.02665284 0 0 ## 10 Pride &amp; Prejudice to 4162 122204 0.03405780 0 0 ## # ... with 40,369 more rows Notice that idf and thus tf-idf are zero for these extremely common words. These are all words that appear in all six of Jane Austen’s novels, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection. Let’s look at terms with high tf-idf in Jane Austen’s works. book_words %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 40,379 x 6 ## book word n tf idf tf_idf ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sense &amp; Sensibility elinor 623 0.005193528 1.791759 0.009305552 ## 2 Sense &amp; Sensibility marianne 492 0.004101470 1.791759 0.007348847 ## 3 Mansfield Park crawford 493 0.003072417 1.791759 0.005505032 ## 4 Pride &amp; Prejudice darcy 373 0.003052273 1.791759 0.005468939 ## 5 Persuasion elliot 254 0.003036207 1.791759 0.005440153 ## 6 Emma emma 786 0.004882109 1.098612 0.005363545 ## 7 Northanger Abbey tilney 196 0.002519928 1.791759 0.004515105 ## 8 Emma weston 389 0.002416209 1.791759 0.004329266 ## 9 Pride &amp; Prejudice bennet 294 0.002405813 1.791759 0.004310639 ## 10 Persuasion wentworth 191 0.002283132 1.791759 0.004090824 ## # ... with 40,369 more rows Here we see all proper nouns, names that are in fact important in these novels. None of them occur in all of novels, and they are important, characteristic words for each text. Some of the values for idf are the same for different terms because there are 6 documents in this corpus and we are seeing the numerical value for \\(\\ln(6/1)\\), \\(\\ln(6/2)\\), etc. Let’s look specifically at Pride and Prejudice. book_words %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 6,538 x 6 ## book word n tf idf tf_idf ## &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Pride &amp; Prejudice darcy 373 0.0030522732 1.7917595 0.005468939 ## 2 Pride &amp; Prejudice bennet 294 0.0024058132 1.7917595 0.004310639 ## 3 Pride &amp; Prejudice bingley 257 0.0021030408 1.7917595 0.003768143 ## 4 Pride &amp; Prejudice elizabeth 597 0.0048852738 0.6931472 0.003386214 ## 5 Pride &amp; Prejudice wickham 162 0.0013256522 1.7917595 0.002375250 ## 6 Pride &amp; Prejudice collins 156 0.0012765540 1.7917595 0.002287278 ## 7 Pride &amp; Prejudice lydia 133 0.0010883441 1.7917595 0.001950051 ## 8 Pride &amp; Prejudice lizzy 95 0.0007773886 1.7917595 0.001392893 ## 9 Pride &amp; Prejudice longbourn 88 0.0007201074 1.7917595 0.001290259 ## 10 Pride &amp; Prejudice gardiner 84 0.0006873752 1.7917595 0.001231611 ## # ... with 6,528 more rows These words are, as measured by tf-idf, the most important to Pride and Prejudice and most readers would likely agree. "],
["tidying-and-casting-document-term-matrices.html", "Chapter 5 Tidying and casting document-term matrices 5.1 Tidying a document-term matrix 5.2 Casting tidy text data into a DocumentTermMatrix 5.3 Tidying corpus objects with metadata", " Chapter 5 Tidying and casting document-term matrices Intro text here. 5.1 Tidying a document-term matrix Many existing text mining datasets are in the form of a DocumentTermMatrix class (from the tm package). For example, consider the corpus of 2246 Associated Press articles from the topicmodels package: library(tm) data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) AssociatedPress ## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) If we want to analyze this with tidy tools, we need to turn it into a one-token-per-document-per-row data frame first. The tidy function does this. (For more on the tidy verb, see the broom package). library(dplyr) library(tidytext) ap_td &lt;- tidy(AssociatedPress) Just as shown in this vignette, having the text in this format is convenient for analysis with the tidytext package. For example, you can perform sentiment analysis on these newspaper articles. bing &lt;- sentiments %&gt;% filter(lexicon == &quot;bing&quot;) %&gt;% select(word, sentiment) ap_sentiments &lt;- ap_td %&gt;% inner_join(bing, by = c(term = &quot;word&quot;)) ap_sentiments ## # A tibble: 30,094 x 4 ## document term count sentiment ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 assault 1 negative ## 2 1 complex 1 negative ## 3 1 death 1 negative ## 4 1 died 1 negative ## 5 1 good 2 positive ## 6 1 illness 1 negative ## 7 1 killed 2 negative ## 8 1 like 2 positive ## 9 1 liked 1 positive ## 10 1 miracle 1 positive ## # ... with 30,084 more rows We can find the most negative documents: library(tidyr) ap_sentiments %&gt;% count(document, sentiment, wt = count) %&gt;% ungroup() %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative) %&gt;% arrange(sentiment) ## # A tibble: 2,190 x 4 ## document negative positive sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1251 54 6 -48 ## 2 1380 53 5 -48 ## 3 531 51 9 -42 ## 4 43 45 11 -34 ## 5 1263 44 10 -34 ## 6 2178 40 6 -34 ## 7 334 45 12 -33 ## 8 1664 38 5 -33 ## 9 2147 47 14 -33 ## 10 516 38 6 -32 ## # ... with 2,180 more rows Or visualize which words contributed to positive and negative sentiment: library(ggplot2) ap_sentiments %&gt;% count(sentiment, term, wt = count) %&gt;% ungroup() %&gt;% filter(n &gt;= 150) %&gt;% mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;% mutate(term = reorder(term, n)) %&gt;% ggplot(aes(term, n, fill = sentiment)) + geom_bar(stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Contribution to sentiment&quot;) Note that a tidier is also available for the dfm class from the quanteda package: data(&quot;inaugCorpus&quot;, package = &quot;quanteda&quot;) d &lt;- quanteda::dfm(inaugCorpus) ## Creating a dfm from a corpus ... ## ... lowercasing ## ... tokenizing ## ... indexing documents: 57 documents ## ... indexing features: 9,214 feature types ## ... created a 57 x 9215 sparse dfm ## ... complete. ## Elapsed time: 0.17 seconds. d ## Document-feature matrix of: 57 documents, 9,215 features. tidy(d) ## # A tibble: 43,719 x 3 ## document term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1789-Washington fellow-citizens 1 ## 2 1797-Adams fellow-citizens 3 ## 3 1801-Jefferson fellow-citizens 2 ## 4 1809-Madison fellow-citizens 1 ## 5 1813-Madison fellow-citizens 1 ## 6 1817-Monroe fellow-citizens 5 ## 7 1821-Monroe fellow-citizens 1 ## 8 1841-Harrison fellow-citizens 11 ## 9 1845-Polk fellow-citizens 1 ## 10 1849-Taylor fellow-citizens 1 ## # ... with 43,709 more rows 5.2 Casting tidy text data into a DocumentTermMatrix Some existing text mining tools or algorithms work only on sparse document-term matrices. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices. ap_td ## # A tibble: 302,031 x 3 ## document term count ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 adding 1 ## 2 1 adult 2 ## 3 1 ago 1 ## 4 1 alcohol 1 ## 5 1 allegedly 1 ## 6 1 allen 1 ## 7 1 apparently 2 ## 8 1 appeared 1 ## 9 1 arrested 1 ## 10 1 assault 1 ## # ... with 302,021 more rows # cast into a Document-Term Matrix ap_td %&gt;% cast_dtm(document, term, count) ## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) # cast into a Term-Document Matrix ap_td %&gt;% cast_tdm(term, document, count) ## &lt;&lt;TermDocumentMatrix (terms: 10473, documents: 2246)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) # cast into quanteda&#39;s dfm ap_td %&gt;% cast_dfm(term, document, count) ## Document-feature matrix of: 10,473 documents, 2,246 features. # cast into a Matrix object m &lt;- ap_td %&gt;% cast_sparse(document, term, count) class(m) ## [1] &quot;dgCMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;Matrix&quot; dim(m) ## [1] 2246 10473 This allows for easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications. 5.3 Tidying corpus objects with metadata You can also tidy Corpus objects from the tm package. For example, consider a Corpus containing 20 documents: reut21578 &lt;- system.file(&quot;texts&quot;, &quot;crude&quot;, package = &quot;tm&quot;) reuters &lt;- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain)) reuters ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 20 The tidy verb creates a table with one row per document: reuters_td &lt;- tidy(reuters) reuters_td ## # A tibble: 20 x 17 ## author datetimestamp description ## &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 1987-02-26 10:00:56 ## 2 BY TED D&#39;AFFLISIO, Reuters 1987-02-26 10:34:11 ## 3 &lt;NA&gt; 1987-02-26 11:18:00 ## 4 &lt;NA&gt; 1987-02-26 11:21:01 ## 5 &lt;NA&gt; 1987-02-26 12:00:57 ## 6 &lt;NA&gt; 1987-02-28 20:25:46 ## 7 By Jeremy Clift, Reuters 1987-02-28 20:39:14 ## 8 &lt;NA&gt; 1987-02-28 22:27:27 ## 9 &lt;NA&gt; 1987-03-01 01:22:30 ## 10 &lt;NA&gt; 1987-03-01 11:31:44 ## 11 &lt;NA&gt; 1987-03-01 18:05:49 ## 12 &lt;NA&gt; 1987-03-02 00:39:23 ## 13 &lt;NA&gt; 1987-03-02 00:43:22 ## 14 &lt;NA&gt; 1987-03-02 00:43:41 ## 15 &lt;NA&gt; 1987-03-02 01:25:42 ## 16 &lt;NA&gt; 1987-03-02 04:20:05 ## 17 &lt;NA&gt; 1987-03-02 04:28:26 ## 18 &lt;NA&gt; 1987-03-02 05:13:46 ## 19 By BERNICE NAPACH, Reuters 1987-03-02 07:38:34 ## 20 &lt;NA&gt; 1987-03-02 07:49:06 ## heading id language origin ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES 127 en Reuters-21578 XML ## 2 OPEC MAY HAVE TO MEET TO FIRM PRICES - ANALYSTS 144 en Reuters-21578 XML ## 3 TEXACO CANADA &lt;TXC&gt; LOWERS CRUDE POSTINGS 191 en Reuters-21578 XML ## 4 MARATHON PETROLEUM REDUCES CRUDE POSTINGS 194 en Reuters-21578 XML ## 5 HOUSTON OIL &lt;HO&gt; RESERVES STUDY COMPLETED 211 en Reuters-21578 XML ## 6 KUWAIT SAYS NO PLANS FOR EMERGENCY OPEC TALKS 236 en Reuters-21578 XML ## 7 INDONESIA SEEN AT CROSSROADS OVER ECONOMIC CHANGE 237 en Reuters-21578 XML ## 8 SAUDI RIYAL DEPOSIT RATES REMAIN FIRM 242 en Reuters-21578 XML ## 9 QATAR UNVEILS BUDGET FOR FISCAL 1987/88 246 en Reuters-21578 XML ## 10 SAUDI ARABIA REITERATES COMMITMENT TO OPEC PACT 248 en Reuters-21578 XML ## 11 SAUDI FEBRUARY CRUDE OUTPUT PUT AT 3.5 MLN BPD 273 en Reuters-21578 XML ## 12 GULF ARAB DEPUTY OIL MINISTERS TO MEET IN BAHRAIN 349 en Reuters-21578 XML ## 13 SAUDI ARABIA REITERATES COMMITMENT TO OPEC ACCORD 352 en Reuters-21578 XML ## 14 KUWAIT MINISTER SAYS NO EMERGENCY OPEC TALKS SET 353 en Reuters-21578 XML ## 15 PHILADELPHIA PORT CLOSED BY TANKER CRASH 368 en Reuters-21578 XML ## 16 STUDY GROUP URGES INCREASED U.S. OIL RESERVES 489 en Reuters-21578 XML ## 17 STUDY GROUP URGES INCREASED U.S. OIL RESERVES 502 en Reuters-21578 XML ## 18 UNOCAL &lt;UCL&gt; UNIT CUTS CRUDE OIL POSTED PRICES 543 en Reuters-21578 XML ## 19 NYMEX WILL EXPAND OFF-HOUR TRADING APRIL ONE 704 en Reuters-21578 XML ## 20 ARGENTINE OIL PRODUCTION DOWN IN JANUARY 1987 708 en Reuters-21578 XML ## # ... with 10 more variables: topics &lt;chr&gt;, lewissplit &lt;chr&gt;, cgisplit &lt;chr&gt;, oldid &lt;chr&gt;, ## # topics_cat &lt;list&gt;, places &lt;list&gt;, people &lt;chr&gt;, orgs &lt;chr&gt;, exchanges &lt;chr&gt;, text &lt;chr&gt; Similarly, you can tidy a corpus object from the quanteda package: library(quanteda) data(&quot;inaugCorpus&quot;) inaugCorpus ## Corpus consisting of 57 documents and 3 docvars. inaug_td &lt;- tidy(inaugCorpus) inaug_td ## # A tibble: 57 x 4 ## text ## * &lt;chr&gt; ## 1 Fellow-Citizens of the Senate and of the House of Representatives:\\n\\nAmong the vicissitudes incident to life no event could have filled me with greater ## 2 Fellow citizens, I am again called upon by the voice of my country to execute the functions of its Chief Magistrate. When the occasion proper for it s ## 3 When it was first perceived, in early times, that no middle course for America remained between unlimited submission to a foreign legislature and a to ## 4 Friends and Fellow Citizens:\\n\\nCalled upon to undertake the duties of the first executive office of our country, I avail myself of the presence of that ## 5 Proceeding, fellow citizens, to that qualification which the Constitution requires before my entrance on the charge again conferred on me, it is my du ## 6 Unwilling to depart from examples of the most revered authority, I avail myself of the occasion now presented to express the profound impression made ## 7 About to add the solemnity of an oath to the obligations imposed by a second call to the station in which my country heretofore placed me, I find in t ## 8 I should be destitute of feeling if I was not deeply affected by the strong proof which my fellow-citizens have given me of their confidence in callin ## 9 Fellow citizens, I shall not attempt to describe the grateful emotions which the new and very distinguished proof of the confidence of my fellow citiz ## 10 In compliance with an usage coeval with the existence of our Federal Constitution, and sanctioned by the example of my predecessors in the career upon ## # ... with 47 more rows, and 3 more variables: Year &lt;int&gt;, President &lt;chr&gt;, FirstName &lt;chr&gt; This lets us work with tidy tools like unnest_tokens to analyze the text alongside the metadata. inaug_words &lt;- inaug_td %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) inaug_words ## # A tibble: 49,621 x 4 ## Year President FirstName word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 Obama Barack waves ## 2 2013 Obama Barack realizes ## 3 2013 Obama Barack philadelphia ## 4 2013 Obama Barack 400 ## 5 2013 Obama Barack 40 ## 6 2013 Obama Barack absolutism ## 7 2013 Obama Barack contour ## 8 2013 Obama Barack newtown ## 9 2013 Obama Barack lanes ## 10 2013 Obama Barack appalachia ## # ... with 49,611 more rows We could then, for example, see how the appearance of a word changes over time: inaug_freq &lt;- inaug_words %&gt;% count(Year, word) %&gt;% ungroup() %&gt;% complete(Year, word, fill = list(n = 0)) %&gt;% group_by(Year) %&gt;% mutate(year_total = sum(n), percent = n / year_total) %&gt;% ungroup() inaug_freq ## # A tibble: 490,200 x 5 ## Year word n year_total percent ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1789 1 0 529 0.000000000 ## 2 1789 1,000 0 529 0.000000000 ## 3 1789 100 0 529 0.000000000 ## 4 1789 100,000,000 0 529 0.000000000 ## 5 1789 120,000,000 0 529 0.000000000 ## 6 1789 125 0 529 0.000000000 ## 7 1789 13 0 529 0.000000000 ## 8 1789 14th 1 529 0.001890359 ## 9 1789 15th 0 529 0.000000000 ## 10 1789 16 0 529 0.000000000 ## # ... with 490,190 more rows For example, we can use the broom package to perform logistic regression on each word. models &lt;- inaug_freq %&gt;% group_by(word) %&gt;% filter(sum(n) &gt; 50) %&gt;% do(tidy(glm(cbind(n, year_total - n) ~ Year, ., family = &quot;binomial&quot;))) %&gt;% ungroup() %&gt;% filter(term == &quot;Year&quot;) models ## # A tibble: 113 x 6 ## word term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 act Year 0.006894234 0.002191596 3.1457591 1.656564e-03 ## 2 action Year 0.001634417 0.001959204 0.8342250 4.041542e-01 ## 3 administration Year -0.006979577 0.001882474 -3.7076616 2.091819e-04 ## 4 america Year 0.018890081 0.001584306 11.9232506 8.954525e-33 ## 5 american Year 0.007084142 0.001321897 5.3590709 8.365105e-08 ## 6 americans Year 0.032657656 0.003659114 8.9250184 4.456252e-19 ## 7 authority Year -0.005640373 0.002336159 -2.4143787 1.576207e-02 ## 8 business Year 0.003745929 0.002016455 1.8576801 6.321445e-02 ## 9 called Year -0.001935068 0.002088388 -0.9265844 3.541423e-01 ## 10 century Year 0.016480566 0.002495844 6.6032027 4.023687e-11 ## # ... with 103 more rows models %&gt;% filter(term == &quot;Year&quot;) %&gt;% arrange(desc(abs(estimate))) ## # A tibble: 113 x 6 ## word term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 americans Year 0.03265766 0.003659114 8.925018 4.456252e-19 ## 2 america Year 0.01889008 0.001584306 11.923251 8.954525e-33 ## 3 century Year 0.01648057 0.002495844 6.603203 4.023687e-11 ## 4 live Year 0.01448914 0.002490610 5.817506 5.973212e-09 ## 5 democracy Year 0.01432438 0.002394738 5.981606 2.209489e-09 ## 6 god Year 0.01402582 0.001921362 7.299935 2.879058e-13 ## 7 freedom Year 0.01366336 0.001320242 10.349129 4.223092e-25 ## 8 foreign Year -0.01364998 0.002058045 -6.632497 3.300543e-11 ## 9 earth Year 0.01303351 0.002291996 5.686532 1.296449e-08 ## 10 world Year 0.01233715 0.001000739 12.328042 6.398240e-35 ## # ... with 103 more rows You can show these models as a volcano plot, which compares the effect size with the significance: library(ggplot2) theme_set(theme_bw()) models %&gt;% mutate(adjusted.p.value = p.adjust(p.value)) %&gt;% ggplot(aes(estimate, adjusted.p.value)) + geom_point() + scale_y_log10() + geom_text(aes(label = word), vjust = 1, hjust = 1, check_overlap = TRUE) + xlab(&quot;Estimated change over time&quot;) + ylab(&quot;Adjusted p-value&quot;) We can also use the ggplot2 package to display the top 6 terms that have changed in frequency over time. library(scales) models %&gt;% top_n(6, abs(estimate)) %&gt;% inner_join(inaug_freq) %&gt;% ggplot(aes(Year, percent)) + geom_point() + geom_smooth() + facet_wrap(~ word) + scale_y_continuous(labels = percent_format()) + ylab(&quot;Frequency of word in speech&quot;) "],
["topicmodeling.html", "Chapter 6 Topic Modeling 6.1 Can we tell the difference between Dickens, Wells, Verne, and Austen? 6.2 Setup 6.3 Latent Dirichlet Allocation with the topicmodels package 6.4 Per-document classification", " Chapter 6 Topic Modeling Topic modeling is a method for unsupervised classification of documents, by modeling each document as a mixture of topics and each topic as a mixture of words. Latent Dirichlet allocation is a particularly popular method for fitting a topic model. We can use tidy text principles, as described in the main vignette, to approach topic modeling using consistent and effective tools. In particular, we’ll be using tidying functions for LDA objects from the topicmodels package. 6.1 Can we tell the difference between Dickens, Wells, Verne, and Austen? Suppose a vandal has broken into your study and torn apart four of your books: Great Expectations by Charles Dickens The War of the Worlds by H.G. Wells Twenty Thousand Leagues Under the Sea by Jules Verne Pride and Prejudice by Jane Austen This vandal has torn the books into individual chapters, and left them in one large pile. How can we restore these disorganized chapters to their original books? 6.2 Setup We’ll retrieve four books using the gutenbergr package: library(dplyr) library(gutenbergr) titles &lt;- c(&quot;Twenty Thousand Leagues under the Sea&quot;, &quot;The War of the Worlds&quot;, &quot;Pride and Prejudice&quot;, &quot;Great Expectations&quot;) books &lt;- gutenberg_works(title %in% titles) %&gt;% gutenberg_download(meta_fields = &quot;title&quot;) books ## # A tibble: 51,663 x 3 ## gutenberg_id text title ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 36 The War of the Worlds The War of the Worlds ## 2 36 The War of the Worlds ## 3 36 by H. G. Wells [1898] The War of the Worlds ## 4 36 The War of the Worlds ## 5 36 The War of the Worlds ## 6 36 But who shall dwell in these worlds if they be The War of the Worlds ## 7 36 inhabited? . . . Are we or they Lords of the The War of the Worlds ## 8 36 World? . . . And how are all things made for man?-- The War of the Worlds ## 9 36 KEPLER (quoted in The Anatomy of Melancholy) The War of the Worlds ## 10 36 The War of the Worlds ## # ... with 51,653 more rows As pre-processing, we divide these into chapters, use tidytext’s unnest_tokens to separate them into words, then remove stop_words. We’re treating every chapter as a separate “document”, each with a name like Great Expectations_1 or Pride and Prejudice_11. library(tidytext) library(stringr) library(tidyr) by_chapter &lt;- books %&gt;% group_by(title) %&gt;% mutate(chapter = cumsum(str_detect(text, regex(&quot;^chapter &quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% filter(chapter &gt; 0) by_chapter_word &lt;- by_chapter %&gt;% unite(title_chapter, title, chapter) %&gt;% unnest_tokens(word, text) word_counts &lt;- by_chapter_word %&gt;% anti_join(stop_words) %&gt;% count(title_chapter, word, sort = TRUE) %&gt;% ungroup() word_counts ## # A tibble: 104,721 x 3 ## title_chapter word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations_57 joe 88 ## 2 Great Expectations_7 joe 70 ## 3 Great Expectations_17 biddy 63 ## 4 Great Expectations_27 joe 58 ## 5 Great Expectations_38 estella 58 ## 6 Great Expectations_2 joe 56 ## 7 Great Expectations_23 pocket 53 ## 8 Great Expectations_15 joe 50 ## 9 Great Expectations_18 joe 50 ## 10 The War of the Worlds_16 brother 50 ## # ... with 104,711 more rows 6.3 Latent Dirichlet Allocation with the topicmodels package Right now this data frame is in a tidy form, with one-term-per-document-per-row. However, the topicmodels package requires a DocumentTermMatrix (from the tm package). As described in this vignette, we can cast a one-token-per-row table into a DocumentTermMatrix with tidytext’s cast_dtm: chapters_dtm &lt;- word_counts %&gt;% cast_dtm(title_chapter, word, n) chapters_dtm ## &lt;&lt;DocumentTermMatrix (documents: 193, terms: 18215)&gt;&gt; ## Non-/sparse entries: 104721/3410774 ## Sparsity : 97% ## Maximal term length: 19 ## Weighting : term frequency (tf) Now we are ready to use the topicmodels package to create a four topic LDA model. library(topicmodels) chapters_lda &lt;- LDA(chapters_dtm, k = 4, control = list(seed = 1234)) chapters_lda ## A LDA_VEM topic model with 4 topics. (In this case we know there are four topics because there are four books; in practice we may need to try a few different values of k). Now tidytext gives us the option of returning to a tidy analysis, using the tidy and augment verbs borrowed from the broom package. In particular, we start with the tidy verb. chapters_lda_td &lt;- tidy(chapters_lda) chapters_lda_td ## # A tibble: 72,860 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 joe 5.830326e-17 ## 2 2 joe 3.194447e-57 ## 3 3 joe 4.162676e-24 ## 4 4 joe 1.445030e-02 ## 5 1 biddy 7.846976e-27 ## 6 2 biddy 4.672244e-69 ## 7 3 biddy 2.259711e-46 ## 8 4 biddy 4.767972e-03 ## 9 1 estella 3.827272e-06 ## 10 2 estella 5.316964e-65 ## # ... with 72,850 more rows Notice that this has turned the model into a one-topic-per-term-per-row format. For each combination the model has \\(\\beta\\), the probability of that term being generated from that topic. We could use dplyr’s top_n to find the top 5 terms within each topic: top_terms &lt;- chapters_lda_td %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms ## # A tibble: 20 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 elizabeth 0.014107538 ## 2 1 darcy 0.008814258 ## 3 1 miss 0.008706741 ## 4 1 bennet 0.006947431 ## 5 1 jane 0.006497512 ## 6 2 captain 0.015507696 ## 7 2 nautilus 0.013050048 ## 8 2 sea 0.008850073 ## 9 2 nemo 0.008708397 ## 10 2 ned 0.008030799 ## 11 3 people 0.006797400 ## 12 3 martians 0.006512569 ## 13 3 time 0.005347115 ## 14 3 black 0.005278302 ## 15 3 night 0.004483143 ## 16 4 joe 0.014450300 ## 17 4 time 0.006847574 ## 18 4 pip 0.006817363 ## 19 4 looked 0.006365257 ## 20 4 miss 0.006228387 This model lends itself to a visualization: library(ggplot2) theme_set(theme_bw()) top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ topic, scales = &quot;free&quot;) + theme(axis.text.x = element_text(size = 15, angle = 90, hjust = 1)) These topics are pretty clearly associated with the four books! There’s no question that the topic of “nemo”, “sea”, and “nautilus” belongs to Twenty Thousand Leagues Under the Sea, and that “jane”, “darcy”, and “elizabeth” belongs to Pride and Prejudice. We see “pip” and “joe” from Great Expectations and “martians”, “black”, and “night” from The War of the Worlds. 6.4 Per-document classification Each chapter was a “document” in this analysis. Thus, we may want to know which topics are associated with each document. Can we put the chapters back together in the correct books? chapters_lda_gamma &lt;- tidy(chapters_lda, matrix = &quot;gamma&quot;) chapters_lda_gamma ## # A tibble: 772 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations_57 1 1.351886e-05 ## 2 Great Expectations_7 1 1.470726e-05 ## 3 Great Expectations_17 1 2.117127e-05 ## 4 Great Expectations_27 1 1.919746e-05 ## 5 Great Expectations_38 1 3.544403e-01 ## 6 Great Expectations_2 1 1.723723e-05 ## 7 Great Expectations_23 1 5.507241e-01 ## 8 Great Expectations_15 1 1.682503e-02 ## 9 Great Expectations_18 1 1.272044e-05 ## 10 The War of the Worlds_16 1 1.084337e-05 ## # ... with 762 more rows Setting matrix = &quot;gamma&quot; returns a tidied version with one-document-per-topic-per-row. Now that we have these document classifiations, we can see how well our unsupervised learning did at distinguishing the four books. First we re-separate the document name into title and chapter: chapters_lda_gamma &lt;- chapters_lda_gamma %&gt;% separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE) chapters_lda_gamma ## # A tibble: 772 x 4 ## title chapter topic gamma ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations 57 1 1.351886e-05 ## 2 Great Expectations 7 1 1.470726e-05 ## 3 Great Expectations 17 1 2.117127e-05 ## 4 Great Expectations 27 1 1.919746e-05 ## 5 Great Expectations 38 1 3.544403e-01 ## 6 Great Expectations 2 1 1.723723e-05 ## 7 Great Expectations 23 1 5.507241e-01 ## 8 Great Expectations 15 1 1.682503e-02 ## 9 Great Expectations 18 1 1.272044e-05 ## 10 The War of the Worlds 16 1 1.084337e-05 ## # ... with 762 more rows Then we examine what fraction of chapters we got right for each: ggplot(chapters_lda_gamma, aes(gamma, fill = factor(topic))) + geom_histogram() + facet_wrap(~ title, nrow = 2) We notice that almost all of the chapters from Pride and Prejudice, War of the Worlds, and Twenty Thousand Leagues Under the Sea were uniquely identified as a single topic each. chapter_classifications &lt;- chapters_lda_gamma %&gt;% group_by(title, chapter) %&gt;% top_n(1, gamma) %&gt;% ungroup() %&gt;% arrange(gamma) chapter_classifications ## # A tibble: 193 x 4 ## title chapter topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations 54 3 0.4803234 ## 2 Great Expectations 22 4 0.5356506 ## 3 Great Expectations 31 4 0.5464851 ## 4 Great Expectations 23 1 0.5507241 ## 5 Great Expectations 33 4 0.5700737 ## 6 Great Expectations 47 4 0.5802089 ## 7 Great Expectations 56 4 0.5984806 ## 8 Great Expectations 38 4 0.6455341 ## 9 Great Expectations 11 4 0.6689600 ## 10 Great Expectations 44 4 0.6777974 ## # ... with 183 more rows We can determine this by finding the consensus book for each, which we note is correct based on our earlier visualization: book_topics &lt;- chapter_classifications %&gt;% count(title, topic) %&gt;% top_n(1, n) %&gt;% ungroup() %&gt;% transmute(consensus = title, topic) book_topics ## # A tibble: 4 x 2 ## consensus topic ## &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations 4 ## 2 Pride and Prejudice 1 ## 3 The War of the Worlds 3 ## 4 Twenty Thousand Leagues under the Sea 2 Then we see which chapters were misidentified: chapter_classifications %&gt;% inner_join(book_topics, by = &quot;topic&quot;) %&gt;% count(title, consensus) ## Source: local data frame [6 x 3] ## Groups: title [?] ## ## title consensus n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations Great Expectations 57 ## 2 Great Expectations Pride and Prejudice 1 ## 3 Great Expectations The War of the Worlds 1 ## 4 Pride and Prejudice Pride and Prejudice 61 ## 5 The War of the Worlds The War of the Worlds 27 ## 6 Twenty Thousand Leagues under the Sea Twenty Thousand Leagues under the Sea 46 We see that only a few chapters from Great Expectations were misclassified. Not bad for unsupervised clustering! 6.4.1 By word assignments: augment One important step in the topic modeling expectation-maximization algorithm is assigning each word in each document to a topic. The more words in a document are assigned to that topic, generally, the more weight (gamma) will go on that document-topic classification. We may want to take the original document-word pairs and find which words in each document were assigned to which topic. This is the job of the augment verb. assignments &lt;- augment(chapters_lda, data = chapters_dtm) We can combine this with the consensus book titles to find which words were incorrectly classified. assignments &lt;- assignments %&gt;% separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% inner_join(book_topics, by = c(&quot;.topic&quot; = &quot;topic&quot;)) assignments ## # A tibble: 104,721 x 6 ## title chapter term count .topic consensus ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Great Expectations 57 joe 88 4 Great Expectations ## 2 Great Expectations 7 joe 70 4 Great Expectations ## 3 Great Expectations 17 joe 5 4 Great Expectations ## 4 Great Expectations 27 joe 58 4 Great Expectations ## 5 Great Expectations 2 joe 56 4 Great Expectations ## 6 Great Expectations 23 joe 1 4 Great Expectations ## 7 Great Expectations 15 joe 50 4 Great Expectations ## 8 Great Expectations 18 joe 50 4 Great Expectations ## 9 Great Expectations 9 joe 44 4 Great Expectations ## 10 Great Expectations 13 joe 40 4 Great Expectations ## # ... with 104,711 more rows We can, for example, create a “confusion matrix” using dplyr’s count and tidyr’s spread: assignments %&gt;% count(title, consensus, wt = count) %&gt;% spread(consensus, n, fill = 0) ## Source: local data frame [4 x 5] ## Groups: title [4] ## ## title Great Expectations Pride and Prejudice ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Great Expectations 49770 3876 ## 2 Pride and Prejudice 1 37229 ## 3 The War of the Worlds 0 0 ## 4 Twenty Thousand Leagues under the Sea 0 5 ## The War of the Worlds Twenty Thousand Leagues under the Sea ## * &lt;dbl&gt; &lt;dbl&gt; ## 1 1845 77 ## 2 7 5 ## 3 22561 7 ## 4 0 39629 We notice that almost all the words for Pride and Prejudice, Twenty Thousand Leagues Under the Sea, and War of the Worlds were correctly assigned, while Great Expectations had a fair amount of misassignment. What were the most commonly mistaken words? wrong_words &lt;- assignments %&gt;% filter(title != consensus) wrong_words ## # A tibble: 4,535 x 6 ## title chapter term count .topic ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Great Expectations 38 brother 2 1 ## 2 Great Expectations 22 brother 4 1 ## 3 Great Expectations 23 miss 2 1 ## 4 Great Expectations 22 miss 23 1 ## 5 Twenty Thousand Leagues under the Sea 8 miss 1 1 ## 6 Great Expectations 31 miss 1 1 ## 7 Great Expectations 5 sergeant 37 1 ## 8 Great Expectations 46 captain 1 2 ## 9 Great Expectations 32 captain 1 2 ## 10 The War of the Worlds 17 captain 5 2 ## consensus ## &lt;chr&gt; ## 1 Pride and Prejudice ## 2 Pride and Prejudice ## 3 Pride and Prejudice ## 4 Pride and Prejudice ## 5 Pride and Prejudice ## 6 Pride and Prejudice ## 7 Pride and Prejudice ## 8 Twenty Thousand Leagues under the Sea ## 9 Twenty Thousand Leagues under the Sea ## 10 Twenty Thousand Leagues under the Sea ## # ... with 4,525 more rows wrong_words %&gt;% count(title, consensus, term, wt = count) %&gt;% ungroup() %&gt;% arrange(desc(n)) ## # A tibble: 3,500 x 4 ## title consensus term n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Great Expectations Pride and Prejudice love 44 ## 2 Great Expectations Pride and Prejudice sergeant 37 ## 3 Great Expectations Pride and Prejudice lady 32 ## 4 Great Expectations Pride and Prejudice miss 26 ## 5 Great Expectations The War of the Worlds boat 25 ## 6 Great Expectations Pride and Prejudice father 19 ## 7 Great Expectations The War of the Worlds water 19 ## 8 Great Expectations Pride and Prejudice baby 18 ## 9 Great Expectations Pride and Prejudice flopson 18 ## 10 Great Expectations Pride and Prejudice family 16 ## # ... with 3,490 more rows Notice the word “flopson” here; these wrong words do not necessarily appear in the novels they were misassigned to. Indeed, we can confirm “flopson” appears only in Great Expectations: word_counts %&gt;% filter(word == &quot;flopson&quot;) ## # A tibble: 3 x 3 ## title_chapter word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations_22 flopson 10 ## 2 Great Expectations_23 flopson 7 ## 3 Great Expectations_33 flopson 1 The algorithm is stochastic and iterative, and it can accidentally land on a topic that spans multiple books. "],
["word2vec.html", "Chapter 7 Tidying word2vec Models from the glove Package", " Chapter 7 Tidying word2vec Models from the glove Package TODO: still a lot of work to be done on the methods as well as the chapter, may or may not make it in "],
["yelp.html", "Chapter 8 Predicting ratings from text in the Yelp food reviews dataset 8.1 Setup 8.2 Tidy sentiment analysis 8.3 Which words are positive or negative? 8.4 Comparing to sentiment analysis", " Chapter 8 Predicting ratings from text in the Yelp food reviews dataset Intro goes here 8.1 Setup I’ve downloaded the yelp_dataset_challenge_academic_dataset folder from here.[^termsofuse] First I read and process them. library(readr) library(dplyr) # You may have used the built-in readLines before, but read_lines from # readr is faster for large files # we&#39;re reading only 100,000 in this example # you can try it with the full dataset too, it&#39;s just a little slower! # in the final version of the book we&#39;re probably going to read all, it # just makes this chapter take a while to compile infile &lt;- &quot;~/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json&quot; review_lines &lt;- read_lines(infile, n_max = 100000) library(stringr) # Each line is a JSON object- the fastest way to process is to combine into a # single JSON string and use jsonlite::fromJSON reviews_combined &lt;- str_c(&quot;[&quot;, str_c(review_lines, collapse = &quot;, &quot;), &quot;]&quot;) reviews &lt;- jsonlite::fromJSON(reviews_combined) %&gt;% jsonlite::flatten() %&gt;% tbl_df() reviews ## # A tibble: 100,000 x 10 ## user_id review_id stars date ## * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 PUFPaY9KxDAcGqfsorJp3Q Ya85v4eqdd6k9Od8HbQjyA 4 2012-08-01 ## 2 Iu6AxdBYGR4A0wspR9BYHA KPvLNJ21_4wbYNctrOwWdQ 5 2014-02-13 ## 3 auESFwWvW42h6alXgFxAXQ fFSoGV46Yxuwbr3fHNuZig 5 2015-10-31 ## 4 uK8tzraOp4M5u3uYrqIBXg Di3exaUCFNw1V4kSNW5pgA 5 2013-11-08 ## 5 I_47G-R2_egp7ME5u_ltew 0Lua2-PbqEQMjD9r89-asw 3 2014-03-29 ## 6 PP_xoMSYlGr2pb67BbqBdA 7N9j5YbBHBW6qguE5DAeyA 1 2014-10-29 ## 7 JPPhyFE-UE453zA6K0TVgw mjCJR33jvUNt41iJCxDU_g 4 2014-11-28 ## 8 2d5HeDvZTDUNVog_WuUpSg Ieh3kfZ-5J9pLju4JiQDvQ 5 2014-02-27 ## 9 BShxMIUwaJS378xcrz4Nmg PU28OoBSHpZLkYGCmNxlmg 5 2015-06-16 ## 10 fhNxoMwwTipzjO8A9LFe8Q XsA6AojkWjOHA4FmuAb8XQ 3 2012-08-19 ## # ... with 99,990 more rows, and 6 more variables: text &lt;chr&gt;, type &lt;chr&gt;, business_id &lt;chr&gt;, ## # votes.funny &lt;int&gt;, votes.useful &lt;int&gt;, votes.cool &lt;int&gt; 8.2 Tidy sentiment analysis Right now, there is one row for each review. To analyze in the tidy text framework, we need to use the unnest_tokens function and turn this into one-row-per-term-per-document: library(tidytext) review_words &lt;- reviews %&gt;% select(review_id, business_id, stars, text) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word, str_detect(word, &quot;^[a-z&#39;]+$&quot;)) review_words ## # A tibble: 3,971,444 x 4 ## review_id business_id stars word ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 hoagie ## 2 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 institution ## 3 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 walking ## 4 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 throwback ## 5 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 ago ## 6 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 fashioned ## 7 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 menu ## 8 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 board ## 9 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 booths ## 10 Ya85v4eqdd6k9Od8HbQjyA 5UmKMjUEUNdYWqANhGckJw 4 selection ## # ... with 3,971,434 more rows Notice that there is now one-row-per-term-per-document: the In this cleaning process we’ve also removed “stopwords” (such as “I”, “the”, “and”, etc), and removing things things that are formatting (e.g. “—-”) rather than a word. Now I’m going to do sentiment analysis on each review. We’ll use the AFINN lexicon, which provides a positivity score for each word, from -5 (most negative) to 5 (most positive). AFINN &lt;- sentiments %&gt;% filter(lexicon == &quot;AFINN&quot;) %&gt;% select(word, afinn_score = score) AFINN ## # A tibble: 2,476 x 2 ## word afinn_score ## &lt;chr&gt; &lt;int&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # ... with 2,466 more rows Now as described in this post, our sentiment analysis is just an inner-join operation followed by a summary: reviews_sentiment &lt;- review_words %&gt;% inner_join(AFINN, by = &quot;word&quot;) %&gt;% group_by(review_id, stars) %&gt;% summarize(sentiment = mean(afinn_score)) reviews_sentiment ## Source: local data frame [93,947 x 3] ## Groups: review_id [?] ## ## review_id stars sentiment ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 __-r0eC3hZlaejvuliC8zQ 5 4.0000000 ## 2 __56FUEaW57kZEm56OZk7w 5 0.8333333 ## 3 __6tOxx2VcvGR02d2ILkuw 5 1.7500000 ## 4 __77nP3Nf1wsGz5HPs2hdw 5 1.6000000 ## 5 __B5KInsYxFKIHKXAS6_rA 1 -2.0000000 ## 6 __BIQ3tcFZg6_PpdadEfLQ 4 1.6000000 ## 7 __DK9Vsmyoo0zJQhIl5cbg 1 -2.1000000 ## 8 __ELCJ0wzDM2QNRfVUq26Q 5 3.5000000 ## 9 __esH_kgJZeS8k3i6HaG7Q 5 0.2142857 ## 10 __GXnNfKFLqFhMtpCTTT2g 3 0.8750000 ## # ... with 93,937 more rows Now we can see how our estimates did! library(ggplot2) theme_set(theme_bw()) ggplot(reviews_sentiment, aes(stars, sentiment, group = stars)) + geom_boxplot() + ylab(&quot;Average sentiment score&quot;) Well, it’s a good start! Our sentiment scores are correlated with positivity ratings. But we do see that there’s a large amount of prediction error- some 5-star reviews have a highly negative sentiment score, and vice versa. 8.3 Which words are positive or negative? We’re interested in analyzing the properties of words. Which are suggestive of positive reviews, and which are negative? To do this, we’ll create a per-word summary. review_words_counted &lt;- review_words %&gt;% count(review_id, business_id, stars, word) %&gt;% ungroup() review_words_counted ## # A tibble: 3,405,173 x 5 ## review_id business_id stars word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 batter 1 ## 2 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 chips 3 ## 3 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 compares 1 ## 4 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 fashioned 1 ## 5 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 filleted 1 ## 6 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 fish 4 ## 7 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 fries 1 ## 8 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 frozen 1 ## 9 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 greenlake 1 ## 10 ___XYEos-RIkPsQwplRYyw YxMnfznT3eYya0YV37tE8w 5 hand 1 ## # ... with 3,405,163 more rows word_summaries &lt;- review_words_counted %&gt;% group_by(word) %&gt;% summarize(reviews = n(), uses = sum(n), average_stars = mean(stars)) %&gt;% ungroup() word_summaries ## # A tibble: 73,816 x 4 ## word reviews uses average_stars ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 a&#39;boiling 1 1 4.00 ## 2 a&#39;fare 1 1 4.00 ## 3 a&#39;ight 2 2 1.50 ## 4 a&#39;la 2 2 4.50 ## 5 a&#39;ll 1 1 1.00 ## 6 a&#39;lyce 1 2 5.00 ## 7 a&#39;more 2 2 5.00 ## 8 a&#39;orange 1 1 5.00 ## 9 a&#39;prowling 1 1 3.00 ## 10 aa 20 23 3.25 ## # ... with 73,806 more rows We can start by looking only at words that appear in at least 100 (out of 100000) reviews. This makes sense both because words that appear more rarely will have a noisier measurement (a few good or bad reviews could shift the balance), and because they’re less likely to be useful in classifying future reviews or text. word_summaries_filtered &lt;- word_summaries %&gt;% filter(reviews &gt;= 100) word_summaries_filtered ## # A tibble: 4,465 x 4 ## word reviews uses average_stars ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 aaa 100 145 3.780000 ## 2 ability 210 215 3.580952 ## 3 absolute 589 600 3.755518 ## 4 absolutely 3195 3401 3.812520 ## 5 ac 306 420 3.058824 ## 6 accent 112 115 3.446429 ## 7 accept 350 370 3.060000 ## 8 acceptable 313 319 2.645367 ## 9 accepted 162 167 3.030864 ## 10 access 530 588 3.541509 ## # ... with 4,455 more rows What were the most positive and negative words? word_summaries_filtered %&gt;% arrange(desc(average_stars)) ## # A tibble: 4,465 x 4 ## word reviews uses average_stars ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 exceeded 160 161 4.675000 ## 2 knowledgable 371 374 4.630728 ## 3 compassionate 112 115 4.625000 ## 4 exquisite 108 112 4.601852 ## 5 chihuly 117 151 4.547009 ## 6 treasure 152 159 4.546053 ## 7 compliments 212 215 4.542453 ## 8 gem 982 997 4.518330 ## 9 botanical 184 241 4.510870 ## 10 trustworthy 105 105 4.495238 ## # ... with 4,455 more rows word_summaries_filtered %&gt;% arrange(average_stars) ## # A tibble: 4,465 x 4 ## word reviews uses average_stars ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 incompetent 156 167 1.423077 ## 2 unprofessional 362 383 1.447514 ## 3 disgusted 124 126 1.475806 ## 4 rudely 167 179 1.520958 ## 5 lied 154 177 1.532468 ## 6 refund 390 497 1.558974 ## 7 refused 455 507 1.610989 ## 8 unacceptable 201 203 1.616915 ## 9 worst 2433 2653 1.619400 ## 10 denied 100 111 1.650000 ## # ... with 4,455 more rows Makes a lot of sense! We can also plot positivity by frequency: ggplot(word_summaries_filtered, aes(reviews, average_stars)) + geom_point() + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) + scale_x_log10() + geom_hline(yintercept = mean(reviews$stars), color = &quot;red&quot;, lty = 2) + xlab(&quot;# of reviews&quot;) + ylab(&quot;Average Stars&quot;) Note that some of the most common words (e.g. “food”) are pretty neutral. There are some common words that are pretty positive (e.g. “amazing”, “awesome”) and others that are pretty negative (“bad”, “told”). 8.4 Comparing to sentiment analysis When we perform sentiment analysis, we’re often comparing to a pre-existing lexicon, one that was developed. The tidytext package also comes with several tidy sentiment analysis lexicons: sentiments ## # A tibble: 23,165 x 4 ## word sentiment lexicon score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 abacus trust nrc NA ## 2 abandon fear nrc NA ## 3 abandon negative nrc NA ## 4 abandon sadness nrc NA ## 5 abandoned anger nrc NA ## 6 abandoned fear nrc NA ## 7 abandoned negative nrc NA ## 8 abandoned sadness nrc NA ## 9 abandonment anger nrc NA ## 10 abandonment fear nrc NA ## # ... with 23,155 more rows We might expect that more positive words are associated with higher star reviews. Does this hold? We can combine and compare the two datasets with inner_join. words_afinn &lt;- word_summaries_filtered %&gt;% inner_join(AFINN) words_afinn ## # A tibble: 520 x 5 ## word reviews uses average_stars afinn_score ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ability 210 215 3.580952 2 ## 2 accept 350 370 3.060000 1 ## 3 accepted 162 167 3.030864 1 ## 4 accident 213 239 3.629108 -2 ## 5 accidentally 152 152 3.348684 -2 ## 6 active 109 115 3.981651 1 ## 7 adequate 290 304 3.262069 1 ## 8 admit 740 754 3.666216 -1 ## 9 admitted 111 118 2.225225 -1 ## 10 adorable 255 266 4.250980 3 ## # ... with 510 more rows ggplot(words_afinn, aes(afinn_score, average_stars, group = afinn_score)) + geom_boxplot() + xlab(&quot;AFINN score of word&quot;) + ylab(&quot;Average stars of reviews with this word&quot;) Just like in our per-review predictions, there’s a very clear trend. AFINN sentiment analysis works, at least a little bit! But we may want to see some of those details. Which positive/negative words were most successful in predicting a positive/negative review, and which broke the trend? ## mapping: x = x ## geom_blank: na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity For example, we can see that most curse words have an AFINN score of -4, and that while some words, like “wtf”, successfully predict a negative review, others, like “damn”, are often positive. (They’re likely part of “damn good”, or something similar). Some of the words that AFINN most underestimated included “die” (“the pork chops are to die for!”), and one of the words it most overestimated was “joke” (“the service is a complete joke!”). One other way we could look at mis "],
["placeholder.html", "Chapter 9 Some analysis goes here", " Chapter 9 Some analysis goes here I don’t know what will go here, but I’d like to have one more analysis that touches on all of the areas of tidy text analysis. If we can’t find one we’ll skip it! "],
["references.html", "Chapter 10 References", " Chapter 10 References "]
]
