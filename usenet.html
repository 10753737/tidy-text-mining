<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Tidy Text Mining with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Tidy Text Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/tidycover.png" />
  <meta property="og:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="github-repo" content="dgrtwo/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Tidy Text Mining with R" />
  
  <meta name="twitter:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="twitter:image" content="images/tidycover.png" />

<meta name="author" content="Julia Silge and David Robinson">


<meta name="date" content="2016-12-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="nasa.html">
<link rel="next" href="references.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68765210-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Tidy Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Tidy Text Mining with R</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-tidy-text"><i class="fa fa-check"></i><b>1.1</b> What is tidy text?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#outline"><i class="fa fa-check"></i><b>1.3</b> Outline</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#topics-this-book-does-not-cover"><i class="fa fa-check"></i><b>1.4</b> Topics this book does not cover</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#acknowledgements"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tidytext.html"><a href="tidytext.html"><i class="fa fa-check"></i><b>2</b> The tidy text format</a><ul>
<li class="chapter" data-level="2.1" data-path="tidytext.html"><a href="tidytext.html#the-unnest_tokens-function"><i class="fa fa-check"></i><b>2.1</b> The <code>unnest_tokens</code> function</a></li>
<li class="chapter" data-level="2.2" data-path="tidytext.html"><a href="tidytext.html#tidying-the-works-of-jane-austen"><i class="fa fa-check"></i><b>2.2</b> Tidying the works of Jane Austen</a></li>
<li class="chapter" data-level="2.3" data-path="tidytext.html"><a href="tidytext.html#the-gutenbergr-package"><i class="fa fa-check"></i><b>2.3</b> The gutenbergr package</a></li>
<li class="chapter" data-level="2.4" data-path="tidytext.html"><a href="tidytext.html#word-frequencies"><i class="fa fa-check"></i><b>2.4</b> Word frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sentiment.html"><a href="sentiment.html"><i class="fa fa-check"></i><b>3</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="3.1" data-path="sentiment.html"><a href="sentiment.html#the-sentiments-dataset"><i class="fa fa-check"></i><b>3.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="3.2" data-path="sentiment.html"><a href="sentiment.html#sentiment-analysis-with-inner-join"><i class="fa fa-check"></i><b>3.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="3.3" data-path="sentiment.html"><a href="sentiment.html#comparing-the-three-sentiment-dictionaries"><i class="fa fa-check"></i><b>3.3</b> Comparing the three sentiment dictionaries</a></li>
<li class="chapter" data-level="3.4" data-path="sentiment.html"><a href="sentiment.html#most-common-positive-and-negative-words"><i class="fa fa-check"></i><b>3.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="3.5" data-path="sentiment.html"><a href="sentiment.html#wordclouds"><i class="fa fa-check"></i><b>3.5</b> Wordclouds</a></li>
<li class="chapter" data-level="3.6" data-path="sentiment.html"><a href="sentiment.html#looking-at-units-beyond-just-words"><i class="fa fa-check"></i><b>3.6</b> Looking at units beyond just words</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tfidf.html"><a href="tfidf.html"><i class="fa fa-check"></i><b>4</b> Analyzing word and document frequency: tf-idf</a><ul>
<li class="chapter" data-level="4.1" data-path="tfidf.html"><a href="tfidf.html#term-frequency-and-inverse-document-frequency"><i class="fa fa-check"></i><b>4.1</b> Term frequency and inverse document frequency</a></li>
<li class="chapter" data-level="4.2" data-path="tfidf.html"><a href="tfidf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>4.2</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="4.3" data-path="tfidf.html"><a href="tfidf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>4.3</b> The <code>bind_tf_idf</code> function</a></li>
<li class="chapter" data-level="4.4" data-path="tfidf.html"><a href="tfidf.html#a-corpus-of-physics-texts"><i class="fa fa-check"></i><b>4.4</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ngrams.html"><a href="ngrams.html"><i class="fa fa-check"></i><b>5</b> Working with combinations of words using n-grams and widyr</a><ul>
<li class="chapter" data-level="5.1" data-path="ngrams.html"><a href="ngrams.html#tokenizing-by-n-gram"><i class="fa fa-check"></i><b>5.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="5.1.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-filtering-n-grams"><i class="fa fa-check"></i><b>5.1.1</b> Counting and filtering n-grams</a></li>
<li class="chapter" data-level="5.1.2" data-path="ngrams.html"><a href="ngrams.html#analyzing-bigrams"><i class="fa fa-check"></i><b>5.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="5.1.3" data-path="ngrams.html"><a href="ngrams.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>5.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="5.1.4" data-path="ngrams.html"><a href="ngrams.html#visualizing-a-network-of-bigrams-with-igraph"><i class="fa fa-check"></i><b>5.1.4</b> Visualizing a network of bigrams with igraph</a></li>
<li class="chapter" data-level="5.1.5" data-path="ngrams.html"><a href="ngrams.html#visualizing-bigrams-in-other-texts"><i class="fa fa-check"></i><b>5.1.5</b> Visualizing bigrams in other texts</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-pairs-of-words-with-the-widyr-package"><i class="fa fa-check"></i><b>5.2</b> Counting and correlating pairs of words with the widyr package</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>5.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="5.2.2" data-path="ngrams.html"><a href="ngrams.html#pairwise-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="dtm.html"><a href="dtm.html"><i class="fa fa-check"></i><b>6</b> Tidying and casting document-term matrices</a><ul>
<li class="chapter" data-level="6.1" data-path="dtm.html"><a href="dtm.html#tidying-a-document-term-matrix"><i class="fa fa-check"></i><b>6.1</b> Tidying a document-term matrix</a></li>
<li class="chapter" data-level="6.2" data-path="dtm.html"><a href="dtm.html#casting-tidy-text-data-into-a-documenttermmatrix"><i class="fa fa-check"></i><b>6.2</b> Casting tidy text data into a DocumentTermMatrix</a></li>
<li class="chapter" data-level="6.3" data-path="dtm.html"><a href="dtm.html#tidying-corpus-objects-with-metadata"><i class="fa fa-check"></i><b>6.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="topicmodeling.html"><a href="topicmodeling.html"><i class="fa fa-check"></i><b>7</b> Topic modeling</a><ul>
<li class="chapter" data-level="7.1" data-path="topicmodeling.html"><a href="topicmodeling.html#the-great-library-heist"><i class="fa fa-check"></i><b>7.1</b> The great library heist</a></li>
<li class="chapter" data-level="7.2" data-path="topicmodeling.html"><a href="topicmodeling.html#latent-dirichlet-allocation-with-the-topicmodels-package"><i class="fa fa-check"></i><b>7.2</b> Latent Dirichlet allocation with the topicmodels package</a></li>
<li class="chapter" data-level="7.3" data-path="topicmodeling.html"><a href="topicmodeling.html#per-document-classification"><i class="fa fa-check"></i><b>7.3</b> Per-document classification</a></li>
<li class="chapter" data-level="7.4" data-path="topicmodeling.html"><a href="topicmodeling.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>7.4</b> By word assignments: <code>augment</code></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="twitter.html"><a href="twitter.html"><i class="fa fa-check"></i><b>8</b> Case study: comparing Twitter archives</a><ul>
<li class="chapter" data-level="8.1" data-path="twitter.html"><a href="twitter.html#getting-the-data-and-distribution-of-tweets"><i class="fa fa-check"></i><b>8.1</b> Getting the data and distribution of tweets</a></li>
<li class="chapter" data-level="8.2" data-path="twitter.html"><a href="twitter.html#word-frequencies-1"><i class="fa fa-check"></i><b>8.2</b> Word frequencies</a></li>
<li class="chapter" data-level="8.3" data-path="twitter.html"><a href="twitter.html#comparing-word-usage"><i class="fa fa-check"></i><b>8.3</b> Comparing word usage</a></li>
<li class="chapter" data-level="8.4" data-path="twitter.html"><a href="twitter.html#sentiment-analysis"><i class="fa fa-check"></i><b>8.4</b> Sentiment analysis</a></li>
<li class="chapter" data-level="8.5" data-path="twitter.html"><a href="twitter.html#words-that-contribute-to-sentiment-in-tweets"><i class="fa fa-check"></i><b>8.5</b> Words that contribute to sentiment in tweets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nasa.html"><a href="nasa.html"><i class="fa fa-check"></i><b>9</b> Case study: mining NASA metadata</a><ul>
<li class="chapter" data-level="9.1" data-path="nasa.html"><a href="nasa.html#getting-the-metadata"><i class="fa fa-check"></i><b>9.1</b> Getting the metadata</a></li>
<li class="chapter" data-level="9.2" data-path="nasa.html"><a href="nasa.html#wrangling-and-tidying-the-data"><i class="fa fa-check"></i><b>9.2</b> Wrangling and tidying the data</a></li>
<li class="chapter" data-level="9.3" data-path="nasa.html"><a href="nasa.html#some-initial-simple-exploration"><i class="fa fa-check"></i><b>9.3</b> Some initial simple exploration</a></li>
<li class="chapter" data-level="9.4" data-path="nasa.html"><a href="nasa.html#word-co-ocurrences"><i class="fa fa-check"></i><b>9.4</b> Word co-ocurrences</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="usenet.html"><a href="usenet.html"><i class="fa fa-check"></i><b>10</b> Case study: analyzing usenet text</a><ul>
<li class="chapter" data-level="10.1" data-path="usenet.html"><a href="usenet.html#wrangling-the-data"><i class="fa fa-check"></i><b>10.1</b> Wrangling the data</a></li>
<li class="chapter" data-level="10.2" data-path="usenet.html"><a href="usenet.html#term-frequency-and-inverse-document-frequency-tf-idf"><i class="fa fa-check"></i><b>10.2</b> Term frequency and inverse document frequency: tf-idf</a></li>
<li class="chapter" data-level="10.3" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-1"><i class="fa fa-check"></i><b>10.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="10.4" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-word"><i class="fa fa-check"></i><b>10.4</b> Sentiment analysis by word</a></li>
<li class="chapter" data-level="10.5" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-message"><i class="fa fa-check"></i><b>10.5</b> Sentiment analysis by message</a></li>
<li class="chapter" data-level="10.6" data-path="usenet.html"><a href="usenet.html#n-grams"><i class="fa fa-check"></i><b>10.6</b> N-grams</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Tidy Text Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="usenet" class="section level1">
<h1><span class="header-section-number">10</span> Case study: analyzing usenet text</h1>
<p>In our final chapter, we’ll use what we’ve learned in this book to perform a start-to-finish analysis of a set of 20,000 messages sent to 20 Usenet bulletin boards in 1993. The Usenet bulletin boards in this data set include boards for topics like politics, autos, “for sale”, atheism, etc. This data set is <a href="http://qwone.com/~jason/20Newsgroups/">publicly available</a> and has become popular for testing and exercises in text analysis and machine learning.</p>
<div id="wrangling-the-data" class="section level2">
<h2><span class="header-section-number">10.1</span> Wrangling the data</h2>
<p>We’ll start by reading in all the messages. (Note that this step takes several minutes).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)
<span class="kw">library</span>(purrr)
<span class="kw">library</span>(readr)
<span class="kw">library</span>(stringr)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">training_folder &lt;-<span class="st"> &quot;data/20news-bydate/20news-bydate-train/&quot;</span>

read_folder &lt;-<span class="st"> </span>function(infolder) {
  <span class="kw">print</span>(infolder)
  <span class="kw">data_frame</span>(<span class="dt">file =</span> <span class="kw">dir</span>(infolder, <span class="dt">full.names =</span> <span class="ot">TRUE</span>)) %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">map</span>(file, read_lines)) %&gt;%
<span class="st">    </span><span class="kw">transmute</span>(<span class="dt">id =</span> <span class="kw">basename</span>(file), text) %&gt;%
<span class="st">    </span><span class="kw">unnest</span>(text)
}

raw_text &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">folder =</span> <span class="kw">dir</span>(training_folder, <span class="dt">full.names =</span> <span class="ot">TRUE</span>)) %&gt;%
<span class="st">  </span><span class="kw">unnest</span>(<span class="kw">map</span>(folder, read_folder)) %&gt;%
<span class="st">  </span><span class="kw">transmute</span>(<span class="dt">board =</span> <span class="kw">basename</span>(folder), id, text)</code></pre></div>
<p>Each email has structure we need to remove. For starters:</p>
<ul>
<li>Every email has one or more headers (e.g. “from:”, “in_reply_to:”)</li>
<li>Many have signatures, which (since they’re constant for each user) we wouldn’t want to examine alongside the content</li>
</ul>
<p>We need to remove headers and signatures.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># remove headers and signatures</span>
cleaned_text &lt;-<span class="st"> </span>raw_text %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(id) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">cumsum</span>(text ==<span class="st"> &quot;&quot;</span>) &gt;<span class="st"> </span><span class="dv">0</span>,
         <span class="kw">cumsum</span>(<span class="kw">str_detect</span>(text, <span class="st">&quot;^--&quot;</span>)) ==<span class="st"> </span><span class="dv">0</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>()

<span class="co"># remove nested text (starting with &quot;&gt;&quot;) and lines that note the author</span>
<span class="co"># of those</span>
cleaned_text &lt;-<span class="st"> </span>cleaned_text %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">str_detect</span>(text, <span class="st">&quot;^[^&gt;]+[A-Za-z</span><span class="ch">\\</span><span class="st">d]&quot;</span>) |<span class="st"> </span>text ==<span class="st"> &quot;&quot;</span>,
         !<span class="kw">str_detect</span>(text, <span class="st">&quot;writes(:|</span><span class="ch">\\</span><span class="st">.</span><span class="ch">\\</span><span class="st">.</span><span class="ch">\\</span><span class="st">.)$&quot;</span>),
         !<span class="kw">str_detect</span>(text, <span class="st">&quot;^In article &lt;&quot;</span>),
         !id %in%<span class="st"> </span><span class="kw">c</span>(<span class="dv">9704</span>, <span class="dv">9985</span>))</code></pre></div>
<p>Now it is time to use <code>unnest_tokens</code> to identify the words in this data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidytext)

usenet_words &lt;-<span class="st"> </span>cleaned_text %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">str_detect</span>(word, <span class="st">&quot;^[a-z]&quot;</span>),
         <span class="kw">str_detect</span>(word, <span class="st">&quot;[a-z]$&quot;</span>),
         !word %in%<span class="st"> </span>stop_words$word)</code></pre></div>
<p>What are the most common words?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">usenet_words %&gt;%
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## # A tibble: 63,937 × 2
##       word     n
##      &lt;chr&gt; &lt;int&gt;
## 1   people  3397
## 2     time  2569
## 3      god  1611
## 4   system  1571
## 5  subject  1312
## 6    lines  1188
## 7  program  1086
## 8  windows  1085
## 9      bit  1070
## 10   space  1062
## # ... with 63,927 more rows</code></pre>
<p>Or perhaps more sensibly, we could examine the most common words by board.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">words_by_board &lt;-<span class="st"> </span>usenet_words %&gt;%
<span class="st">  </span><span class="kw">count</span>(board, word) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">words_by_board %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(board) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>## Source: local data frame [60 x 3]
## Groups: board [20]
## 
##                       board     word     n
##                       &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;
## 1               alt.atheism      god   268
## 2               alt.atheism    jesus   129
## 3               alt.atheism   people   276
## 4             comp.graphics graphics   217
## 5             comp.graphics    image   169
## 6             comp.graphics  program   134
## 7   comp.os.ms-windows.misc      dos   194
## 8   comp.os.ms-windows.misc     file   232
## 9   comp.os.ms-windows.misc  windows   625
## 10 comp.sys.ibm.pc.hardware     card   237
## # ... with 50 more rows</code></pre>
<p>These look sensible and illuminating so far; let’s move on to some more sophisticated analysis!</p>
</div>
<div id="term-frequency-and-inverse-document-frequency-tf-idf" class="section level2">
<h2><span class="header-section-number">10.2</span> Term frequency and inverse document frequency: tf-idf</h2>
<p>Some words are likely to be more common on particular boards. Let’s try quantifying this using the tf-idf metric we learned in <a href="tfidf.html#tfidf">Chapter 4</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tf_idf &lt;-<span class="st"> </span>words_by_board %&gt;%
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, board, n) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))

tf_idf</code></pre></div>
<pre><code>## # A tibble: 166,528 × 6
##                       board           word     n          tf      idf     tf_idf
##                       &lt;chr&gt;          &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1  comp.sys.ibm.pc.hardware           scsi   483 0.018138801 1.203973 0.02183862
## 2           rec.motorcycles           bike   321 0.013750268 1.386294 0.01906192
## 3     talk.politics.mideast       armenian   440 0.007348275 2.302585 0.01692003
## 4                 sci.crypt     encryption   410 0.008311878 1.897120 0.01576863
## 5     talk.politics.mideast      armenians   396 0.006613447 2.302585 0.01522803
## 6          rec.sport.hockey            nhl   151 0.004291114 2.995732 0.01285503
## 7  comp.sys.ibm.pc.hardware            ide   208 0.007811326 1.609438 0.01257184
## 8        talk.politics.misc stephanopoulos   158 0.004175145 2.995732 0.01250762
## 9           rec.motorcycles          bikes    97 0.004155065 2.995732 0.01244746
## 10         rec.sport.hockey         hockey   265 0.007530762 1.609438 0.01212029
## # ... with 166,518 more rows</code></pre>
<p>We can visualize this for a few select boards. First, let’s look at all the <code>sci.</code> boards.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)

tf_idf %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">str_detect</span>(board, <span class="st">&quot;^sci</span><span class="ch">\\</span><span class="st">.&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(board) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">12</span>, tf_idf) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, tf_idf)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, tf_idf, <span class="dt">fill =</span> board)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>board, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;tf-idf&quot;</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre></div>
<p><img src="10-usenet_files/figure-html/unnamed-chunk-7-1.png" width="864" /></p>
<p>We could use almost the same code (not shown) to compare the “rec.” (recreation) or “talk.” boards:</p>
<p><img src="10-usenet_files/figure-html/unnamed-chunk-8-1.png" width="864" /><img src="10-usenet_files/figure-html/unnamed-chunk-8-2.png" width="864" /></p>
<p>We see lots of characteristic words for these boards, from “pitching” and “hitter” for the baseball board to “firearm” and “militia” on the guns board. Notice how high tf-idf is for words like “Stephanopoulos” or “Armenian”; this means that these words are very unique among the documents as a whole and important to those particular boards.</p>
</div>
<div id="sentiment-analysis-1" class="section level2">
<h2><span class="header-section-number">10.3</span> Sentiment analysis</h2>
<p>We can use the sentiment analysis techniques we explored in <a href="sentiment.html#sentiment">Chapter 3</a> to examine how positive and negative words were used in these Usenet posts. Which boards used the most positive and negative words?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">AFINN &lt;-<span class="st"> </span><span class="kw">get_sentiments</span>(<span class="st">&quot;afinn&quot;</span>)

word_board_sentiments &lt;-<span class="st"> </span>words_by_board %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(AFINN, <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>)

board_sentiments &lt;-<span class="st"> </span>word_board_sentiments %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(board) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">score =</span> <span class="kw">sum</span>(score *<span class="st"> </span>n) /<span class="st"> </span><span class="kw">sum</span>(n))

board_sentiments %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">board =</span> <span class="kw">reorder</span>(board, score)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(board, score, <span class="dt">fill =</span> score &gt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Average sentiment score&quot;</span>)</code></pre></div>
<p><img src="10-usenet_files/figure-html/board_sentiments-1.png" width="672" /></p>
</div>
<div id="sentiment-analysis-by-word" class="section level2">
<h2><span class="header-section-number">10.4</span> Sentiment analysis by word</h2>
<p>It’s worth looking deeper to understand <em>why</em> some boards ended up more positive than others. For that, we can examine the total positive and negative contributions of each word.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">contributions &lt;-<span class="st"> </span>usenet_words %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(AFINN, <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(word) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">occurences =</span> <span class="kw">n</span>(),
            <span class="dt">contribution =</span> <span class="kw">sum</span>(score))

contributions</code></pre></div>
<pre><code>## # A tibble: 1,891 × 3
##         word occurences contribution
##        &lt;chr&gt;      &lt;int&gt;        &lt;int&gt;
## 1    abandon         12          -24
## 2  abandoned         18          -36
## 3   abandons          3           -6
## 4  abduction          1           -2
## 5      abhor          3           -9
## 6   abhorred          1           -3
## 7  abhorrent          2           -6
## 8  abilities         16           32
## 9    ability        160          320
## 10    aboard          8            8
## # ... with 1,881 more rows</code></pre>
<p>Which words had the most effect?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">contributions %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">25</span>, <span class="kw">abs</span>(contribution)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, contribution)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, contribution, <span class="dt">fill =</span> contribution &gt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre></div>
<p><img src="10-usenet_files/figure-html/unnamed-chunk-9-1.png" width="576" /></p>
<p>These words look generally reasonable as indicators of each message’s sentiment, but we can spot possible problems with the approach. “True” could just as easily be a part of “not true” or a similar negative expression, and the words “God” and “Jesus” are apparently very common on Usenet but could easily be used in many contexts, positive or negative.</p>
<p>The important point is that we may also care about which words contributed the most <em>within each board</em>. We can calculate each word’s contribution to each board’s sentiment score from our <code>word_board_sentiments</code> variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">top_sentiment_words &lt;-<span class="st"> </span>word_board_sentiments %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">contribution =</span> score *<span class="st"> </span>n /<span class="st"> </span><span class="kw">sum</span>(n))

top_sentiment_words %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(board) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">8</span>, <span class="kw">abs</span>(contribution)) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">board =</span> <span class="kw">reorder</span>(board, contribution),
         <span class="dt">word =</span> <span class="kw">reorder</span>(word, contribution)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, contribution, <span class="dt">fill =</span> contribution &gt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>board, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre></div>
<p><img src="10-usenet_files/figure-html/top_sentiment_words-1.png" width="960" /></p>
<p>We can see here how much sentiment is confounded with topic in this particular approach. An atheism board is likely to discuss “god” in detail even in a negative context, and we can see it makes the board look more positive. Similarly, the negative contribution of the word “gun” to the “talk.politics.guns” board would occur even if the board members were discussing guns positively.</p>
</div>
<div id="sentiment-analysis-by-message" class="section level2">
<h2><span class="header-section-number">10.5</span> Sentiment analysis by message</h2>
<p>We can also try finding the most positive and negative <em>messages</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sentiment_messages &lt;-<span class="st"> </span>usenet_words %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(AFINN, <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(board, id) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">sentiment =</span> <span class="kw">mean</span>(score),
            <span class="dt">words =</span> <span class="kw">n</span>()) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">filter</span>(words &gt;=<span class="st"> </span><span class="dv">5</span>)</code></pre></div>
<p>As a simple measure to reduce the role of randomness, we filtered out messages that had fewer than five words that contributed to sentiment.</p>
<p>What were the most positive messages?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sentiment_messages %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(sentiment))</code></pre></div>
<pre><code>## # A tibble: 3,385 × 4
##                      board     id sentiment words
##                      &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;
## 1         rec.sport.hockey  53560  3.888889    18
## 2         rec.sport.hockey  53602  3.833333    30
## 3         rec.sport.hockey  53822  3.833333     6
## 4         rec.sport.hockey  53645  3.230769    13
## 5                rec.autos 102768  3.200000     5
## 6             misc.forsale  75965  3.000000     5
## 7             misc.forsale  76037  3.000000     5
## 8       rec.sport.baseball 104458  2.916667    12
## 9  comp.os.ms-windows.misc   9620  2.857143     7
## 10            misc.forsale  74787  2.833333     6
## # ... with 3,375 more rows</code></pre>
<p>Let’s check this by looking at the most positive message in the whole data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">print_message &lt;-<span class="st"> </span>function(message_id) {
  cleaned_text %&gt;%
<span class="st">    </span><span class="kw">filter</span>(id ==<span class="st"> </span>message_id) %&gt;%
<span class="st">    </span><span class="kw">filter</span>(text !=<span class="st"> &quot;&quot;</span>) %&gt;%
<span class="st">    </span>.$text %&gt;%
<span class="st">    </span><span class="kw">cat</span>(<span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
}

<span class="kw">print_message</span>(<span class="dv">53560</span>)</code></pre></div>
<pre><code>## Everybody.  Please send me your predictions for the Stanley Cup Playoffs!
## I want to see who people think will win.!!!!!!!
## Please Send them in this format, or something comparable:
## 1. Winner of Buffalo-Boston
## 2. Winner of Montreal-Quebec
## 3. Winner of Pittsburgh-New York
## 4. Winner of New Jersey-Washington
## 5. Winner of Chicago-(Minnesota/St.Louis)
## 6. Winner of Toronto-Detroit
## 7. Winner of Vancouver-Winnipeg
## 8. Winner of Calgary-Los Angeles
## 9. Winner of Adams Division (1-2 above)
## 10. Winner of Patrick Division (3-4 above)
## 11. Winner of Norris Division (5-6 above)
## 12. Winner of Smythe Division (7-8 above)
## 13. Winner of Wales Conference (9-10 above)
## 14. Winner of Campbell Conference (11-12 above)
## 15. Winner of Stanley Cup (13-14 above)
## I will summarize the predictions, and see who is the biggest
## INTERNET GURU PREDICTING GUY/GAL.
## Send entries to Richard Madison
## rrmadiso@napier.uwaterloo.ca
## PS:  I will send my entries to one of you folks so you know when I say
## I won, that I won!!!!!
## From: sknapp@iastate.edu (Steven M. Knapp)
## Subject: Re: Radar detector DETECTORS?
## Organization: Iowa State University, Ames, IA
## Lines: 16
## Yes some radar detectors are less detectable by radar detector
## detectors. ;-)
## Look in Car and Driver (last 6 months should do), they had a big
## review of the &quot;better&quot; detectors, and stealth was a factor.
## Steven M. Knapp                             Computer Engineering Student
## sknapp@iastate.edu                  President Cyclone Amateur Radio Club
## Iowa State University; Ames, IA; USA      Durham Center Operations Staff</code></pre>
<p>Looks like it’s because the message uses the word “winner” a lot! How about the most negative message? Turns out it’s also from the hockey site, but has a very different attitude.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sentiment_messages %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(sentiment)</code></pre></div>
<pre><code>## # A tibble: 3,385 × 4
##                    board     id sentiment words
##                    &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;
## 1       rec.sport.hockey  53907 -3.000000     6
## 2        sci.electronics  53899 -3.000000     5
## 3              rec.autos 101627 -2.833333     6
## 4          comp.graphics  37948 -2.800000     5
## 5         comp.windows.x  67204 -2.700000    10
## 6     talk.politics.guns  53362 -2.666667     6
## 7            alt.atheism  51309 -2.600000     5
## 8  comp.sys.mac.hardware  51513 -2.600000     5
## 9              rec.autos 102883 -2.600000     5
## 10       rec.motorcycles  72052 -2.600000     5
## # ... with 3,375 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print_message</span>(<span class="dv">53907</span>)</code></pre></div>
<pre><code>## Losers like us? You are the fucking moron who has never heard of the Western
## Business School, or the University of Western Ontario for that matter. Why 
## don&#39;t you pull your head out of your asshole and smell something other than
## shit for once so you can look on a map to see where UWO is! Back to hockey,
## the North Stars should be moved because for the past few years they have
## just been SHIT. A real team like Toronto would never be moved!!!
## Andrew--</code></pre>
<p>Well then.</p>
</div>
<div id="n-grams" class="section level2">
<h2><span class="header-section-number">10.6</span> N-grams</h2>
<p>We can also examine the effect of words that are used in negation, like we did in <a href="ngrams.html#ngrams">Chapter 5</a>. Let’s start by finding all the bigrams in the Usenet posts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">usenet_bigrams &lt;-<span class="st"> </span>cleaned_text %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>)

usenet_bigrams</code></pre></div>
<pre><code>## # A tibble: 1,762,089 × 3
##          board    id            bigram
##          &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;
## 1  alt.atheism 49960      archive name
## 2  alt.atheism 49960      name atheism
## 3  alt.atheism 49960 atheism resources
## 4  alt.atheism 49960     resources alt
## 5  alt.atheism 49960       alt atheism
## 6  alt.atheism 49960   atheism archive
## 7  alt.atheism 49960      archive name
## 8  alt.atheism 49960    name resources
## 9  alt.atheism 49960    resources last
## 10 alt.atheism 49960     last modified
## # ... with 1,762,079 more rows</code></pre>
<p>Now let’s count how many of these bigrams are used in each board.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">usenet_bigram_counts &lt;-<span class="st"> </span>usenet_bigrams %&gt;%
<span class="st">  </span><span class="kw">count</span>(board, bigram)

usenet_bigram_counts %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n))</code></pre></div>
<pre><code>## Source: local data frame [1,006,415 x 3]
## Groups: board [20]
## 
##                     board bigram     n
##                     &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;
## 1  soc.religion.christian of the  1141
## 2   talk.politics.mideast of the  1135
## 3   talk.politics.mideast in the   857
## 4               sci.space of the   684
## 5               sci.crypt of the   671
## 6      talk.politics.misc of the   645
## 7  soc.religion.christian in the   637
## 8      talk.religion.misc of the   630
## 9      talk.politics.guns of the   618
## 10            alt.atheism of the   474
## # ... with 1,006,405 more rows</code></pre>
<p>Next, we can calculate tf-idf for the bigrams to find the ones that are important for each board.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bigram_tf_idf &lt;-<span class="st"> </span>usenet_bigram_counts %&gt;%
<span class="st">  </span><span class="kw">bind_tf_idf</span>(bigram, board, n)

bigram_tf_idf %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))</code></pre></div>
<pre><code>## Source: local data frame [1,006,415 x 6]
## Groups: board [20]
## 
##                       board            bigram     n          tf      idf      tf_idf
##                       &lt;chr&gt;             &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
## 1        talk.politics.misc mr stephanopoulos   155 0.001477344 2.995732 0.004425728
## 2            comp.windows.x               n x   177 0.001917577 2.302585 0.004415384
## 3            comp.windows.x          x printf   130 0.001408390 2.995732 0.004219158
## 4           rec.motorcycles          the bike   104 0.001675663 2.302585 0.003858356
## 5  comp.sys.ibm.pc.hardware            scsi 2   107 0.001478983 2.302585 0.003405485
## 6            comp.windows.x            file x   104 0.001126712 2.995732 0.003375327
## 7     talk.politics.mideast     the armenians   169 0.001111988 2.995732 0.003331220
## 8          rec.sport.hockey               1 0   256 0.002733816 1.203973 0.003291440
## 9            comp.windows.x      output oname   100 0.001083377 2.995732 0.003245506
## 10           comp.windows.x            x char    98 0.001061709 2.995732 0.003180596
## # ... with 1,006,405 more rows</code></pre>
<p>Now we come back to the words used in negation that we are interested in examining. Let’s define a vector of words that we suspect are used in negation, and use the same joining and counting approach from <a href="ngrams.html#ngrams">Chapter 5</a> to examine all of them at once.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">negate_words &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;not&quot;</span>, <span class="st">&quot;without&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;can&#39;t&quot;</span>, <span class="st">&quot;don&#39;t&quot;</span>, <span class="st">&quot;won&#39;t&quot;</span>)

usenet_bigram_counts %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">separate</span>(bigram, <span class="kw">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word2&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(word1 %in%<span class="st"> </span>negate_words) %&gt;%
<span class="st">  </span><span class="kw">count</span>(word1, word2, <span class="dt">wt =</span> n, <span class="dt">sort =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(AFINN, <span class="dt">by =</span> <span class="kw">c</span>(<span class="dt">word2 =</span> <span class="st">&quot;word&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">contribution =</span> score *<span class="st"> </span>nn) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>, <span class="kw">abs</span>(contribution)) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word2 =</span> <span class="kw">reorder</span>(word2, contribution)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word2, contribution, <span class="dt">fill =</span> contribution &gt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>word1, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">nrow =</span> <span class="dv">3</span>) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Words preceded by negation&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Sentiment score * # of occurrences&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre></div>
<p><img src="10-usenet_files/figure-html/negate_words-1.png" width="768" /></p>
<p>These words are the ones that contribute the most to the sentiment scores in the wrong direction, because they are being used with negation words before them. Phrases like “no problem” and “don’t want” are important sources of misidentification.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nasa.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dgrtwo/tidy-text-mining/edit/master/10-usenet.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
