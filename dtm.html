<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Text Mining with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Text Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="github-repo" content="dgrtwo/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Text Mining with R" />
  
  <meta name="twitter:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Julia Silge and David Robinson">


<meta name="date" content="2017-03-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ngrams.html">
<link rel="next" href="topicmodeling.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68765210-2', 'auto');
  ga('send', 'pageview');

</script>
p.caption {
  color: #777;
  margin-top: 10px;
}
p code {
  white-space: inherit;
}
pre {
  word-break: normal;
  word-wrap: normal;
}
pre code {
  white-space: inherit;
}


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Text Mining with R</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#topics-this-book-does-not-cover"><i class="fa fa-check"></i>Topics this book does not cover</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#using-code-examples"><i class="fa fa-check"></i>Using code examples</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="tidytext.html"><a href="tidytext.html"><i class="fa fa-check"></i><b>1</b> The tidy text format</a><ul>
<li class="chapter" data-level="1.1" data-path="tidytext.html"><a href="tidytext.html#contrasting-tidy-text-with-other-data-structures"><i class="fa fa-check"></i><b>1.1</b> Contrasting tidy text with other data structures</a></li>
<li class="chapter" data-level="1.2" data-path="tidytext.html"><a href="tidytext.html#the-unnest_tokens-function"><i class="fa fa-check"></i><b>1.2</b> The <code>unnest_tokens</code> function</a></li>
<li class="chapter" data-level="1.3" data-path="tidytext.html"><a href="tidytext.html#tidying-the-works-of-jane-austen"><i class="fa fa-check"></i><b>1.3</b> Tidying the works of Jane Austen</a></li>
<li class="chapter" data-level="1.4" data-path="tidytext.html"><a href="tidytext.html#the-gutenbergr-package"><i class="fa fa-check"></i><b>1.4</b> The gutenbergr package</a></li>
<li class="chapter" data-level="1.5" data-path="tidytext.html"><a href="tidytext.html#word-frequencies"><i class="fa fa-check"></i><b>1.5</b> Word frequencies</a></li>
<li class="chapter" data-level="1.6" data-path="tidytext.html"><a href="tidytext.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sentiment.html"><a href="sentiment.html"><i class="fa fa-check"></i><b>2</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="2.1" data-path="sentiment.html"><a href="sentiment.html#the-sentiments-dataset"><i class="fa fa-check"></i><b>2.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="2.2" data-path="sentiment.html"><a href="sentiment.html#sentiment-analysis-with-inner-join"><i class="fa fa-check"></i><b>2.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="2.3" data-path="sentiment.html"><a href="sentiment.html#comparing-the-three-sentiment-dictionaries"><i class="fa fa-check"></i><b>2.3</b> Comparing the three sentiment dictionaries</a></li>
<li class="chapter" data-level="2.4" data-path="sentiment.html"><a href="sentiment.html#most-positive-negative"><i class="fa fa-check"></i><b>2.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="2.5" data-path="sentiment.html"><a href="sentiment.html#wordclouds"><i class="fa fa-check"></i><b>2.5</b> Wordclouds</a></li>
<li class="chapter" data-level="2.6" data-path="sentiment.html"><a href="sentiment.html#looking-at-units-beyond-just-words"><i class="fa fa-check"></i><b>2.6</b> Looking at units beyond just words</a></li>
<li class="chapter" data-level="2.7" data-path="sentiment.html"><a href="sentiment.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tfidf.html"><a href="tfidf.html"><i class="fa fa-check"></i><b>3</b> Analyzing word and document frequency: tf-idf</a><ul>
<li class="chapter" data-level="3.1" data-path="tfidf.html"><a href="tfidf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>3.1</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="3.2" data-path="tfidf.html"><a href="tfidf.html#zipfs-law"><i class="fa fa-check"></i><b>3.2</b> Zipf’s law</a></li>
<li class="chapter" data-level="3.3" data-path="tfidf.html"><a href="tfidf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>3.3</b> The <code>bind_tf_idf</code> function</a></li>
<li class="chapter" data-level="3.4" data-path="tfidf.html"><a href="tfidf.html#a-corpus-of-physics-texts"><i class="fa fa-check"></i><b>3.4</b> A corpus of physics texts</a></li>
<li class="chapter" data-level="3.5" data-path="tfidf.html"><a href="tfidf.html#summary-2"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ngrams.html"><a href="ngrams.html"><i class="fa fa-check"></i><b>4</b> Relationships between words</a><ul>
<li class="chapter" data-level="4.1" data-path="ngrams.html"><a href="ngrams.html#tokenizing-by-n-gram"><i class="fa fa-check"></i><b>4.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-filtering-n-grams"><i class="fa fa-check"></i><b>4.1.1</b> Counting and filtering n-grams</a></li>
<li class="chapter" data-level="4.1.2" data-path="ngrams.html"><a href="ngrams.html#analyzing-bigrams"><i class="fa fa-check"></i><b>4.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="4.1.3" data-path="ngrams.html"><a href="ngrams.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>4.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="4.1.4" data-path="ngrams.html"><a href="ngrams.html#visualizing-a-network-of-bigrams-with-igraph"><i class="fa fa-check"></i><b>4.1.4</b> Visualizing a network of bigrams with igraph</a></li>
<li class="chapter" data-level="4.1.5" data-path="ngrams.html"><a href="ngrams.html#visualizing-bigrams-in-other-texts"><i class="fa fa-check"></i><b>4.1.5</b> Visualizing bigrams in other texts</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-pairs-of-words-with-the-widyr-package"><i class="fa fa-check"></i><b>4.2</b> Counting and correlating pairs of words with the widyr package</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>4.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="4.2.2" data-path="ngrams.html"><a href="ngrams.html#pairwise-correlation"><i class="fa fa-check"></i><b>4.2.2</b> Pairwise correlation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ngrams.html"><a href="ngrams.html#summary-3"><i class="fa fa-check"></i><b>4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dtm.html"><a href="dtm.html"><i class="fa fa-check"></i><b>5</b> Tidying and casting document-term matrices and corpus objects</a><ul>
<li class="chapter" data-level="5.1" data-path="dtm.html"><a href="dtm.html#tidy-dtm"><i class="fa fa-check"></i><b>5.1</b> Tidying a document-term matrix</a><ul>
<li class="chapter" data-level="5.1.1" data-path="dtm.html"><a href="dtm.html#tidying-documenttermmatrix-objects"><i class="fa fa-check"></i><b>5.1.1</b> Tidying DocumentTermMatrix objects</a></li>
<li class="chapter" data-level="5.1.2" data-path="dtm.html"><a href="dtm.html#tidying-dfm-objects"><i class="fa fa-check"></i><b>5.1.2</b> Tidying dfm objects</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dtm.html"><a href="dtm.html#cast-dtm"><i class="fa fa-check"></i><b>5.2</b> Casting tidy text data into a matrix</a></li>
<li class="chapter" data-level="5.3" data-path="dtm.html"><a href="dtm.html#tidying-corpus-objects-with-metadata"><i class="fa fa-check"></i><b>5.3</b> Tidying corpus objects with metadata</a><ul>
<li class="chapter" data-level="5.3.1" data-path="dtm.html"><a href="dtm.html#financial"><i class="fa fa-check"></i><b>5.3.1</b> Example: mining financial articles</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="dtm.html"><a href="dtm.html#summary-4"><i class="fa fa-check"></i><b>5.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="topicmodeling.html"><a href="topicmodeling.html"><i class="fa fa-check"></i><b>6</b> Topic modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="topicmodeling.html"><a href="topicmodeling.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>6.1</b> Latent Dirichlet allocation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="topicmodeling.html"><a href="topicmodeling.html#word-topic-probabilities"><i class="fa fa-check"></i><b>6.1.1</b> Word-topic probabilities</a></li>
<li class="chapter" data-level="6.1.2" data-path="topicmodeling.html"><a href="topicmodeling.html#document-topic-probabilities"><i class="fa fa-check"></i><b>6.1.2</b> Document-topic probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="topicmodeling.html"><a href="topicmodeling.html#library-heist"><i class="fa fa-check"></i><b>6.2</b> Example: the great library heist</a><ul>
<li class="chapter" data-level="6.2.1" data-path="topicmodeling.html"><a href="topicmodeling.html#lda-on-chapters"><i class="fa fa-check"></i><b>6.2.1</b> LDA on chapters</a></li>
<li class="chapter" data-level="6.2.2" data-path="topicmodeling.html"><a href="topicmodeling.html#per-document"><i class="fa fa-check"></i><b>6.2.2</b> Per-document classification</a></li>
<li class="chapter" data-level="6.2.3" data-path="topicmodeling.html"><a href="topicmodeling.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>6.2.3</b> By word assignments: <code>augment</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="topicmodeling.html"><a href="topicmodeling.html#alternative-lda-implementations"><i class="fa fa-check"></i><b>6.3</b> Alternative LDA implementations</a></li>
<li class="chapter" data-level="6.4" data-path="topicmodeling.html"><a href="topicmodeling.html#summary-5"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="twitter.html"><a href="twitter.html"><i class="fa fa-check"></i><b>7</b> Case study: comparing Twitter archives</a><ul>
<li class="chapter" data-level="7.1" data-path="twitter.html"><a href="twitter.html#getting-the-data-and-distribution-of-tweets"><i class="fa fa-check"></i><b>7.1</b> Getting the data and distribution of tweets</a></li>
<li class="chapter" data-level="7.2" data-path="twitter.html"><a href="twitter.html#word-frequencies-1"><i class="fa fa-check"></i><b>7.2</b> Word frequencies</a></li>
<li class="chapter" data-level="7.3" data-path="twitter.html"><a href="twitter.html#comparing-word-usage"><i class="fa fa-check"></i><b>7.3</b> Comparing word usage</a></li>
<li class="chapter" data-level="7.4" data-path="twitter.html"><a href="twitter.html#changes-in-word-use"><i class="fa fa-check"></i><b>7.4</b> Changes in word use</a></li>
<li class="chapter" data-level="7.5" data-path="twitter.html"><a href="twitter.html#favorites-and-retweets"><i class="fa fa-check"></i><b>7.5</b> Favorites and retweets</a></li>
<li class="chapter" data-level="7.6" data-path="twitter.html"><a href="twitter.html#summary-6"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nasa.html"><a href="nasa.html"><i class="fa fa-check"></i><b>8</b> Case study: mining NASA metadata</a><ul>
<li class="chapter" data-level="8.1" data-path="nasa.html"><a href="nasa.html#how-data-is-organized-at-nasa"><i class="fa fa-check"></i><b>8.1</b> How data is organized at NASA</a><ul>
<li class="chapter" data-level="8.1.1" data-path="nasa.html"><a href="nasa.html#wrangling-and-tidying-the-data"><i class="fa fa-check"></i><b>8.1.1</b> Wrangling and tidying the data</a></li>
<li class="chapter" data-level="8.1.2" data-path="nasa.html"><a href="nasa.html#some-initial-simple-exploration"><i class="fa fa-check"></i><b>8.1.2</b> Some initial simple exploration</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="nasa.html"><a href="nasa.html#word-co-ocurrences-and-correlations"><i class="fa fa-check"></i><b>8.2</b> Word co-ocurrences and correlations</a><ul>
<li class="chapter" data-level="8.2.1" data-path="nasa.html"><a href="nasa.html#networks-of-description-and-title-words"><i class="fa fa-check"></i><b>8.2.1</b> Networks of Description and Title Words</a></li>
<li class="chapter" data-level="8.2.2" data-path="nasa.html"><a href="nasa.html#networks-of-keywords"><i class="fa fa-check"></i><b>8.2.2</b> Networks of Keywords</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nasa.html"><a href="nasa.html#calculating-tf-idf-for-the-description-fields"><i class="fa fa-check"></i><b>8.3</b> Calculating tf-idf for the description fields</a><ul>
<li class="chapter" data-level="8.3.1" data-path="nasa.html"><a href="nasa.html#what-is-tf-idf-for-the-description-field-words"><i class="fa fa-check"></i><b>8.3.1</b> What is tf-idf for the description field words?</a></li>
<li class="chapter" data-level="8.3.2" data-path="nasa.html"><a href="nasa.html#connecting-description-fields-to-keywords"><i class="fa fa-check"></i><b>8.3.2</b> Connecting description fields to keywords</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="nasa.html"><a href="nasa.html#topic-modeling"><i class="fa fa-check"></i><b>8.4</b> Topic modeling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="nasa.html"><a href="nasa.html#casting-to-a-document-term-matrix"><i class="fa fa-check"></i><b>8.4.1</b> Casting to a document-term matrix</a></li>
<li class="chapter" data-level="8.4.2" data-path="nasa.html"><a href="nasa.html#ready-for-topic-modeling"><i class="fa fa-check"></i><b>8.4.2</b> Ready for topic modeling</a></li>
<li class="chapter" data-level="8.4.3" data-path="nasa.html"><a href="nasa.html#interpreting-the-topic-model"><i class="fa fa-check"></i><b>8.4.3</b> Interpreting the topic model</a></li>
<li class="chapter" data-level="8.4.4" data-path="nasa.html"><a href="nasa.html#connecting-topic-modeling-with-keywords"><i class="fa fa-check"></i><b>8.4.4</b> Connecting topic modeling with keywords</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="nasa.html"><a href="nasa.html#summary-7"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="usenet.html"><a href="usenet.html"><i class="fa fa-check"></i><b>9</b> Case study: analyzing usenet text</a><ul>
<li class="chapter" data-level="9.1" data-path="usenet.html"><a href="usenet.html#pre-processing"><i class="fa fa-check"></i><b>9.1</b> Pre-processing</a><ul>
<li class="chapter" data-level="9.1.1" data-path="usenet.html"><a href="usenet.html#pre-processing-text"><i class="fa fa-check"></i><b>9.1.1</b> Pre-processing text</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="usenet.html"><a href="usenet.html#words-in-newsgroups"><i class="fa fa-check"></i><b>9.2</b> Words in newsgroups</a><ul>
<li class="chapter" data-level="9.2.1" data-path="usenet.html"><a href="usenet.html#finding-tf-idf-within-newsgroups"><i class="fa fa-check"></i><b>9.2.1</b> Finding tf-idf within newsgroups</a></li>
<li class="chapter" data-level="9.2.2" data-path="usenet.html"><a href="usenet.html#topic-modeling-1"><i class="fa fa-check"></i><b>9.2.2</b> Topic modeling</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="usenet.html"><a href="usenet.html#sentiment-analysis"><i class="fa fa-check"></i><b>9.3</b> Sentiment analysis</a><ul>
<li class="chapter" data-level="9.3.1" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-word"><i class="fa fa-check"></i><b>9.3.1</b> Sentiment analysis by word</a></li>
<li class="chapter" data-level="9.3.2" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-message"><i class="fa fa-check"></i><b>9.3.2</b> Sentiment analysis by message</a></li>
<li class="chapter" data-level="9.3.3" data-path="usenet.html"><a href="usenet.html#n-gram-analysis"><i class="fa fa-check"></i><b>9.3.3</b> N-gram analysis</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="usenet.html"><a href="usenet.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>10</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dtm" class="section level1">
<h1><span class="header-section-number">5</span> Tidying and casting document-term matrices and corpus objects</h1>
<p>In the previous chapters, we’ve been analyzing text arranged in the tidy text format: a table with one-token-per-document-per-row, such as is constructed by the <code>unnest_tokens()</code> function. This lets us use the popular suite of tidy tools such as dplyr, tidyr, and ggplot2 to explore and visualize text data. We’ve demonstrated that many informative text analyses can be performed using these tools.</p>
<p>However, most of the existing R tools for natural language processing, besides the tidytext package, aren’t compatible with this format. The <a href="https://cran.r-project.org/web/views/NaturalLanguageProcessing.html">CRAN Task View for Natural Language Processing</a> lists a large selection of packages that take other structures of input and provide non-tidy outputs. These packages are very useful in text mining applications, and many existing text datasets are structured according to these formats.</p>
<p>Computer scientist Hal Abelson has observed that “No matter how complex and polished the individual operations are, it is often the quality of the glue that most directly determines the power of the system” <span class="citation">(Abelson <a href="#ref-Friedman:2008:EPL:1378240">2008</a>)</span>. In that spirit, this chapter will discuss the “glue” that connects the tidy text format with other important packages and data structures, allowing you to rely on both existing text mining packages and the suite of tidy tools to perform your analysis. In particular, we’ll examine the process of tidying document-term matrices, as well as casting a tidy data frame into a sparse matrix.</p>
<div id="tidy-dtm" class="section level2">
<h2><span class="header-section-number">5.1</span> Tidying a document-term matrix</h2>
<p>One of the most common structures that text mining packages work with is the <a href="https://en.wikipedia.org/wiki/Document-term_matrix">document-term matrix</a> (or DTM). This is a matrix where:</p>
<ul>
<li>each row represents one document (such as a book or article),</li>
<li>each column represents one term, and</li>
<li>each value (typically) contains the number of appearances of that term in that document.</li>
</ul>
<p>Since most pairings of document and term do not occur (they have the value zero), DTMs are usually implemented as sparse matrices. These objects can be treated as though they were matrices (for example, accessing particular rows and columns), but are stored in a more efficient format. We’ll discuss several implementations of these matrices in this chapter.</p>
<p>DTM objects cannot be used directly with tidy tools, just as tidy data frames cannot be used as input for most text mining packages. Thus, the tidytext package provides two verbs that convert between the two formats.</p>
<ul>
<li><code>tidy()</code> turns a document-term matrix into a tidy data frame. This verb comes from the broom package <span class="citation">(Robinson et al. <a href="#ref-R-broom">2015</a>)</span>, which provides similar tidying functions for many statistical models and objects.</li>
<li><code>cast()</code> turns a tidy one-term-per-row data frame into a matrix. tidytext provides three variations of this verb, each converting to a different type of matrix: <code>cast_sparse()</code> (converting to a sparse matrix from the Matrix package), <code>cast_dtm()</code> (converting to a <code>DocumentTermMatrix</code> object from tm), and <code>cast_dfm()</code> (converting to a <code>dfm</code> object from quanteda).</li>
</ul>
<div id="tidying-documenttermmatrix-objects" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Tidying DocumentTermMatrix objects</h3>
<p>Perhaps the most widely used implementation of DTMs in R is the <code>DocumentTermMatrix</code> class in the tm package. Many available text mining datasets are provided in this format. For example, consider the collection of Associated Press newspaper articles included in the topicmodels package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tm)

<span class="kw">data</span>(<span class="st">&quot;AssociatedPress&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;topicmodels&quot;</span>)
AssociatedPress</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;
## Non-/sparse entries: 302031/23220327
## Sparsity           : 99%
## Maximal term length: 18
## Weighting          : term frequency (tf)</code></pre>
<p>We see that this dataset contains documents (each of them an AP article) and terms (distinct words). Notice that this DTM is 99% sparse (99% of document-word pairs are zero). We could access the terms in the document with the <code>Terms()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">terms &lt;-<span class="st"> </span><span class="kw">Terms</span>(AssociatedPress)
<span class="kw">head</span>(terms)</code></pre></div>
<pre><code>## [1] &quot;aaron&quot;      &quot;abandon&quot;    &quot;abandoned&quot;  &quot;abandoning&quot; &quot;abbott&quot;     &quot;abboud&quot;</code></pre>
<p>If we wanted to analyze this data with tidy tools, we would first need to turn it into a data frame with one-token-per-document-per-row. The broom package introduced the <code>tidy()</code> verb, which takes a non-tidy object and turns it into a tidy data frame. The tidytext package implements this method for <code>DocumentTermMatrix</code> objects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidytext)

ap_td &lt;-<span class="st"> </span><span class="kw">tidy</span>(AssociatedPress)
ap_td</code></pre></div>
<pre><code>## # A tibble: 302,031 × 3
##    document       term count
##       &lt;int&gt;      &lt;chr&gt; &lt;dbl&gt;
## 1         1     adding     1
## 2         1      adult     2
## 3         1        ago     1
## 4         1    alcohol     1
## 5         1  allegedly     1
## 6         1      allen     1
## 7         1 apparently     2
## 8         1   appeared     1
## 9         1   arrested     1
## 10        1    assault     1
## # ... with 302,021 more rows</code></pre>
<p>Notice that we now have a tidy three-column <code>tbl_df</code>, with variables <code>document</code>, <code>term</code>, and <code>count</code>. This tidying operation is similar to the <code>melt()</code> function from the reshape2 package <span class="citation">(Wickham <a href="#ref-R-reshape2">2007</a>)</span> for non-sparse matrices. Notice that only the non-zero values are included: document 1 includes terms such as “adding” and “adult”, but not “aaron” or “abandon”, which means the tidied version has no rows where <code>count</code> is zero.</p>
<p>As we’ve seen in previous chapters, this form is convenient for analysis with the dplyr, tidytext and ggplot2 packages. For example, you can perform sentiment analysis on these newspaper articles with the approach described in Chapter <a href="sentiment.html#sentiment">2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ap_sentiments &lt;-<span class="st"> </span>ap_td %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;bing&quot;</span>), <span class="dt">by =</span> <span class="kw">c</span>(<span class="dt">term =</span> <span class="st">&quot;word&quot;</span>))

ap_sentiments</code></pre></div>
<pre><code>## # A tibble: 30,094 × 4
##    document    term count sentiment
##       &lt;int&gt;   &lt;chr&gt; &lt;dbl&gt;     &lt;chr&gt;
## 1         1 assault     1  negative
## 2         1 complex     1  negative
## 3         1   death     1  negative
## 4         1    died     1  negative
## 5         1    good     2  positive
## 6         1 illness     1  negative
## 7         1  killed     2  negative
## 8         1    like     2  positive
## 9         1   liked     1  positive
## 10        1 miracle     1  positive
## # ... with 30,084 more rows</code></pre>
<p>This would let us visualize which words from the AP articles most often contributed to positive or negative sentiment, seen in Figure <a href="dtm.html#fig:apsentiments">5.1</a>. We can see that the most common positive words include “like”, “work”, “support”, and “good”, while the most negative words include “killed”, “death”, and “vice”. (The inclusion of “vice” as a negative term is probably a mistake on the algorithm’s part, since it likely usually refers to “vice president”).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)

ap_sentiments %&gt;%
<span class="st">  </span><span class="kw">count</span>(sentiment, term, <span class="dt">wt =</span> count) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">filter</span>(n &gt;=<span class="st"> </span><span class="dv">200</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">ifelse</span>(sentiment ==<span class="st"> &quot;negative&quot;</span>, -n, n)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">term =</span> <span class="kw">reorder</span>(term, n)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(term, n, <span class="dt">fill =</span> sentiment)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Contribution to sentiment&quot;</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre></div>
<div class="figure"><span id="fig:apsentiments"></span>
<img src="05-document-term-matrices_files/figure-html/apsentiments-1.png" alt="Words from AP articles with the greatest contribution to positive or negative sentiments, computed as the product of the word's AFINN sentiment score and its frequency." width="672" />
<p class="caption">
Figure 5.1: Words from AP articles with the greatest contribution to positive or negative sentiments, computed as the product of the word’s AFINN sentiment score and its frequency.
</p>
</div>
</div>
<div id="tidying-dfm-objects" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Tidying dfm objects</h3>
<p>Other text mining packages provide alternative implementations of document-term matrices, such as the <code>dfm</code> (document-feature matrix) class from the quanteda package <span class="citation">(Benoit and Nulty <a href="#ref-R-quanteda">2016</a>)</span>. For example, the quanteda package comes with a corpus of presidential inauguration speeches, which can be converted to a <code>dfm</code> using the appropriate function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(methods)

<span class="kw">data</span>(<span class="st">&quot;data_corpus_inaugural&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;quanteda&quot;</span>)
inaug_dfm &lt;-<span class="st"> </span>quanteda::<span class="kw">dfm</span>(data_corpus_inaugural)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inaug_dfm</code></pre></div>
<pre><code>## Document-feature matrix of: 58 documents, 9,232 features (91.6% sparse).</code></pre>
<p>The <code>tidy</code> method works on these document-feature matrices as well, turning them into a one-token-per-document-per-row table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inaug_td &lt;-<span class="st"> </span><span class="kw">tidy</span>(inaug_dfm)
inaug_td</code></pre></div>
<pre><code>## # A tibble: 44,725 × 3
##           document   term count
##              &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;
## 1  1789-Washington fellow     3
## 2  1793-Washington fellow     1
## 3       1797-Adams fellow     3
## 4   1801-Jefferson fellow     7
## 5   1805-Jefferson fellow     8
## 6     1809-Madison fellow     1
## 7     1813-Madison fellow     1
## 8      1817-Monroe fellow     6
## 9      1821-Monroe fellow    10
## 10      1825-Adams fellow     3
## # ... with 44,715 more rows</code></pre>
<p>We may be interested in finding the words most specific to each of the inaugural speeches. This could be quantified by calculating the tf-idf of each term-speech pair using the <code>bind_tf_idf()</code> function, as described in Chapter <a href="tfidf.html#tfidf">3</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inaug_tf_idf &lt;-<span class="st"> </span>inaug_td %&gt;%
<span class="st">  </span><span class="kw">bind_tf_idf</span>(term, document, count) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))

inaug_tf_idf</code></pre></div>
<pre><code>## # A tibble: 44,725 × 6
##           document        term count          tf      idf     tf_idf
##              &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1  1793-Washington      arrive     1 0.006802721 4.060443 0.02762206
## 2  1793-Washington upbraidings     1 0.006802721 4.060443 0.02762206
## 3  1793-Washington    violated     1 0.006802721 3.367296 0.02290677
## 4  1793-Washington   willingly     1 0.006802721 3.367296 0.02290677
## 5  1793-Washington   incurring     1 0.006802721 3.367296 0.02290677
## 6  1793-Washington    previous     1 0.006802721 2.961831 0.02014851
## 7  1793-Washington   knowingly     1 0.006802721 2.961831 0.02014851
## 8  1793-Washington injunctions     1 0.006802721 2.961831 0.02014851
## 9  1793-Washington   witnesses     1 0.006802721 2.961831 0.02014851
## 10 1793-Washington     besides     1 0.006802721 2.674149 0.01819149
## # ... with 44,715 more rows</code></pre>
<p>We could use this data to pick four notable inaugural addresses (from Presidents Lincoln, Roosevelt, Kennedy, and Obama), and visualize the words most specific to each speech, as shown in Figure <a href="dtm.html#fig:presidentspeeches">5.2</a>.</p>
<div class="figure"><span id="fig:presidentspeeches"></span>
<img src="05-document-term-matrices_files/figure-html/presidentspeeches-1.png" alt="The terms with the highest tf-idf from each of four selected inaugural addresses. Note that quanteda's tokenizer includes the '?' punctuation mark as a term, though the texts we've tokenized ourselves with unnest_tokens do not." width="576" />
<p class="caption">
Figure 5.2: The terms with the highest tf-idf from each of four selected inaugural addresses. Note that quanteda’s tokenizer includes the ‘?’ punctuation mark as a term, though the texts we’ve tokenized ourselves with unnest_tokens do not.
</p>
</div>
<p>As another example of a visualization possible with tidy data, we could extract the year from each document’s name, and compute the total number of words within each year. Note that we’ve used tidyr’s <code>complete()</code> function to include zeroes (cases where a word didn’t appear in a document) in the table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyr)

year_term_counts &lt;-<span class="st"> </span>inaug_td %&gt;%
<span class="st">  </span><span class="kw">extract</span>(document, <span class="st">&quot;year&quot;</span>, <span class="st">&quot;(</span><span class="ch">\\</span><span class="st">d+)&quot;</span>, <span class="dt">convert =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">complete</span>(year, term, <span class="dt">fill =</span> <span class="kw">list</span>(<span class="dt">count =</span> <span class="dv">0</span>)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(year) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">year_total =</span> <span class="kw">sum</span>(count))</code></pre></div>
<p>This lets us pick several words and visualize how they changed in frequency over time, as shown in <a href="dtm.html#fig:yearterm">5.3</a>. We can see that over time, American presidents became less likely to refer to the country as the “Union” and more likely to refer to “America”. They also became less likely to talk about the “constitution” and foreign&quot; countries, and more likely to mention “freedom” and “God”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">year_term_counts %&gt;%
<span class="st">  </span><span class="kw">filter</span>(term %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;god&quot;</span>, <span class="st">&quot;america&quot;</span>, <span class="st">&quot;foreign&quot;</span>, <span class="st">&quot;union&quot;</span>, <span class="st">&quot;constitution&quot;</span>, <span class="st">&quot;freedom&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(year, count /<span class="st"> </span>year_total)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>() +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>term, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>) +
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales::<span class="kw">percent_format</span>()) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;% frequency of word in inaugural address&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:yearterm"></span>
<img src="05-document-term-matrices_files/figure-html/yearterm-1.png" alt="Changes in word frequency over time within Presidential inaugural addresses, for four selected terms" width="672" />
<p class="caption">
Figure 5.3: Changes in word frequency over time within Presidential inaugural addresses, for four selected terms
</p>
</div>
<p>These examples show how you can use tidytext, and the related suite of tidy tools, to analyze sources even if their origin was not in a tidy format.</p>
</div>
</div>
<div id="cast-dtm" class="section level2">
<h2><span class="header-section-number">5.2</span> Casting tidy text data into a matrix</h2>
<p>Just as some existing text mining packages provide document-term matrices as sample data or output, some algorithms expect such matrices as input. Therefore, tidytext provides <code>cast_</code> verbs for converting from a tidy form to these matrices.</p>
<p>For example, we could take the tidied AP dataset and cast it back into a document-term matrix using the <code>cast_dtm()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ap_td %&gt;%
<span class="st">  </span><span class="kw">cast_dtm</span>(document, term, count)</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;
## Non-/sparse entries: 302031/23220327
## Sparsity           : 99%
## Maximal term length: 18
## Weighting          : term frequency (tf)</code></pre>
<p>Similarly, we could cast the table into a <code>dfm</code> object from quanteda’s dfm with <code>cast_dfm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ap_td %&gt;%
<span class="st">  </span><span class="kw">cast_dfm</span>(term, document, count)</code></pre></div>
<pre><code>## Document-feature matrix of: 10,473 documents, 2,246 features (98.7% sparse).</code></pre>
<p>Some tools simply require a sparse matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Matrix)

<span class="co"># cast into a Matrix object</span>
m &lt;-<span class="st"> </span>ap_td %&gt;%
<span class="st">  </span><span class="kw">cast_sparse</span>(document, term, count)

<span class="kw">class</span>(m)</code></pre></div>
<pre><code>## [1] &quot;dgCMatrix&quot;
## attr(,&quot;package&quot;)
## [1] &quot;Matrix&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(m)</code></pre></div>
<pre><code>## [1]  2246 10473</code></pre>
<p>This kind of conversion could easily be done from any of the tidy text structures we’ve used so far in this book. For example, we could create a DTM of Jane Austen’s books in just a few lines of code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(janeaustenr)

austen_dtm &lt;-<span class="st"> </span><span class="kw">austen_books</span>() %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text) %&gt;%
<span class="st">  </span><span class="kw">count</span>(book, word) %&gt;%
<span class="st">  </span><span class="kw">cast_dtm</span>(book, word, n)

austen_dtm</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 6, terms: 14520)&gt;&gt;
## Non-/sparse entries: 40379/46741
## Sparsity           : 54%
## Maximal term length: 19
## Weighting          : term frequency (tf)</code></pre>
<p>This casting process allows for reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications. In Chapter <a href="topicmodeling.html#topicmodeling">6</a>, we’ll examine some examples where a tidy-text dataset has to be converted into a DocumentTermMatrix for processing.</p>
</div>
<div id="tidying-corpus-objects-with-metadata" class="section level2">
<h2><span class="header-section-number">5.3</span> Tidying corpus objects with metadata</h2>
<p>Some data structures are designed to store document collections <em>before</em> tokenization, often called a “corpus”. One common example is <code>Corpus</code> objects from the tm package. These store text alongside <strong>metadata</strong>, which may include an ID, date/time, title, or language for each document.</p>
<p>For example, the tm package comes with the <code>acq</code> corpus, containing 50 articles from the news service Reuters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;acq&quot;</span>)
acq</code></pre></div>
<pre><code>## &lt;&lt;VCorpus&gt;&gt;
## Metadata:  corpus specific: 0, document level (indexed): 0
## Content:  documents: 50</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># first document</span>
acq[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## &lt;&lt;PlainTextDocument&gt;&gt;
## Metadata:  15
## Content:  chars: 1287</code></pre>
<p>A corpus object is structured like a list, with each item containing both text and metadata (see the tm documentation for more on working with Corpus documents). This is a flexible storage method for documents, but doesn’t lend itself to processing with tidy tools.</p>
<p>We can thus use the <code>tidy()</code> method to construct a table with one row per document, including the metadata (such as <code>id</code> and <code>datetimestamp</code>) as columns alongside the <code>text</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acq_td &lt;-<span class="st"> </span><span class="kw">tidy</span>(acq)
acq_td</code></pre></div>
<pre><code>## # A tibble: 50 × 16
##                       author       datetimestamp description
##                        &lt;chr&gt;              &lt;dttm&gt;       &lt;chr&gt;
## 1                       &lt;NA&gt; 1987-02-26 15:18:06            
## 2                       &lt;NA&gt; 1987-02-26 15:19:15            
## 3                       &lt;NA&gt; 1987-02-26 15:49:56            
## 4  By Cal Mankowski, Reuters 1987-02-26 15:51:17            
## 5                       &lt;NA&gt; 1987-02-26 16:08:33            
## 6                       &lt;NA&gt; 1987-02-26 16:32:37            
## 7      By Patti Domm, Reuter 1987-02-26 16:43:13            
## 8                       &lt;NA&gt; 1987-02-26 16:59:25            
## 9                       &lt;NA&gt; 1987-02-26 17:01:28            
## 10                      &lt;NA&gt; 1987-02-26 17:08:27            
##                                             heading    id language            origin topics
##                                               &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;
## 1   COMPUTER TERMINAL SYSTEMS &lt;CPML&gt; COMPLETES SALE    10       en Reuters-21578 XML    YES
## 2    OHIO MATTRESS &lt;OMT&gt; MAY HAVE LOWER 1ST QTR NET    12       en Reuters-21578 XML    YES
## 3     MCLEAN&#39;S &lt;MII&gt; U.S. LINES SETS ASSET TRANSFER    44       en Reuters-21578 XML    YES
## 4    CHEMLAWN &lt;CHEM&gt; RISES ON HOPES FOR HIGHER BIDS    45       en Reuters-21578 XML    YES
## 5    &lt;COFAB INC&gt; BUYS GULFEX FOR UNDISCLOSED AMOUNT    68       en Reuters-21578 XML    YES
## 6          INVESTMENT FIRMS CUT CYCLOPS &lt;CYL&gt; STAKE    96       en Reuters-21578 XML    YES
## 7  AMERICAN EXPRESS &lt;AXP&gt; SEEN IN POSSIBLE SPINNOFF   110       en Reuters-21578 XML    YES
## 8   HONG KONG FIRM UPS WRATHER&lt;WCO&gt; STAKE TO 11 PCT   125       en Reuters-21578 XML    YES
## 9               LIEBERT CORP &lt;LIEB&gt; APPROVES MERGER   128       en Reuters-21578 XML    YES
## 10     GULF APPLIED TECHNOLOGIES &lt;GATS&gt; SELLS UNITS   134       en Reuters-21578 XML    YES
## # ... with 40 more rows, and 8 more variables: lewissplit &lt;chr&gt;, cgisplit &lt;chr&gt;, oldid &lt;chr&gt;,
## #   places &lt;list&gt;, people &lt;lgl&gt;, orgs &lt;lgl&gt;, exchanges &lt;lgl&gt;, text &lt;chr&gt;</code></pre>
<p>This can then be used with <code>unnest_tokens()</code> to, for example, find the most common words across the 50 Reuters articles, or the ones most specific to each article.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acq_tokens &lt;-<span class="st"> </span>acq_td %&gt;%
<span class="st">  </span><span class="kw">select</span>(-places) %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text) %&gt;%
<span class="st">  </span><span class="kw">anti_join</span>(stop_words, <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>)

<span class="co"># most common words</span>
acq_tokens %&gt;%
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## # A tibble: 1,566 × 2
##        word     n
##       &lt;chr&gt; &lt;int&gt;
## 1      dlrs   100
## 2       pct    70
## 3       mln    65
## 4   company    63
## 5    shares    52
## 6    reuter    50
## 7     stock    46
## 8     offer    34
## 9     share    34
## 10 american    28
## # ... with 1,556 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># tf-idf</span>
acq_tokens %&gt;%
<span class="st">  </span><span class="kw">count</span>(id, word) %&gt;%
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, id, n) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))</code></pre></div>
<pre><code>## Source: local data frame [2,853 x 6]
## Groups: id [50]
## 
##       id     word     n         tf      idf    tf_idf
##    &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1    186   groupe     2 0.13333333 3.912023 0.5216031
## 2    128  liebert     3 0.13043478 3.912023 0.5102639
## 3    474  esselte     5 0.10869565 3.912023 0.4252199
## 4    371  burdett     6 0.10344828 3.912023 0.4046920
## 5    442 hazleton     4 0.10256410 3.912023 0.4012331
## 6    199  circuit     5 0.10204082 3.912023 0.3991860
## 7    162 suffield     2 0.10000000 3.912023 0.3912023
## 8    498     west     3 0.10000000 3.912023 0.3912023
## 9    441      rmj     8 0.12121212 3.218876 0.3901668
## 10   467  nursery     3 0.09677419 3.912023 0.3785829
## # ... with 2,843 more rows</code></pre>
<div id="financial" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Example: mining financial articles</h3>
<p><code>Corpus</code> objects are a common output format for data ingesting packages, which means the <code>tidy()</code> function gives us access to a wide variety of text data. One example is <a href="https://cran.r-project.org/package=tm.plugin.webmining">tm.plugin.webmining</a>, which connects to online feeds to retrieve news articles based on a keyword. For instance, performing <code>WebCorpus(GoogleFinanceSource(&quot;NASDAQ:MSFT&quot;)))</code> allows us to retrieve the 20 most recent articles related to the Microsoft (MSFT) stock.</p>
<p>Here we’ll retrieve recent articles relevant to nine major technology stocks: Microsoft, Apple, Google, Amazon, Facebook, Twitter, IBM, Yahoo, and Netflix. These results were downloaded in January 2017, when this chapter was written, but you’ll certainly find different results if you ran it for yourself. (Note that this code takes several minutes to run).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tm.plugin.webmining)
<span class="kw">library</span>(purrr)

company &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Microsoft&quot;</span>, <span class="st">&quot;Apple&quot;</span>, <span class="st">&quot;Google&quot;</span>, <span class="st">&quot;Amazon&quot;</span>, <span class="st">&quot;Facebook&quot;</span>,
             <span class="st">&quot;Twitter&quot;</span>, <span class="st">&quot;IBM&quot;</span>, <span class="st">&quot;Yahoo&quot;</span>, <span class="st">&quot;Netflix&quot;</span>)
symbol &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;MSFT&quot;</span>, <span class="st">&quot;AAPL&quot;</span>, <span class="st">&quot;GOOG&quot;</span>, <span class="st">&quot;AMZN&quot;</span>, <span class="st">&quot;FB&quot;</span>, <span class="st">&quot;TWTR&quot;</span>, <span class="st">&quot;IBM&quot;</span>, <span class="st">&quot;YHOO&quot;</span>, <span class="st">&quot;NFLX&quot;</span>)

download_articles &lt;-<span class="st"> </span>function(symbol) {
  <span class="kw">WebCorpus</span>(<span class="kw">GoogleFinanceSource</span>(<span class="kw">paste0</span>(<span class="st">&quot;NASDAQ:&quot;</span>, symbol)))
}

stock_articles &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">company =</span> company,
                             <span class="dt">symbol =</span> symbol) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">corpus =</span> <span class="kw">map</span>(symbol, download_articles))</code></pre></div>
<p>This uses the <code>map()</code> function from the purrr package, which applies a function to each item in <code>symbol</code> to create a list, which we store in the <code>corpus</code> list column.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stock_articles</code></pre></div>
<pre><code>## # A tibble: 9 × 3
##     company symbol          corpus
##       &lt;chr&gt;  &lt;chr&gt;          &lt;list&gt;
## 1 Microsoft   MSFT &lt;S3: WebCorpus&gt;
## 2     Apple   AAPL &lt;S3: WebCorpus&gt;
## 3    Google   GOOG &lt;S3: WebCorpus&gt;
## 4    Amazon   AMZN &lt;S3: WebCorpus&gt;
## 5  Facebook     FB &lt;S3: WebCorpus&gt;
## 6   Twitter   TWTR &lt;S3: WebCorpus&gt;
## 7       IBM    IBM &lt;S3: WebCorpus&gt;
## 8     Yahoo   YHOO &lt;S3: WebCorpus&gt;
## 9   Netflix   NFLX &lt;S3: WebCorpus&gt;</code></pre>
<p>Each of the items in the <code>corpus</code> list column is a <code>WebCorpus</code> object, which is a special case of a corpus like <code>acq</code>. We can thus turn each into a data frame using the <code>tidy()</code> function, unnest it with tidyr’s <code>unnest()</code>, then tokenize the <code>text</code> column of the individual articles using <code>unnest_tokens()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stock_tokens &lt;-<span class="st"> </span>stock_articles %&gt;%
<span class="st">  </span><span class="kw">unnest</span>(<span class="kw">map</span>(corpus, tidy)) %&gt;%
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, text) %&gt;%
<span class="st">  </span><span class="kw">select</span>(company, datetimestamp, word, id, heading)

stock_tokens</code></pre></div>
<pre><code>## # A tibble: 105,057 × 5
##      company       datetimestamp        word                                            id
##        &lt;chr&gt;              &lt;dttm&gt;       &lt;chr&gt;                                         &lt;chr&gt;
## 1  Microsoft 2017-01-17 12:07:24   microsoft tag:finance.google.com,cluster:52779347599411
## 2  Microsoft 2017-01-17 12:07:24 corporation tag:finance.google.com,cluster:52779347599411
## 3  Microsoft 2017-01-17 12:07:24        data tag:finance.google.com,cluster:52779347599411
## 4  Microsoft 2017-01-17 12:07:24     privacy tag:finance.google.com,cluster:52779347599411
## 5  Microsoft 2017-01-17 12:07:24       could tag:finance.google.com,cluster:52779347599411
## 6  Microsoft 2017-01-17 12:07:24        send tag:finance.google.com,cluster:52779347599411
## 7  Microsoft 2017-01-17 12:07:24        msft tag:finance.google.com,cluster:52779347599411
## 8  Microsoft 2017-01-17 12:07:24       stock tag:finance.google.com,cluster:52779347599411
## 9  Microsoft 2017-01-17 12:07:24     soaring tag:finance.google.com,cluster:52779347599411
## 10 Microsoft 2017-01-17 12:07:24          by tag:finance.google.com,cluster:52779347599411
## # ... with 105,047 more rows, and 1 more variables: heading &lt;chr&gt;</code></pre>
<p>Here we see some of each article’s metadata alongside the words used. We could use tf-idf to determine which words were most specific to each stock symbol.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(stringr)

stock_tf_idf &lt;-<span class="st"> </span>stock_tokens %&gt;%
<span class="st">  </span><span class="kw">count</span>(company, word) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">str_detect</span>(word, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">d+&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, company, n) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(-tf_idf)</code></pre></div>
<p>The top terms for each are visualized in Figure <a href="dtm.html#fig:stocktfidf">5.4</a>. As we’d expect the company’s name and symbol are typically included, but so are several of their product offerings and executives, as well as companies they are making deals with (such as Disney with Netflix).</p>
<div class="figure"><span id="fig:stocktfidf"></span>
<img src="05-document-term-matrices_files/figure-html/stocktfidf-1.png" alt="The 8 words with the highest tf-idf in recent articles specific to each company" width="768" />
<p class="caption">
Figure 5.4: The 8 words with the highest tf-idf in recent articles specific to each company
</p>
</div>
<p>If we were interested in using recent news to analyze the market and make investment decisions, we’d likely want to use sentiment analysis to determine whether the news coverage was positive or negative. Before we run such an analysis, we should look at what words would contribute the most to positive and negative sentiments, as was shown in Chapter <a href="sentiment.html#most-positive-negative">2.4</a>. For example, we could examine this within the AFINN lexicon (Figure <a href="dtm.html#fig:stockafinn">5.5</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stock_tokens %&gt;%
<span class="st">  </span><span class="kw">anti_join</span>(stop_words, <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">count</span>(word, id, <span class="dt">sort =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;afinn&quot;</span>), <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">contribution =</span> <span class="kw">sum</span>(n *<span class="st"> </span>score)) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">12</span>, <span class="kw">abs</span>(contribution)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, contribution)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, contribution)) +
<span class="st">  </span><span class="kw">geom_col</span>() +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Frequency of word * AFINN score&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:stockafinn"></span>
<img src="05-document-term-matrices_files/figure-html/stockafinn-1.png" alt="The words with the largest contribution to sentiment scores in recent financial articles, according to the AFINN dictionary. The 'contribution' is the product of the word and the sentiment score." width="672" />
<p class="caption">
Figure 5.5: The words with the largest contribution to sentiment scores in recent financial articles, according to the AFINN dictionary. The ‘contribution’ is the product of the word and the sentiment score.
</p>
</div>
<p>In the context of these financial articles, there are a few big red flags here. The words “share” and “shares” are counted as positive verbs by the AFINN lexicon (“Alice will <strong>share</strong> her cake with Bob”), but they’re actually neutral nouns (“The stock price is $12 per <strong>share</strong>”) that could just as easily be in a positive sentence as a negative one. The word “fool” is even more deceptive: it refers to Motley Fool, a financial services company. In short, we can see that the AFINN sentiment lexicon is entirely unsuited to the context of financial data (as are the NRC and Bing).</p>
<p>Instead, we introduce another sentiment lexicon: the Loughran and McDonald dictionary of financial sentiment terms <span class="citation">(LOUGHRAN and MCDONALD <a href="#ref-loughran2011liability">2011</a>)</span>. This dictionary was developed based on analyses of financial reports, and intentionally avoids words like “share” and “fool”, as well as subtler terms like “liability” and “risk” that may not have a negative meaning in a financial context.</p>
<p>The Loughran data divides words into six sentiments: “positive”, “negative”, “litigious”, “uncertain”, “constraining”, and “superfluous”. We could start by examining the most common words belonging to each sentiment within this text dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stock_tokens %&gt;%
<span class="st">  </span><span class="kw">count</span>(word) %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;loughran&quot;</span>), <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(sentiment) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">5</span>, n) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, n)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n)) +
<span class="st">  </span><span class="kw">geom_col</span>() +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>sentiment, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency of this word in the recent financial articles&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:stockloughransentiments"></span>
<img src="05-document-term-matrices_files/figure-html/stockloughransentiments-1.png" alt="The most common words in the financial news articles associated with each of the six sentiments in the Loughran and McDonald lexicon" width="672" />
<p class="caption">
Figure 5.6: The most common words in the financial news articles associated with each of the six sentiments in the Loughran and McDonald lexicon
</p>
</div>
<p>These assignments (Figure <a href="dtm.html#fig:stockloughransentiments">5.6</a>) of words to sentiments look more reasonable: common positive words include “strong” and “better”, but not “shares” or “growth”, while negative words include “volatility” but not “fool”. The other sentiments look reasonable as well: the most common “uncertainty” terms include “could” and “may”.</p>
<p>Now that we know we can trust the dictionary to approximate the articles’ sentiments, we can use our typical methods for counting the number of uses of each sentiment-associated word in each corpus.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stock_sentiment_count &lt;-<span class="st"> </span>stock_tokens %&gt;%
<span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;loughran&quot;</span>), <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">count</span>(sentiment, company) %&gt;%
<span class="st">  </span><span class="kw">spread</span>(sentiment, n, <span class="dt">fill =</span> <span class="dv">0</span>)

stock_sentiment_count</code></pre></div>
<pre><code>## # A tibble: 9 × 7
##     company constraining litigious negative positive superfluous uncertainty
## *     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1    Amazon            7         8       84      144           3          70
## 2     Apple            9        11      161      156           2         132
## 3  Facebook            4        32      128      150           4          81
## 4    Google            7         8       60      103           0          58
## 5       IBM            8        22      147      148           0         104
## 6 Microsoft            6        19       92      129           3         116
## 7   Netflix            4         7      111      162           0         106
## 8   Twitter            4        12      157       79           1          75
## 9     Yahoo            3        28      130       74           0          71</code></pre>
<p>It might be interesting to examine which company has the most news with “litigious” or “uncertain” terms. But the simplest measure, much as it was for most analysis in Chapter <a href="sentiment.html#sentiment">2</a>, is to see whether the news is more positive or negative. As a general quantitative measure of sentiment, we’ll use “(positive - negative) / (positive + negative)” (Figure <a href="dtm.html#fig:stockpositivity">5.7</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stock_sentiment_count %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">score =</span> (positive -<span class="st"> </span>negative) /<span class="st"> </span>(positive +<span class="st"> </span>negative)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">company =</span> <span class="kw">reorder</span>(company, score)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(company, score, <span class="dt">fill =</span> score &gt;<span class="st"> </span><span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Company&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Positivity score among 20 recent news articles&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:stockpositivity"></span>
<img src="05-document-term-matrices_files/figure-html/stockpositivity-1.png" alt="&quot;Positivity&quot; of the news coverage around each stock in January 2017, calculated as (positive - negative) / (positive + negative), based on uses of positive and negative words in 20 recent news articles about each company" width="672" />
<p class="caption">
Figure 5.7: “Positivity” of the news coverage around each stock in January 2017, calculated as (positive - negative) / (positive + negative), based on uses of positive and negative words in 20 recent news articles about each company
</p>
</div>
<p>Based on this analysis, we’d say that in January 2017 most of the coverage of Yahoo and Twitter was strongly negative, while coverage of Google and Amazon was the most positive. A glance at current financial headlines suggest that it’s on the right track. If you were interested in further analysis, you could use one of R’s many quantitative finance packages to compare these articles to recent stock prices and other metrics.</p>
</div>
</div>
<div id="summary-4" class="section level2">
<h2><span class="header-section-number">5.4</span> Summary</h2>
<p>TODO</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Friedman:2008:EPL:1378240">
<p>Abelson, Hal. 2008. “Foreword.” In <em>Essentials of Programming Languages, 3rd Edition</em>, 3rd ed. The MIT Press.</p>
</div>
<div id="ref-R-broom">
<p>Robinson, David, Matthieu Gomez, Boris Demeshev, Dieter Menne, Benjamin Nutter, Luke Johnston, Ben Bolker, Francois Briatte, and Hadley Wickham. 2015. <em>Broom: Convert Statistical Analysis Objects into Tidy Data Frames</em>. <a href="https://CRAN.R-project.org/package=broom" class="uri">https://CRAN.R-project.org/package=broom</a>.</p>
</div>
<div id="ref-R-reshape2">
<p>Wickham, Hadley. 2007. “Reshaping Data with the reshape Package.” <em>Journal of Statistical Software</em> 21 (12): 1–20. <a href="http://www.jstatsoft.org/v21/i12/" class="uri">http://www.jstatsoft.org/v21/i12/</a>.</p>
</div>
<div id="ref-R-quanteda">
<p>Benoit, Kenneth, and Paul Nulty. 2016. <em>Quanteda: Quantitative Analysis of Textual Data</em>. <a href="https://CRAN.R-project.org/package=quanteda" class="uri">https://CRAN.R-project.org/package=quanteda</a>.</p>
</div>
<div id="ref-loughran2011liability">
<p>LOUGHRAN, TIM, and BILL MCDONALD. 2011. “When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks.” <em>The Journal of Finance</em> 66 (1). Blackwell Publishing Inc: 35–65. doi:<a href="https://doi.org/10.1111/j.1540-6261.2010.01625.x">10.1111/j.1540-6261.2010.01625.x</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ngrams.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="topicmodeling.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dgrtwo/tidy-text-mining/edit/master/05-document-term-matrices.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
