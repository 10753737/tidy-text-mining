<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Tidy Text Mining with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Tidy Text Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/tidycover.png" />
  <meta property="og:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="github-repo" content="dgrtwo/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Tidy Text Mining with R" />
  
  <meta name="twitter:description" content="A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools" />
  <meta name="twitter:image" content="images/tidycover.png" />

<meta name="author" content="Julia Silge and David Robinson">


<meta name="date" content="2016-12-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="twitter.html">
<link rel="next" href="usenet.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68765210-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Tidy Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Tidy Text Mining with R</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-tidy-text"><i class="fa fa-check"></i><b>1.1</b> What is tidy text?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#about-this-book"><i class="fa fa-check"></i><b>1.2</b> About this book</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#outline"><i class="fa fa-check"></i><b>1.3</b> Outline</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#topics-this-book-does-not-cover"><i class="fa fa-check"></i><b>1.4</b> Topics this book does not cover</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#acknowledgements"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tidytext.html"><a href="tidytext.html"><i class="fa fa-check"></i><b>2</b> The tidy text format</a><ul>
<li class="chapter" data-level="2.1" data-path="tidytext.html"><a href="tidytext.html#the-unnest_tokens-function"><i class="fa fa-check"></i><b>2.1</b> The <code>unnest_tokens</code> function</a></li>
<li class="chapter" data-level="2.2" data-path="tidytext.html"><a href="tidytext.html#tidying-the-works-of-jane-austen"><i class="fa fa-check"></i><b>2.2</b> Tidying the works of Jane Austen</a></li>
<li class="chapter" data-level="2.3" data-path="tidytext.html"><a href="tidytext.html#the-gutenbergr-package"><i class="fa fa-check"></i><b>2.3</b> The gutenbergr package</a></li>
<li class="chapter" data-level="2.4" data-path="tidytext.html"><a href="tidytext.html#word-frequencies"><i class="fa fa-check"></i><b>2.4</b> Word frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sentiment.html"><a href="sentiment.html"><i class="fa fa-check"></i><b>3</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="3.1" data-path="sentiment.html"><a href="sentiment.html#the-sentiments-dataset"><i class="fa fa-check"></i><b>3.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="3.2" data-path="sentiment.html"><a href="sentiment.html#sentiment-analysis-with-inner-join"><i class="fa fa-check"></i><b>3.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="3.3" data-path="sentiment.html"><a href="sentiment.html#comparing-the-three-sentiment-dictionaries"><i class="fa fa-check"></i><b>3.3</b> Comparing the three sentiment dictionaries</a></li>
<li class="chapter" data-level="3.4" data-path="sentiment.html"><a href="sentiment.html#most-common-positive-and-negative-words"><i class="fa fa-check"></i><b>3.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="3.5" data-path="sentiment.html"><a href="sentiment.html#wordclouds"><i class="fa fa-check"></i><b>3.5</b> Wordclouds</a></li>
<li class="chapter" data-level="3.6" data-path="sentiment.html"><a href="sentiment.html#looking-at-units-beyond-just-words"><i class="fa fa-check"></i><b>3.6</b> Looking at units beyond just words</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tfidf.html"><a href="tfidf.html"><i class="fa fa-check"></i><b>4</b> Analyzing word and document frequency: tf-idf</a><ul>
<li class="chapter" data-level="4.1" data-path="tfidf.html"><a href="tfidf.html#term-frequency-and-inverse-document-frequency"><i class="fa fa-check"></i><b>4.1</b> Term frequency and inverse document frequency</a></li>
<li class="chapter" data-level="4.2" data-path="tfidf.html"><a href="tfidf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>4.2</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="4.3" data-path="tfidf.html"><a href="tfidf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>4.3</b> The <code>bind_tf_idf</code> function</a></li>
<li class="chapter" data-level="4.4" data-path="tfidf.html"><a href="tfidf.html#a-corpus-of-physics-texts"><i class="fa fa-check"></i><b>4.4</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ngrams.html"><a href="ngrams.html"><i class="fa fa-check"></i><b>5</b> Working with combinations of words using n-grams and widyr</a><ul>
<li class="chapter" data-level="5.1" data-path="ngrams.html"><a href="ngrams.html#tokenizing-by-n-gram"><i class="fa fa-check"></i><b>5.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="5.1.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-filtering-n-grams"><i class="fa fa-check"></i><b>5.1.1</b> Counting and filtering n-grams</a></li>
<li class="chapter" data-level="5.1.2" data-path="ngrams.html"><a href="ngrams.html#analyzing-bigrams"><i class="fa fa-check"></i><b>5.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="5.1.3" data-path="ngrams.html"><a href="ngrams.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>5.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="5.1.4" data-path="ngrams.html"><a href="ngrams.html#visualizing-a-network-of-bigrams-with-igraph"><i class="fa fa-check"></i><b>5.1.4</b> Visualizing a network of bigrams with igraph</a></li>
<li class="chapter" data-level="5.1.5" data-path="ngrams.html"><a href="ngrams.html#visualizing-bigrams-in-other-texts"><i class="fa fa-check"></i><b>5.1.5</b> Visualizing bigrams in other texts</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-pairs-of-words-with-the-widyr-package"><i class="fa fa-check"></i><b>5.2</b> Counting and correlating pairs of words with the widyr package</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ngrams.html"><a href="ngrams.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>5.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="5.2.2" data-path="ngrams.html"><a href="ngrams.html#pairwise-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="dtm.html"><a href="dtm.html"><i class="fa fa-check"></i><b>6</b> Tidying and casting document-term matrices</a><ul>
<li class="chapter" data-level="6.1" data-path="dtm.html"><a href="dtm.html#tidying-a-document-term-matrix"><i class="fa fa-check"></i><b>6.1</b> Tidying a document-term matrix</a></li>
<li class="chapter" data-level="6.2" data-path="dtm.html"><a href="dtm.html#casting-tidy-text-data-into-a-documenttermmatrix"><i class="fa fa-check"></i><b>6.2</b> Casting tidy text data into a DocumentTermMatrix</a></li>
<li class="chapter" data-level="6.3" data-path="dtm.html"><a href="dtm.html#tidying-corpus-objects-with-metadata"><i class="fa fa-check"></i><b>6.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="topicmodeling.html"><a href="topicmodeling.html"><i class="fa fa-check"></i><b>7</b> Topic modeling</a><ul>
<li class="chapter" data-level="7.1" data-path="topicmodeling.html"><a href="topicmodeling.html#the-great-library-heist"><i class="fa fa-check"></i><b>7.1</b> The great library heist</a></li>
<li class="chapter" data-level="7.2" data-path="topicmodeling.html"><a href="topicmodeling.html#latent-dirichlet-allocation-with-the-topicmodels-package"><i class="fa fa-check"></i><b>7.2</b> Latent Dirichlet allocation with the topicmodels package</a></li>
<li class="chapter" data-level="7.3" data-path="topicmodeling.html"><a href="topicmodeling.html#per-document-classification"><i class="fa fa-check"></i><b>7.3</b> Per-document classification</a></li>
<li class="chapter" data-level="7.4" data-path="topicmodeling.html"><a href="topicmodeling.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>7.4</b> By word assignments: <code>augment</code></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="twitter.html"><a href="twitter.html"><i class="fa fa-check"></i><b>8</b> Case study: comparing Twitter archives</a><ul>
<li class="chapter" data-level="8.1" data-path="twitter.html"><a href="twitter.html#getting-the-data-and-distribution-of-tweets"><i class="fa fa-check"></i><b>8.1</b> Getting the data and distribution of tweets</a></li>
<li class="chapter" data-level="8.2" data-path="twitter.html"><a href="twitter.html#word-frequencies-1"><i class="fa fa-check"></i><b>8.2</b> Word frequencies</a></li>
<li class="chapter" data-level="8.3" data-path="twitter.html"><a href="twitter.html#comparing-word-usage"><i class="fa fa-check"></i><b>8.3</b> Comparing word usage</a></li>
<li class="chapter" data-level="8.4" data-path="twitter.html"><a href="twitter.html#sentiment-analysis"><i class="fa fa-check"></i><b>8.4</b> Sentiment analysis</a></li>
<li class="chapter" data-level="8.5" data-path="twitter.html"><a href="twitter.html#words-that-contribute-to-sentiment-in-tweets"><i class="fa fa-check"></i><b>8.5</b> Words that contribute to sentiment in tweets</a></li>
<li class="chapter" data-level="8.6" data-path="twitter.html"><a href="twitter.html#favorites-and-retweets"><i class="fa fa-check"></i><b>8.6</b> Favorites and retweets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nasa.html"><a href="nasa.html"><i class="fa fa-check"></i><b>9</b> Case study: mining NASA metadata</a><ul>
<li class="chapter" data-level="9.1" data-path="nasa.html"><a href="nasa.html#getting-the-metadata"><i class="fa fa-check"></i><b>9.1</b> Getting the metadata</a></li>
<li class="chapter" data-level="9.2" data-path="nasa.html"><a href="nasa.html#wrangling-and-tidying-the-data"><i class="fa fa-check"></i><b>9.2</b> Wrangling and tidying the data</a></li>
<li class="chapter" data-level="9.3" data-path="nasa.html"><a href="nasa.html#some-initial-simple-exploration"><i class="fa fa-check"></i><b>9.3</b> Some initial simple exploration</a></li>
<li class="chapter" data-level="9.4" data-path="nasa.html"><a href="nasa.html#word-co-ocurrences-and-correlations"><i class="fa fa-check"></i><b>9.4</b> Word co-ocurrences and correlations</a></li>
<li class="chapter" data-level="9.5" data-path="nasa.html"><a href="nasa.html#calculating-tf-idf-for-the-description-fields"><i class="fa fa-check"></i><b>9.5</b> Calculating tf-idf for the description fields</a></li>
<li class="chapter" data-level="9.6" data-path="nasa.html"><a href="nasa.html#topic-modeling"><i class="fa fa-check"></i><b>9.6</b> Topic modeling</a></li>
<li class="chapter" data-level="9.7" data-path="nasa.html"><a href="nasa.html#interpreting-the-topic-model"><i class="fa fa-check"></i><b>9.7</b> Interpreting the topic model</a></li>
<li class="chapter" data-level="9.8" data-path="nasa.html"><a href="nasa.html#connecting-topic-modeling-with-keywords"><i class="fa fa-check"></i><b>9.8</b> Connecting topic modeling with keywords</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="usenet.html"><a href="usenet.html"><i class="fa fa-check"></i><b>10</b> Case study: analyzing usenet text</a><ul>
<li class="chapter" data-level="10.1" data-path="usenet.html"><a href="usenet.html#wrangling-the-data"><i class="fa fa-check"></i><b>10.1</b> Wrangling the data</a></li>
<li class="chapter" data-level="10.2" data-path="usenet.html"><a href="usenet.html#term-frequency-and-inverse-document-frequency-tf-idf"><i class="fa fa-check"></i><b>10.2</b> Term frequency and inverse document frequency: tf-idf</a></li>
<li class="chapter" data-level="10.3" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-1"><i class="fa fa-check"></i><b>10.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="10.4" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-word"><i class="fa fa-check"></i><b>10.4</b> Sentiment analysis by word</a></li>
<li class="chapter" data-level="10.5" data-path="usenet.html"><a href="usenet.html#sentiment-analysis-by-message"><i class="fa fa-check"></i><b>10.5</b> Sentiment analysis by message</a></li>
<li class="chapter" data-level="10.6" data-path="usenet.html"><a href="usenet.html#n-grams"><i class="fa fa-check"></i><b>10.6</b> N-grams</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Tidy Text Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nasa" class="section level1">
<h1><span class="header-section-number">9</span> Case study: mining NASA metadata</h1>
<p>There are over 32,000 datasets hosted and/or maintained by <a href="https://www.nasa.gov/">NASA</a>; these datasets cover topics from Earth science to aerospace engineering to management of NASA itself. We can use the metadata for these datasets to understand the connections between them.</p>
<p>What is metadata? Metadata is a term that refers to data that gives information about other data; in this case, the metadata informs users about what is in these numerous NASA datasets but does include the content of the datasets themselves. The metadata includes information like the title of the dataset, a description field, what organization(s) within NASA is responsible for the dataset, keywords for the dataset that have been assigned by a human being, and so forth. NASA places a high priority on making its data open and accessible, even requiring all NASA-funded research to be <a href="https://www.nasa.gov/press-release/nasa-unveils-new-public-web-portal-for-research-results">openly accessible online</a>. The metadata for all its datasets is <a href="https://data.nasa.gov/data.json">publicly available online in JSON format</a>.</p>
<p>In this chapter, we will treat the NASA metadata as a text dataset and show how to implement several tidy text approaches with this real-life text. We will use word co-occurrences and correlations, tf-idf, and topic modeling to explore the connections between the datasets. Can we find datasets that are related to each other? Can we find clusters of similar datasets? Since we have several text fields in the NASA metadata, most importantly the title, description, and keyword fields, we can explore the connections between the fields to better understand the complex world of data at NASA. This type of approach can be extended to many domains, so let’s take a look at this metadata and get started.</p>
<div id="getting-the-metadata" class="section level2">
<h2><span class="header-section-number">9.1</span> Getting the metadata</h2>
<p>First, let’s download the JSON file and take a look at the names of what is stored in the metadata.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(jsonlite)
metadata &lt;-<span class="st"> </span><span class="kw">fromJSON</span>(<span class="st">&quot;https://data.nasa.gov/data.json&quot;</span>)
<span class="kw">names</span>(metadata$dataset)</code></pre></div>
<pre><code>##  [1] &quot;_id&quot;                &quot;@type&quot;              &quot;accessLevel&quot;        &quot;accrualPeriodicity&quot;
##  [5] &quot;bureauCode&quot;         &quot;contactPoint&quot;       &quot;description&quot;        &quot;distribution&quot;      
##  [9] &quot;identifier&quot;         &quot;issued&quot;             &quot;keyword&quot;            &quot;landingPage&quot;       
## [13] &quot;language&quot;           &quot;modified&quot;           &quot;programCode&quot;        &quot;publisher&quot;         
## [17] &quot;spatial&quot;            &quot;temporal&quot;           &quot;theme&quot;              &quot;title&quot;             
## [21] &quot;license&quot;            &quot;isPartOf&quot;           &quot;references&quot;         &quot;rights&quot;            
## [25] &quot;describedBy&quot;</code></pre>
<p>We see here that we could extract information from who publishes each dataset to what license they are released under. What type of data is available here?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sapply</span>(metadata$dataset, class)</code></pre></div>
<pre><code>##                _id              @type        accessLevel accrualPeriodicity         bureauCode 
##       &quot;data.frame&quot;        &quot;character&quot;        &quot;character&quot;        &quot;character&quot;             &quot;list&quot; 
##       contactPoint        description       distribution         identifier             issued 
##       &quot;data.frame&quot;        &quot;character&quot;             &quot;list&quot;        &quot;character&quot;        &quot;character&quot; 
##            keyword        landingPage           language           modified        programCode 
##             &quot;list&quot;        &quot;character&quot;             &quot;list&quot;        &quot;character&quot;             &quot;list&quot; 
##          publisher            spatial           temporal              theme              title 
##       &quot;data.frame&quot;        &quot;character&quot;        &quot;character&quot;             &quot;list&quot;        &quot;character&quot; 
##            license           isPartOf         references             rights        describedBy 
##        &quot;character&quot;        &quot;character&quot;             &quot;list&quot;        &quot;character&quot;        &quot;character&quot;</code></pre>
<p>It seems likely that the title, description, and keywords for each dataset may be most fruitful for drawing connections between datasets. Let’s check them out.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(metadata$dataset$title)</code></pre></div>
<pre><code>## [1] &quot;character&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(metadata$dataset$description)</code></pre></div>
<pre><code>## [1] &quot;character&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(metadata$dataset$keyword)</code></pre></div>
<pre><code>## [1] &quot;list&quot;</code></pre>
<p>The title and description fields are stored as character vectors, but the keywords are stored as a list of character vectors.</p>
</div>
<div id="wrangling-and-tidying-the-data" class="section level2">
<h2><span class="header-section-number">9.2</span> Wrangling and tidying the data</h2>
<p>Let’s set up separate tidy data frames for title, description, and keyword, keeping the dataset ids for each so that we can connect them later in the analysis if necessary.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)

nasa_title &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">id =</span> metadata$dataset$<span class="st">`</span><span class="dt">_id</span><span class="st">`</span>$<span class="st">`</span><span class="dt">$oid</span><span class="st">`</span>, 
                         <span class="dt">title =</span> metadata$dataset$title)
nasa_title</code></pre></div>
<pre><code>## # A tibble: 32,089 × 2
##                          id                                               title
##                       &lt;chr&gt;                                               &lt;chr&gt;
## 1  55942a57c63a7fe59b495a77             15 Minute Stream Flow Data: USGS (FIFE)
## 2  55942a57c63a7fe59b495a78             15 Minute Stream Flow Data: USGS (FIFE)
## 3  55942a58c63a7fe59b495a79             15 Minute Stream Flow Data: USGS (FIFE)
## 4  55942a58c63a7fe59b495a7a 2000 Pilot Environmental Sustainability Index (ESI)
## 5  55942a58c63a7fe59b495a7b 2000 Pilot Environmental Sustainability Index (ESI)
## 6  55942a58c63a7fe59b495a7c 2000 Pilot Environmental Sustainability Index (ESI)
## 7  55942a58c63a7fe59b495a7d       2001 Environmental Sustainability Index (ESI)
## 8  55942a58c63a7fe59b495a7e       2001 Environmental Sustainability Index (ESI)
## 9  55942a58c63a7fe59b495a7f       2001 Environmental Sustainability Index (ESI)
## 10 55942a58c63a7fe59b495a80       2001 Environmental Sustainability Index (ESI)
## # ... with 32,079 more rows</code></pre>
<p>These are just a few example titles from the datasets we will be exploring. Notice that we have the NASA-assigned ids here, and also that there are duplicate titles on separate datasets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_desc &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">id =</span> metadata$dataset$<span class="st">`</span><span class="dt">_id</span><span class="st">`</span>$<span class="st">`</span><span class="dt">$oid</span><span class="st">`</span>, 
                        <span class="dt">desc =</span> metadata$dataset$description)

nasa_desc %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(desc) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</code></pre></div>
<pre><code>## # A tibble: 5 × 1
##                                                                                                    desc
##                                                                                                   &lt;chr&gt;
## 1  This dataset consists of VHS tapes which record the forward and nadir views from the NASA DC-8 aircr
## 2 MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the\nTerra (EOS A
## 3  The Coastal Zone Color Scanner Experiment (CZCS) was the first instrument devoted to the measurement
## 4 The SeaWiFS instrument was launched by Orbital Sciences Corporation on the OrbView-2\n(a.k.a. SeaStar
## 5  Lynntech proposes to develop and demonstrate the ability of a compact, light-weight, and automated u</code></pre>
<p>Here we see the first part of several selected description fields from the metadata.</p>
<p>Now we can build the tidy data frame for the keywords. For this one, we need to use <code>unnest</code> from tidyr, because they are in a list-column.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyr)

nasa_keyword &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">id =</span> metadata$dataset$<span class="st">`</span><span class="dt">_id</span><span class="st">`</span>$<span class="st">`</span><span class="dt">$oid</span><span class="st">`</span>, 
                           <span class="dt">keyword =</span> metadata$dataset$keyword) %&gt;%
<span class="st">  </span><span class="kw">unnest</span>(keyword)

nasa_keyword</code></pre></div>
<pre><code>## # A tibble: 126,814 × 2
##                          id       keyword
##                       &lt;chr&gt;         &lt;chr&gt;
## 1  55942a57c63a7fe59b495a77 EARTH SCIENCE
## 2  55942a57c63a7fe59b495a77   HYDROSPHERE
## 3  55942a57c63a7fe59b495a77 SURFACE WATER
## 4  55942a57c63a7fe59b495a78 EARTH SCIENCE
## 5  55942a57c63a7fe59b495a78   HYDROSPHERE
## 6  55942a57c63a7fe59b495a78 SURFACE WATER
## 7  55942a58c63a7fe59b495a79 EARTH SCIENCE
## 8  55942a58c63a7fe59b495a79   HYDROSPHERE
## 9  55942a58c63a7fe59b495a79 SURFACE WATER
## 10 55942a58c63a7fe59b495a7a EARTH SCIENCE
## # ... with 126,804 more rows</code></pre>
<p>This is a tidy data frame because we have one row for each keyword; this means we will have multiple rows for each dataset because a dataset can have more than one keyword.</p>
<p>Now it is time to use tidytext’s <code>unnest_tokens</code> for the title and description fields so we can do the text analysis. Let’s also remove common English words from the titles and descriptions. We will not remove stop words from the keywords, because those are short, human-assigned keywords like <strong>RADIATION</strong> or <strong>CLIMATE INDICATORS</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidytext)

nasa_title &lt;-<span class="st"> </span>nasa_title %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, title) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">anti_join</span>(stop_words)

nasa_desc &lt;-<span class="st"> </span>nasa_desc %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, desc) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">anti_join</span>(stop_words)</code></pre></div>
<p>These are now in the tidy text format that we have been working with throughout this book, with one token (word, in this case) per row; let’s take a look before we move on in our analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_title</code></pre></div>
<pre><code>## # A tibble: 210,914 × 2
##                          id         word
##                       &lt;chr&gt;        &lt;chr&gt;
## 1  56d07ee5a759fdadc44e5923       marble
## 2  56d07ee5a759fdadc44e5923         epic
## 3  56d07c16a759fdadc44e5922       fitara
## 4  56d07c16a759fdadc44e5922         ocio
## 5  56cf5b00a759fdadc44e5849 implementing
## 6  56cf5b00a759fdadc44e5846     receding
## 7  56cf5b00a759fdadc44e5846    recursive
## 8  56cf5b00a759fdadc44e5840   complaints
## 9  56cf5b00a759fdadc44e583b        score
## 10 56cf5b00a759fdadc44e583a          fix
## # ... with 210,904 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_desc</code></pre></div>
<pre><code>## # A tibble: 2,677,811 × 2
##                          id            word
##                       &lt;chr&gt;           &lt;chr&gt;
## 1  56d07c16a759fdadc44e5922          fitara
## 2  56d07c16a759fdadc44e5922            ocio
## 3  56cf5b00a759fdadc44e584a   degradation&#39;s
## 4  56cf5b00a759fdadc44e5847         dchwp1s
## 5  56cf5b00a759fdadc44e5847        dchwp1sp
## 6  56cf5b00a759fdadc44e5847          dchwdp
## 7  56cf5b00a759fdadc44e5847         dchwsnf
## 8  56cf5b00a759fdadc44e5847         dchwssf
## 9  56cf5b00a759fdadc44e5847        bursting
## 10 56cf5b00a759fdadc44e5847 consequentially
## # ... with 2,677,801 more rows</code></pre>
</div>
<div id="some-initial-simple-exploration" class="section level2">
<h2><span class="header-section-number">9.3</span> Some initial simple exploration</h2>
<p>What are the most common words in the NASA dataset titles? We can use <code>count</code> from dplyr to check this out.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_title %&gt;%
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## # A tibble: 11,614 × 2
##       word     n
##      &lt;chr&gt; &lt;int&gt;
## 1  project  7735
## 2     data  3354
## 3        1  2841
## 4    level  2400
## 5   global  1809
## 6       v1  1478
## 7    daily  1397
## 8        3  1364
## 9     aura  1363
## 10      l2  1311
## # ... with 11,604 more rows</code></pre>
<p>What about the descriptions?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_desc %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## # A tibble: 35,940 × 2
##          word     n
##         &lt;chr&gt; &lt;int&gt;
## 1        data 68871
## 2       modis 24420
## 3      global 23028
## 4           2 16599
## 5           1 15770
## 6      system 15480
## 7     product 14780
## 8        aqua 14738
## 9       earth 14373
## 10 resolution 13879
## # ... with 35,930 more rows</code></pre>
<p>Words like “data” and “global” are used very often in NASA titles and descriptions. We may want to remove digits and some “words” like “v1” from these data frames for many types of analyses; they are not too meaningful for most audiences. We can do this by making a list of custom stop words and using <code>anti_join</code> to remove them from the data frame, just like we removed the default stop words that are in the tidytext package. This approach can be used in many instances and is a great tool to bear in mind.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_stopwords &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">word =</span> <span class="kw">c</span>(<span class="kw">as.character</span>(<span class="dv">1</span>:<span class="dv">10</span>), 
                                    <span class="st">&quot;v1&quot;</span>, <span class="st">&quot;v03&quot;</span>, <span class="st">&quot;l2&quot;</span>, <span class="st">&quot;l3&quot;</span>, <span class="st">&quot;l4&quot;</span>, <span class="st">&quot;v5.2.0&quot;</span>, 
                                    <span class="st">&quot;v003&quot;</span>, <span class="st">&quot;v004&quot;</span>, <span class="st">&quot;v005&quot;</span>, <span class="st">&quot;v006&quot;</span>, <span class="st">&quot;v7&quot;</span>))
nasa_title &lt;-<span class="st"> </span>nasa_title %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">anti_join</span>(my_stopwords)
nasa_desc &lt;-<span class="st"> </span>nasa_desc %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">anti_join</span>(my_stopwords)</code></pre></div>
<p>What are the most common keywords?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_keyword %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(keyword) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(<span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## # A tibble: 1,774 × 2
##                    keyword     n
##                      &lt;chr&gt; &lt;int&gt;
## 1            EARTH SCIENCE 14362
## 2                  Project  7452
## 3               ATMOSPHERE  7321
## 4              Ocean Color  7268
## 5             Ocean Optics  7268
## 6                   Oceans  7268
## 7                completed  6452
## 8  ATMOSPHERIC WATER VAPOR  3142
## 9                   OCEANS  2765
## 10            LAND SURFACE  2720
## # ... with 1,764 more rows</code></pre>
<p>Many NASA datasets have <strong>Project completed</strong> as a set of keywords. We likely want to change all of the keywords to either lower or upper case to get rid of duplicates like <strong>OCEANS</strong> and <strong>Oceans</strong>. Let’s do that here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nasa_keyword &lt;-<span class="st"> </span>nasa_keyword %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">keyword =</span> <span class="kw">toupper</span>(keyword))</code></pre></div>
</div>
<div id="word-co-ocurrences-and-correlations" class="section level2">
<h2><span class="header-section-number">9.4</span> Word co-ocurrences and correlations</h2>
<p>As a next step, let’s examine which words commonly occur together in the titles and descriptions of NASA datasets, as described in <a href="ngrams.html#ngrams">Chapter 5</a>. We can then examine a word network in titles/descriptions; this may help us decide, for example, how many topics to look at in topic modeling. We can use <code>pairwise_count</code> from the widyr package to count how many times each pair of words occurs together in a title or description field.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(widyr)

title_words &lt;-<span class="st"> </span>nasa_title %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">pairwise_count</span>(word, id, <span class="dt">sort =</span> <span class="ot">TRUE</span>, <span class="dt">upper =</span> <span class="ot">FALSE</span>)

title_words</code></pre></div>
<pre><code>## # A tibble: 156,689 × 3
##     item1   item2     n
##     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;
## 1  system project   796
## 2     lba     eco   683
## 3    airs    aqua   641
## 4   level    aqua   623
## 5   level    airs   612
## 6    aura     omi   607
## 7  global    grid   597
## 8  global   daily   574
## 9    data  boreas   551
## 10 ground     gpm   550
## # ... with 156,679 more rows</code></pre>
<p>These are the pairs of words that occur together most often in title fields. Some of these words are obviously acronyms used within NASA, and we see how often words like “project” and “system” are used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">desc_words &lt;-<span class="st"> </span>nasa_desc %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">pairwise_count</span>(word, id, <span class="dt">sort =</span> <span class="ot">TRUE</span>, <span class="dt">upper =</span> <span class="ot">FALSE</span>)

desc_words</code></pre></div>
<pre><code>## # A tibble: 10,889,084 × 3
##         item1      item2     n
##         &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;
## 1        data     global  9864
## 2        data resolution  9302
## 3  instrument resolution  8189
## 4        data    surface  8180
## 5      global resolution  8139
## 6        data instrument  7994
## 7        data     system  7870
## 8  resolution      bands  7584
## 9        data      earth  7576
## 10      orbit resolution  7462
## # ... with 10,889,074 more rows</code></pre>
<p>These are the pairs of words that occur together most often in descripton fields. “Data” is a very common word in description fields; there is no shortage of data in the datasets at NASA!</p>
<p>Let’s plot networks of these co-occurring words so we can see these relationships better. We will again use the ggraph package for visualizing our networks.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(igraph)
<span class="kw">library</span>(ggraph)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
title_words %&gt;%
<span class="st">  </span><span class="kw">filter</span>(n &gt;=<span class="st"> </span><span class="dv">250</span>) %&gt;%
<span class="st">  </span><span class="kw">graph_from_data_frame</span>() %&gt;%
<span class="st">  </span><span class="kw">ggraph</span>(<span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) +
<span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">edge_alpha =</span> n, <span class="dt">edge_width =</span> n)) +
<span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">color =</span> <span class="st">&quot;darkslategray4&quot;</span>, <span class="dt">size =</span> <span class="dv">5</span>) +
<span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">repel =</span> <span class="ot">TRUE</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Word Network in NASA Dataset Titles&quot;</span>) +
<span class="st">  </span><span class="kw">theme_void</span>()</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_title-1.png" width="864" /></p>
<p>We see some clear clustering in this network of title words; words in NASA dataset titles are largely organized into several families of words that tend to go together.</p>
<p>What about the words from the description fields?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
desc_words %&gt;%
<span class="st">  </span><span class="kw">filter</span>(n &gt;=<span class="st"> </span><span class="dv">5000</span>) %&gt;%
<span class="st">  </span><span class="kw">graph_from_data_frame</span>() %&gt;%
<span class="st">  </span><span class="kw">ggraph</span>(<span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) +
<span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">edge_alpha =</span> n, <span class="dt">edge_width =</span> n)) +
<span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">color =</span> <span class="st">&quot;indianred4&quot;</span>, <span class="dt">size =</span> <span class="dv">5</span>) +
<span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">repel =</span> <span class="ot">TRUE</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Word Network in NASA Dataset Descriptions&quot;</span>) +
<span class="st">  </span><span class="kw">theme_void</span>()</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_desc-1.png" width="864" /></p>
<p>Here there are such <em>strong</em> connections between the top dozen or so words (words like “data”, “global”, “resolution”, and “instrument”) that we do not see clear clustering structure in the network. We may want to use tf-idf (as described in detail in <a href="tfidf.html#tfidf">Chapter 4</a>) as a metric to find characteristic words for each description field, instead of looking at counts of words.</p>
<p>Next, let’s make a network of the keywords to see which keywords commonly occur together in the same datasets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">keyword_counts &lt;-<span class="st"> </span>nasa_keyword %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">pairwise_count</span>(keyword, id, <span class="dt">sort =</span> <span class="ot">TRUE</span>, <span class="dt">upper =</span> <span class="ot">FALSE</span>)

keyword_counts</code></pre></div>
<pre><code>## # A tibble: 13,390 × 3
##            item1                   item2     n
##            &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt;
## 1         OCEANS            OCEAN OPTICS  7324
## 2  EARTH SCIENCE              ATMOSPHERE  7318
## 3         OCEANS             OCEAN COLOR  7270
## 4   OCEAN OPTICS             OCEAN COLOR  7270
## 5        PROJECT               COMPLETED  6450
## 6  EARTH SCIENCE ATMOSPHERIC WATER VAPOR  3142
## 7     ATMOSPHERE ATMOSPHERIC WATER VAPOR  3142
## 8  EARTH SCIENCE                  OCEANS  2762
## 9  EARTH SCIENCE            LAND SURFACE  2718
## 10 EARTH SCIENCE               BIOSPHERE  2448
## # ... with 13,380 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
keyword_counts %&gt;%
<span class="st">  </span><span class="kw">filter</span>(n &gt;=<span class="st"> </span><span class="dv">700</span>) %&gt;%
<span class="st">  </span><span class="kw">graph_from_data_frame</span>() %&gt;%
<span class="st">  </span><span class="kw">ggraph</span>(<span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) +
<span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">edge_alpha =</span> n, <span class="dt">edge_width =</span> n)) +
<span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">color =</span> <span class="st">&quot;royalblue3&quot;</span>, <span class="dt">size =</span> <span class="dv">5</span>) +
<span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">repel =</span> <span class="ot">TRUE</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Co-occurrence Network in NASA Dataset Keywords&quot;</span>) +
<span class="st">  </span><span class="kw">theme_void</span>()</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_counts-1.png" width="864" /></p>
<p>We definitely see clustering here, and strong connections between keywords like <strong>OCEANS</strong>, <strong>OCEAN OPTICS</strong>, and <strong>OCEAN COLOR</strong>, or **PROJECT* and *COMPLETED**. These are the most commonly co-occurring words, but also just the most common keywords in general. To examine the relationships among keywords in a different way, we can find the correlation among the keywords as described in <a href="ngrams.html#ngrams">Chapter 5</a>. This looks for which keywords that are more likely to occur together than with other keywords in a description field.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">keyword_cors &lt;-<span class="st"> </span>nasa_keyword %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(keyword) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() &gt;=<span class="st"> </span><span class="dv">50</span>) %&gt;%
<span class="st">  </span><span class="kw">pairwise_cor</span>(keyword, id, <span class="dt">sort =</span> <span class="ot">TRUE</span>, <span class="dt">upper =</span> <span class="ot">FALSE</span>)

keyword_cors</code></pre></div>
<pre><code>## # A tibble: 7,875 × 3
##                  item1       item2 correlation
##                  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;
## 1            KNOWLEDGE     SHARING   1.0000000
## 2             DASHLINK        AMES   1.0000000
## 3             SCHEDULE  EXPEDITION   1.0000000
## 4           TURBULENCE      MODELS   0.9971871
## 5                APPEL   KNOWLEDGE   0.9967945
## 6                APPEL     SHARING   0.9967945
## 7         OCEAN OPTICS OCEAN COLOR   0.9952123
## 8  ATMOSPHERIC SCIENCE       CLOUD   0.9938681
## 9               LAUNCH    SCHEDULE   0.9837078
## 10              LAUNCH  EXPEDITION   0.9837078
## # ... with 7,865 more rows</code></pre>
<p>Notice that these keywords at the top of this sorted data frame have correlation coefficients equal to 1; they always occur together. This means these are redundant keywords and it may not make sense to continue to use both of these sets of pairs.</p>
<p>Let’s visualize the network of keyword correlations, just as we did for keyword co-occurences.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
keyword_cors %&gt;%
<span class="st">  </span><span class="kw">filter</span>(correlation &gt;<span class="st"> </span>.<span class="dv">6</span>) %&gt;%
<span class="st">  </span><span class="kw">graph_from_data_frame</span>() %&gt;%
<span class="st">  </span><span class="kw">ggraph</span>(<span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) +
<span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">edge_alpha =</span> correlation, <span class="dt">edge_width =</span> correlation)) +
<span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">color =</span> <span class="st">&quot;royalblue3&quot;</span>, <span class="dt">size =</span> <span class="dv">5</span>) +
<span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">repel =</span> <span class="ot">TRUE</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Correlation Network in NASA Dataset Keywords&quot;</span>) +
<span class="st">  </span><span class="kw">theme_void</span>()</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_cors-1.png" width="1152" /></p>
<p>This network appears much different than the co-occurence network. The difference is that the co-occurrence network asks a question about which keyword pairs occur most often, and the correlation network asks a question about which keyword pairs occur more often together than with other keywords. Notice here the high number of small clusters of keywords; the network structure can be extracted from the <code>graph_from_data_frame()</code> function above.</p>
</div>
<div id="calculating-tf-idf-for-the-description-fields" class="section level2">
<h2><span class="header-section-number">9.5</span> Calculating tf-idf for the description fields</h2>
<p>As discussed in <a href="#tf-idf">Chapter 4</a>, we can use tf-idf, the term frequency times inverse document frequency, to identify words that are especially important to a document within a collection of documents. Let’s apply that approach to the description fields of these NASA datasets. We will consider each description field a document, and the whole set of description fields the collection or corpus of documents. We have already used <code>unnest_tokens</code> above to make a tidy data frame of the words in the description fields, so now we can use <code>bind_tf_idf</code> to calculate tf-idf for each word.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">desc_tf_idf &lt;-<span class="st"> </span>nasa_desc %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, id, n)</code></pre></div>
<p>What are the highest tf-idf words in the NASA description fields?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">desc_tf_idf %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(-tf_idf)</code></pre></div>
<pre><code>## # A tibble: 1,913,224 × 6
##                          id                                          word     n    tf       idf
##                       &lt;chr&gt;                                         &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1  55942a7cc63a7fe59b49774a                                           rdr     1     1 10.375052
## 2  55942ac9c63a7fe59b49b688 palsar_radiometric_terrain_corrected_high_res     1     1 10.375052
## 3  55942ac9c63a7fe59b49b689  palsar_radiometric_terrain_corrected_low_res     1     1 10.375052
## 4  55942a7bc63a7fe59b4976ca                                          lgrs     1     1  8.765615
## 5  55942a7bc63a7fe59b4976d2                                          lgrs     1     1  8.765615
## 6  55942a7bc63a7fe59b4976e3                                          lgrs     1     1  8.765615
## 7  55942a7dc63a7fe59b497820                                           mri     1     1  8.583293
## 8  55942ad8c63a7fe59b49cf6c                      template_proddescription     1     1  8.295611
## 9  55942ad8c63a7fe59b49cf6d                      template_proddescription     1     1  8.295611
## 10 55942ad8c63a7fe59b49cf6e                      template_proddescription     1     1  8.295611
## # ... with 1,913,214 more rows, and 1 more variables: tf_idf &lt;dbl&gt;</code></pre>
<p>These are the most important words in the description fields as measured by tf-idf, meaning they are common but not too common. Notice we have run into an issue here; both <span class="math inline">\(n\)</span> and term frequency are equal to 1 for these terms, meaning that these were description fields that only had a single word in them. If a description field only contains one word, the tf-idf algorithm will think that is a really important word. Depending on our analytic goals, it might be a good idea to throw out all description fields that have fewer than 5 words or similar.</p>
<p>We now know which words in the descriptions have high tf-idf, and we also have labels for these descriptions in the keywords. Let’s do a full join of the keyword data frame and the data frame of description words with tf-idf, and then find the highest tf-idf words for a given keyword.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">desc_tf_idf &lt;-<span class="st"> </span><span class="kw">full_join</span>(desc_tf_idf, nasa_keyword, <span class="dt">by =</span> <span class="st">&quot;id&quot;</span>)</code></pre></div>
<p>Let’s plot some of the most important words, as measured by tf-idf, for a few example keywords used on NASA datasets. First, let’s use dplyr operations to filter for the keywords we want to examine and take just the top 15 words for each keyword. Then, let’s plot those words.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">desc_tf_idf %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">near</span>(tf, <span class="dv">1</span>)) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(keyword %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;SOLAR ACTIVITY&quot;</span>, <span class="st">&quot;CLOUDS&quot;</span>, 
                        <span class="st">&quot;SEISMOLOGY&quot;</span>, <span class="st">&quot;ASTROPHYSICS&quot;</span>,
                        <span class="st">&quot;HUMAN HEALTH&quot;</span>, <span class="st">&quot;BUDGET&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(keyword) %&gt;%
<span class="st">  </span><span class="kw">distinct</span>(word, keyword, <span class="dt">.keep_all =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">15</span>, tf_idf) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">factor</span>(word, <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">unique</span>(word)))) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, tf_idf, <span class="dt">fill =</span> keyword)) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~keyword, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Highest tf-idf words in NASA Metadata Description Fields&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Distribution of tf-idf for words from datasets labeled with select keywords&quot;</span>,
       <span class="dt">caption =</span> <span class="st">&quot;NASA metadata from https://data.nasa.gov/data.json&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;tf-idf&quot;</span>)</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_tf_idf-1.png" width="960" /></p>
<p>Using tf-idf has allowed us to identify important description words for each of these keywords. Datasets labeled with the keyword <strong>SEISMOLOGY</strong> have words like “earthquake”, “risk”, and “hazard” in their description, while those labeled with <strong>HUMAN HEALTH</strong> have descriptions characterized by words like “wellbeing”, “vulnerability”, and “children.” Most of the combinations of letters that are not English words are certainly acronyms (like OMB for the Office of Management and Budget), and the examples of years and numbers are important for these topics. The tf-idf statistic has identified the kinds of words it is intended to, important words for individual documents within a collection of documents.</p>
</div>
<div id="topic-modeling" class="section level2">
<h2><span class="header-section-number">9.6</span> Topic modeling</h2>
<p>Let’s try another approach to the question of what the NASA descriptions fields are about. We can use topic modeling as described in <a href="topicmodeling.html#topicmodeling">Chapter 7</a> to model each document (description field) as a mixture of topics and each topic as a mixture of words. As in earlier chapters, we will use <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">latent Dirichlet allocation (LDA)</a> for our topic modeling; there are other possible approaches for topic modeling.</p>
<p>To do the topic modeling as implemented here, we need to make a <code>DocumentTermMatrix</code>, a special kind of matrix from the tm package (of course, this is just a specific implementation of the general concept of a “document-term matrix”). Rows correspond to documents (description texts in our case) and columns correspond to terms (i.e., words); it is a sparse matrix and the values are word counts.</p>
<p>Let’s clean up the text a bit using stop words to remove some of the nonsense “words” leftover from HTML or other character encoding.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_stop_words &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(stop_words, 
                           <span class="kw">data_frame</span>(<span class="dt">word =</span> <span class="kw">c</span>(<span class="st">&quot;nbsp&quot;</span>, <span class="st">&quot;amp&quot;</span>, <span class="st">&quot;gt&quot;</span>, <span class="st">&quot;lt&quot;</span>,
                                               <span class="st">&quot;timesnewromanpsmt&quot;</span>, <span class="st">&quot;font&quot;</span>,
                                               <span class="st">&quot;td&quot;</span>, <span class="st">&quot;li&quot;</span>, <span class="st">&quot;br&quot;</span>, <span class="st">&quot;tr&quot;</span>, <span class="st">&quot;quot&quot;</span>,
                                               <span class="st">&quot;st&quot;</span>, <span class="st">&quot;img&quot;</span>, <span class="st">&quot;src&quot;</span>, <span class="st">&quot;strong&quot;</span>,
                                               <span class="kw">as.character</span>(<span class="dv">1</span>:<span class="dv">10</span>)), 
                                      <span class="dt">lexicon =</span> <span class="kw">rep</span>(<span class="st">&quot;custom&quot;</span>, <span class="dv">25</span>)))

word_counts &lt;-<span class="st"> </span>nasa_desc %&gt;%
<span class="st">  </span><span class="kw">anti_join</span>(my_stop_words) %&gt;%
<span class="st">  </span><span class="kw">count</span>(id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>()

word_counts</code></pre></div>
<pre><code>## # A tibble: 1,908,013 × 3
##                          id     word     n
##                       &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;
## 1  55942a8ec63a7fe59b4986ef     suit    82
## 2  55942a8ec63a7fe59b4986ef    space    69
## 3  56cf5b00a759fdadc44e564a     data    41
## 4  56cf5b00a759fdadc44e564a     leak    40
## 5  56cf5b00a759fdadc44e564a     tree    39
## 6  55942a8ec63a7fe59b4986ef pressure    34
## 7  55942a8ec63a7fe59b4986ef   system    34
## 8  55942a89c63a7fe59b4982d9       em    32
## 9  55942a8ec63a7fe59b4986ef       al    32
## 10 55942a8ec63a7fe59b4986ef    human    31
## # ... with 1,908,003 more rows</code></pre>
<p>This is the information we need, the number of times each word is used in each document, to make a <code>DocumentTermMatrix</code>. We can <code>cast</code> from our tidy text format to this non-tidy format as described in detail in <a href="dtm.html#dtm">Chapter 6</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">desc_dtm &lt;-<span class="st"> </span>word_counts %&gt;%
<span class="st">  </span><span class="kw">cast_dtm</span>(id, word, n)

desc_dtm</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 32003, terms: 35906)&gt;&gt;
## Non-/sparse entries: 1908013/1147191705
## Sparsity           : 100%
## Maximal term length: 166
## Weighting          : term frequency (tf)</code></pre>
<p>We see that this dataset contains documents (each of them a NASA description field) and terms (words). Notice that this example document-term matrix is (nearly) 100% sparse.</p>
<p>Now let’s use the <a href="https://cran.r-project.org/package=topicmodels">topicmodels</a> package to create an LDA model. How many topics will we tell the algorithm to make? This is a question much like in <span class="math inline">\(k\)</span>-means clustering; we don’t really know ahead of time. We tried this modeling procedure using 8, 16, 24, 32, and 64 topics; we found that at 24 topics, documents were still getting sorted into topics cleanly but going much beyond that caused the distributions of <span class="math inline">\(\gamma\)</span>, the probability that each document belongs in each topic, to look worrisome. We will show more details on this below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(topicmodels)

desc_lda &lt;-<span class="st"> </span><span class="kw">LDA</span>(desc_dtm, <span class="dt">k =</span> <span class="dv">24</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">seed =</span> <span class="dv">1234</span>))
desc_lda</code></pre></div>
<pre><code>## A LDA_VEM topic model with 24 topics.</code></pre>
<p>This is a stochastic algorithm that could have different results depending on where the algorithm starts, so we need to specify a <code>seed</code> for reproducibility.</p>
</div>
<div id="interpreting-the-topic-model" class="section level2">
<h2><span class="header-section-number">9.7</span> Interpreting the topic model</h2>
<p>Now that we have built the model, let’s <code>tidy</code> the results of the model. The tidytext package includes a tidying method for LDA models from the topicmodels package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tidy_lda &lt;-<span class="st"> </span><span class="kw">tidy</span>(desc_lda)

tidy_lda</code></pre></div>
<pre><code>## # A tibble: 861,792 × 3
##    topic  term          beta
##    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;
## 1      1  suit  1.504577e-62
## 2      2  suit  3.453300e-63
## 3      3  suit 6.482644e-100
## 4      4  suit  2.237356e-84
## 5      5  suit 5.824520e-112
## 6      6  suit  1.253670e-72
## 7      7  suit  1.327232e-03
## 8      8  suit 3.334593e-124
## 9      9  suit  3.649836e-09
## 10    10  suit  4.879224e-21
## # ... with 861,782 more rows</code></pre>
<p>The column <span class="math inline">\(\beta\)</span> tells us the probability of that term being generated from that topic for that document. Notice that some of very, very low, and some are not so low.</p>
<p>What are the top 5 terms for each topic?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">top_terms &lt;-<span class="st"> </span>tidy_lda %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(topic) %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>, beta) %&gt;%
<span class="st">  </span><span class="kw">ungroup</span>() %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(topic, -beta)

top_terms</code></pre></div>
<pre><code>## # A tibble: 240 × 3
##    topic        term       beta
##    &lt;int&gt;       &lt;chr&gt;      &lt;dbl&gt;
## 1      1        data 0.03849671
## 2      1         sea 0.02500301
## 3      1     surface 0.02457409
## 4      1         sst 0.02081033
## 5      1 temperature 0.01701900
## 6      1       level 0.01654994
## 7      1       avhrr 0.01221112
## 8      1     version 0.01176028
## 9      1     product 0.01103262
## 10     1          ir 0.01056180
## # ... with 230 more rows</code></pre>
<p>Let’s look at this visually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(top_terms, <span class="kw">aes</span>(term, beta, <span class="dt">fill =</span> <span class="kw">as.factor</span>(topic))) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Top 10 Terms in Each LDA Topic&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Topic modeling of NASA metadata description field texts&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="kw">expression</span>(beta)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~topic, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_beta-1.png" width="1152" /></p>
<p>We can see what a dominant word “data” is in these description texts. In addition, there are meaningful differences between these collections of terms, from terms about soil and biomass to terms about design, systems, and technology.</p>
<p>Next, let’s examine which topics are associated with which description fields (i.e., documents). We will look at a different probability for this, <span class="math inline">\(\gamma\)</span>, the probability that each document belongs in each topic.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_gamma &lt;-<span class="st"> </span><span class="kw">tidy</span>(desc_lda, <span class="dt">matrix =</span> <span class="st">&quot;gamma&quot;</span>)

lda_gamma</code></pre></div>
<pre><code>## # A tibble: 768,072 × 3
##                    document topic        gamma
##                       &lt;chr&gt; &lt;int&gt;        &lt;dbl&gt;
## 1  55942a8ec63a7fe59b4986ef     1 6.282939e-06
## 2  56cf5b00a759fdadc44e564a     1 1.123270e-05
## 3  55942a89c63a7fe59b4982d9     1 3.587508e-05
## 4  56cf5b00a759fdadc44e55cd     1 2.195722e-05
## 5  55942a89c63a7fe59b4982c6     1 6.461393e-05
## 6  55942a86c63a7fe59b498077     1 5.539564e-05
## 7  56cf5b00a759fdadc44e56f8     1 4.607134e-05
## 8  55942a8bc63a7fe59b4984b5     1 4.211972e-05
## 9  55942a6ec63a7fe59b496bf7     1 4.098247e-05
## 10 55942a8ec63a7fe59b4986f6     1 2.813661e-05
## # ... with 768,062 more rows</code></pre>
<p>Notice that these probabilites visible at the top of the data frame are quite low; some are higher, as we will see in a moment. Our model has assigned some probability to each description belonging to each of the topics we constructed from the sets of words. How are the probabilities distributed? Let’s visualize them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(lda_gamma, <span class="kw">aes</span>(gamma, <span class="dt">fill =</span> <span class="kw">as.factor</span>(topic))) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~topic, <span class="dt">ncol =</span> <span class="dv">4</span>) +
<span class="st">  </span><span class="kw">scale_y_log10</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of Probability for Each Topic&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Topic modeling of NASA metadata description field texts&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Number of documents&quot;</span>, <span class="dt">x =</span> <span class="kw">expression</span>(gamma))</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_gamma-1.png" width="1152" /></p>
<p>First notice that the y-axis is plotted on a log scale; otherwise it is difficult to make out any detail. Next, notice that <span class="math inline">\(\gamma\)</span> runs from 0 to 1 in each panel and remember that this is the probability that a given document belongs in a given topic. There are many values near zero, which means there are many documents that do not belong in each topics. Also, most of these panels show a higher number of documents near <span class="math inline">\(\gamma = 1\)</span>; these are the documents that <em>do</em> belong in those topics. This is the type of information we used to choose how many topics to use in our topic modeling procedure. When we tried options higher than 24 (like 32 or 64), the distributions for <span class="math inline">\(\gamma\)</span> started to look very flat; documents were not getting sorted into topics very well.</p>
</div>
<div id="connecting-topic-modeling-with-keywords" class="section level2">
<h2><span class="header-section-number">9.8</span> Connecting topic modeling with keywords</h2>
<p>Let’s connect these topic models with the keywords and see what happens. We can <code>join</code> this dataframe to the human-tagged keywords and see which keywords are associated with which topic.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_gamma &lt;-<span class="st"> </span><span class="kw">full_join</span>(lda_gamma, nasa_keyword, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;document&quot;</span> =<span class="st"> &quot;id&quot;</span>))

lda_gamma</code></pre></div>
<pre><code>## # A tibble: 3,037,671 × 4
##                    document topic        gamma                     keyword
##                       &lt;chr&gt; &lt;int&gt;        &lt;dbl&gt;                       &lt;chr&gt;
## 1  55942a8ec63a7fe59b4986ef     1 6.282939e-06        JOHNSON SPACE CENTER
## 2  55942a8ec63a7fe59b4986ef     1 6.282939e-06                     PROJECT
## 3  55942a8ec63a7fe59b4986ef     1 6.282939e-06                   COMPLETED
## 4  56cf5b00a759fdadc44e564a     1 1.123270e-05                    DASHLINK
## 5  56cf5b00a759fdadc44e564a     1 1.123270e-05                        AMES
## 6  56cf5b00a759fdadc44e564a     1 1.123270e-05                        NASA
## 7  55942a89c63a7fe59b4982d9     1 3.587508e-05 GODDARD SPACE FLIGHT CENTER
## 8  55942a89c63a7fe59b4982d9     1 3.587508e-05                     PROJECT
## 9  55942a89c63a7fe59b4982d9     1 3.587508e-05                   COMPLETED
## 10 56cf5b00a759fdadc44e55cd     1 2.195722e-05                    DASHLINK
## # ... with 3,037,661 more rows</code></pre>
<p>Now we can use <code>filter</code> to keep only the document-topic entries that have probabilities greater than 0.9. After that, let’s find the top keywords for each topic.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">top_keywords &lt;-<span class="st"> </span>lda_gamma %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(gamma &gt;<span class="st"> </span><span class="fl">0.9</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(topic, keyword) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(keyword, <span class="dt">sort =</span> <span class="ot">TRUE</span>)

top_keywords</code></pre></div>
<pre><code>## Source: local data frame [1,198 x 3]
## Groups: topic [24]
## 
##    topic       keyword     n
##    &lt;int&gt;         &lt;chr&gt; &lt;int&gt;
## 1      5   OCEAN COLOR  4480
## 2      5  OCEAN OPTICS  4480
## 3      5        OCEANS  4480
## 4     16   OCEAN COLOR  1968
## 5     16  OCEAN OPTICS  1968
## 6     16        OCEANS  1968
## 7      1 EARTH SCIENCE  1279
## 8      1        OCEANS  1277
## 9      6 EARTH SCIENCE  1057
## 10     9       PROJECT   905
## # ... with 1,188 more rows</code></pre>
<p>What are the top keywords for each topic?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">top_keywords %&gt;%
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>, n) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(keyword, n, <span class="dt">fill =</span> <span class="kw">as.factor</span>(topic))) +
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Top Keywords for Each LDA Topic&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Topic modeling of NASA metadata description field texts&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;Number of documents&quot;</span>) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">facet_wrap</span>(~topic, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre></div>
<p><img src="09-nasa-metadata_files/figure-html/plot_top_keywords-1.png" width="1536" /></p>
<p>Let’s take a step back and remind ourselves what this plot is telling us. NASA datasets are tagged with keywords by human beings, and we have built an LDA topic model (with 24 topics) for the description fields of the NASA datasets. This plot answers the question, “For the datasets that have description fields with a high probability of belonging to a given topic, what are the most common human-assigned keywords?”</p>
<p>It’s interesting that the keywords for topics 5, 15, and 16 are essentially duplicates of each other, because the top terms in those topics do exhibit meaningful differences. Also note that by number of documents, the combination of 5, 15, and 16 is quite a large percentage of the total number of datasets represented in this plot.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="twitter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="usenet.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dgrtwo/tidy-text-mining/edit/master/09-nasa-metadata.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
